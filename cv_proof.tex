%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[landscape]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\title{Almost-Cross-Validation Theorem}

\maketitle
We are interested in bounding the error of the selected model when
tuning penalty parameters by a ``modified averaged version of cross-validation''.
Our result is an application of Mitchell's result to penalized regression
problems where the fitted functions are smooth with respect to the
penalty parameters.

Suppose that the data is generated from the model
\[
y=g^{*}(x)+\epsilon
\]


Suppose the errors are independent and bounded ($\|\epsilon\|_{\infty}<\infty$
).

The penalized regression model fitted on dataset $D$ is denoted
\[
\hat{g}_{D}(\cdot|\boldsymbol{\lambda})=\arg\min_{g\in\mathcal{G}}L_{D}(g|\boldsymbol{\lambda})
\]


Split data $D$ into $K$ folds, where each fold is $D_{k}$ and $D_{-k}=D\backslash D_{k}$.
Suppose $D$ has size $n$, $D_{k}$ all have size $n_{V}$, and $D_{-k}$
all have size $n_{T}$.

We select penalty parameters such that 
\[
\hat{\boldsymbol{\lambda}}=\arg\min_{\lambda}\sum_{k=1}^{K}\|y-\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda})\|_{k}^{2}
\]


We consider the behavior of the ``modified averaged version of cross-validation''
\[
\hat{g}_{MCV}(\cdot|D)=\frac{1}{K}\sum_{k=1}^{K}\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda})
\]


Under sufficient entropy conditions, the error of the selected model
will converge to the error of the oracle. 

We are interested in bounding its generalization error 
\[
E_{D}\|\hat{g}_{MCV}(\cdot|D)-g^{*}\|^{2}=E_{D}\left[\int\left(\hat{g}_{MCV}(x|D)-g^{*}(x)\right)^{2}d\mu(x)\right]
\]



\section{Theorem 2}

We will assume that $\sup_{g\in\mathcal{G}}\|g\|_{\infty}\le G$.
Suppose $\|\epsilon\|_{\infty}\le G$.

Let $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$.

Suppose that for all $k=1,...,K$, the following smoothness condition
holds: For some constant $C>0$ , for all $\boldsymbol{\lambda}_{1},\boldsymbol{\lambda}_{2}\in\Lambda$,
we have 
\[
\|\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda}_{1})-\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda}_{2})\|_{\infty}\le C\|\boldsymbol{\lambda}_{1}-\boldsymbol{\lambda}_{2}\|
\]


Then there is an absolute constant $c>0$ such that for all $a>0$

\[
E\left[\|g^{*}-\hat{g}_{MCV}(\cdot|D)\|^{2}\right]\le(1+a)\min_{\lambda\in\Lambda}E\left[\|g^{*}-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]+\frac{(1+a)^{2}}{a}\frac{cJ}{n_{V}}\left(C_{\Lambda}+\frac{1}{2}\log n_{V}+4GC_{\Lambda}\log n_{V}\right)
\]


where
\[
C_{\Lambda}=1+\log\left(128GC(\lambda_{max}-\lambda_{min})\right)
\]



\subsubsection*{Proof}

We apply Theorem 3.5 in Mitchell's paper. We consider the loss function
$Q(x,g)=\left(g(x)-y\right)^{2}$. We will use the set of statistics
\[
\mathcal{G}(T)=\left\{ \hat{g}_{T}(\cdot|\boldsymbol{\lambda})\right\} 
\]


where $T$ is some training data.

\textbf{1. Establish Assumptions A.1 and A.2 are satisfied:}

Theorem 3.5 relies on assumption A.1 and A.2 to be satisfied. 

Assumption A.1 states that the Orlicz norm with $\psi_{1}=\exp(x)-1$
is bounded for some constant $K_{0}$:
\[
\left\Vert \left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right\Vert _{L_{\psi_{1}}}\le K_{0}
\]


where 
\[
\|f\|_{\psi_{1}}=\inf\left\{ C>0:E\psi(|f|/C)\le1\right\} 
\]


Since we have assumed that $\|g\|_{\infty}\le G$, then by Lemma Orlicz-norm-properties
(see Appendix)
\begin{eqnarray*}
\left\Vert \left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right\Vert _{L_{\psi_{1}}} & \le & 2\left\Vert \left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right\Vert _{\infty}\\
 & \le & 18G^{2}
\end{eqnarray*}


Assumption A.2 states that
\[
\left\Vert \left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right\Vert _{L_{2}}\le K_{1}\|\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\|_{L_{2}}
\]


To see that this is satisfied, note that
\begin{eqnarray*}
\left\Vert \left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right\Vert _{L_{2}}^{2} & = & \int\left(\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}-y\right)^{2}\right)^{2}d\mu(x)\\
 & = & \int\left(\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right)^{2}+2\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right)\left(g^{*}-y\right)\right)^{2}d\mu(x)\\
 & = & \int\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right)^{2}\left(\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right)+2\left(g^{*}-y\right)\right)^{2}d\mu(x)\\
 & \le & \int\left(\hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right)^{2}\left(2K+2K\right)^{2}d\mu(x)\\
 & \le & 16G^{2}\left\Vert \hat{g}_{D}(\cdot|\boldsymbol{\lambda})-g^{*}\right\Vert _{L_{2}}^{2}
\end{eqnarray*}


\textbf{2. Calculate the $L_{2}$ and $\psi_{1}$ entropies}

Theorem 3.5 requires calculating the entropies of the excess loss
functions 
\[
\mathcal{Q}(T)=\left\{ \left(\hat{g}_{T}(x|\boldsymbol{\lambda})-y\right)^{2}-\left(g^{*}(x)-y\right)^{2}:\lambda\in\Lambda\right\} 
\]


where $T$ is any training dataset.

We are interested in calculating the $\|\cdot\|_{\psi_{1}}$ and $\|\cdot\|_{L_{2}}$
entropy of the function class 
\[
\mathcal{Q}_{d}^{L_{2}}(T)=\left\{ \tilde{Q}\in\mathcal{Q}(T):\|\tilde{Q}\|_{L_{2}}\le\sqrt{d}\right\} 
\]


To bound these two entropies, we'll actually bound $H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\infty})$
since
\[
H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\psi_{1}})\le H(u/2,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\infty})
\]


and
\[
H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}})\le H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\infty})
\]


We show that the excess log functions $Q(x|\boldsymbol{\lambda})$
are smoothly parametric in $\boldsymbol{\lambda}$:

\begin{eqnarray*}
\left\Vert Q(x|\boldsymbol{\lambda}_{1})-Q(x|\boldsymbol{\lambda}_{2})\right\Vert _{\infty} & = & \left\Vert \left(\hat{g}_{T}(x|\boldsymbol{\lambda}_{1})-y\right)^{2}-\left(\hat{g}_{T}(x|\boldsymbol{\lambda}_{2})-y\right)^{2}\right\Vert _{\infty}\\
 & = & \left\Vert \left(\hat{g}_{T}(x|\boldsymbol{\lambda}_{1})-\hat{g}_{T}(x|\boldsymbol{\lambda}_{2})\right)\left(\hat{g}_{T}(x|\boldsymbol{\lambda}_{1})+\hat{g}_{T}(x|\boldsymbol{\lambda}_{2})-2y\right)\right\Vert _{\infty}\\
 & \le & 6G\left\Vert \hat{g}_{T}(x|\boldsymbol{\lambda}_{1})-\hat{g}_{T}(x|\boldsymbol{\lambda}_{2})\right\Vert _{\infty}
\end{eqnarray*}


Under the assumption that
\[
\left\Vert \hat{g}_{T}(x|\boldsymbol{\lambda}_{1})-\hat{g}_{T}(x|\boldsymbol{\lambda}_{2})\right\Vert _{\infty}\le C\left\Vert \boldsymbol{\lambda}_{1}-\boldsymbol{\lambda}_{2}\right\Vert 
\]


then
\[
\left\Vert Q(x|\boldsymbol{\lambda}_{1})-Q(x|\boldsymbol{\lambda}_{2})\right\Vert _{\infty}\le6GC\left\Vert \boldsymbol{\lambda}_{1}-\boldsymbol{\lambda}_{2}\right\Vert 
\]


Hence by lemma param covering cube

\begin{eqnarray*}
H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\infty}) & \le & H\left(\frac{u}{6GC},\Lambda,\|\cdot\|_{2}\right)\\
 & \le & \log\left[\frac{1}{C_{J}}\left(\frac{36GC(\lambda_{max}-\lambda_{min})+2u}{u}\right)^{J}\right]
\end{eqnarray*}


Hence

\[
H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\psi_{1}})\le\log\left[\frac{1}{C_{J}}\left(\frac{72GC(\lambda_{max}-\lambda_{min})+2u}{u}\right)^{J}\right]
\]


and

\[
H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}})\le\log\left[\frac{1}{C_{J}}\left(\frac{36GC(\lambda_{max}-\lambda_{min})+2u}{u}\right)^{J}\right]
\]


We calculate each component of the complexity term $J(d)$: 
\begin{eqnarray*}
\gamma_{1}(\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\psi_{1}}) & \le & \int_{0}^{2G}H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\psi_{1}})du\\
 & = & \int_{0}^{2G}\log\left[\frac{1}{C_{J}}\left(\frac{72GC(\lambda_{max}-\lambda_{min})+2u}{u}\right)^{J}\right]du\\
 & = & 2G\int_{0}^{1}\left[\log\left(\frac{1}{C_{J}}\right)+J\log\left(\frac{72GC(\lambda_{max}-\lambda_{min})+4Gv}{2Gv}\right)\right]dv\\
 & \le & 2G\int_{0}^{1}\left[\log\left(\frac{1}{C_{J}}\right)+J\log\left(\frac{72GC(\lambda_{max}-\lambda_{min})}{v}\right)+J\log4\right]dv\\
 & = & 2G\left[\log\left(\frac{1}{C_{J}}\right)+J+J\log\left(72GC(\lambda_{max}-\lambda_{min})\right)+J\log4\right]\\
 & < & 2GJ\left(1+\log\left(288GC(\lambda_{max}-\lambda_{min})\right)\right)
\end{eqnarray*}


(since $C_{J}>1$ for all $J$)

and

\begin{eqnarray*}
\gamma_{2}(\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}}) & = & \int_{0}^{\sqrt{d}}\left[H(u,\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}})\right]^{1/2}du\\
 & = & \sqrt{d}\int_{0}^{1}\left(\log\left[\frac{1}{C_{J}}\left(\frac{36GC(\lambda_{max}-\lambda_{min})+2\sqrt{d}v}{\sqrt{d}v}\right)^{J}\right]\right)^{1/2}dv\\
 & \le & \sqrt{d}\left[\int_{0}^{1}\left(\log\left(\frac{1}{C_{J}}\right)+J\log\left(\frac{72GC(\lambda_{max}-\lambda_{min})}{\sqrt{d}v}\right)+J\log4\right)dv\right]^{1/2}\\
 & = & \sqrt{d}\left[\int_{0}^{1}\left(\log\left(\frac{1}{C_{J}}\right)+J\log\left(\frac{72GC(\lambda_{max}-\lambda_{min})}{\sqrt{d}}\right)+J\log\frac{1}{v}+J\log4\right)dv\right]^{1/2}\\
 & = & \sqrt{d}\left[\log\left(\frac{1}{C_{J}}\right)+J+J\log4+J\log\left(\frac{72GC(\lambda_{max}-\lambda_{min})}{\sqrt{d}}\right)\right]^{1/2}\\
 & < & \sqrt{d}\left[J\left(1+\log\left(\frac{288GC(\lambda_{max}-\lambda_{min})}{\sqrt{d}}\right)\right)\right]^{1/2}
\end{eqnarray*}


\textbf{3. Apply Theorem 3.5}

Now we must select an increasing function $\mathcal{J}$ such that
$\mathcal{J}^{-1}$ is strictly convex and

\[
\mathcal{J}(d)\ge\gamma_{2}(\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}})+\frac{\left(\log n_{V}\right)\gamma_{1}(\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{\psi_{1}})}{\sqrt{n_{V}}},\forall d\ge d_{min}
\]


Let us choose $d_{min}=1/n_{V}$. Then 
\begin{eqnarray*}
\gamma_{2}(\mathcal{Q}_{d}^{L_{2}}(T),\|\cdot\|_{L_{2}}) & \le & \sqrt{d}\left[J\left(1+\log\left(\frac{128GC(\lambda_{max}-\lambda_{min})}{\sqrt{d}}\right)\right)\right]^{1/2}\\
 & \le & \sqrt{d}\left[J\left(1+\log\left(128GC(\lambda_{max}-\lambda_{min})\sqrt{n_{V}}\right)\right)\right]^{1/2}
\end{eqnarray*}


Let 
\[
K_{n,1}=\left[J\left(1+\log\left(288GC(\lambda_{max}-\lambda_{min})\sqrt{n_{V}}\right)\right)\right]^{1/2}
\]


and
\[
K_{n,2}=2GJ\left(1+\log\left(288GC(\lambda_{max}-\lambda_{min})\right)\right)\left(\log n_{V}\right)
\]


We define

\[
Q(d)\coloneqq\sqrt{d}K_{n,1}+\frac{1}{\sqrt{n_{V}}}K_{n,2}
\]
 

Then $\mathcal{J}^{-1}(b)$ is the strictly convex function
\[
\mathcal{J}^{-1}(b)=\left(\frac{b-\frac{1}{\sqrt{n_{V}}}K_{n,2}}{K_{n,1}}\right)^{2}
\]


The convex conjugate of $\mathcal{J}^{-1}(b)$ is
\begin{eqnarray*}
\psi(z) & = & \sup_{x}xz-\mathcal{J}^{-1}(x)\\
 & = & \sup_{x}xz-\left(\frac{x-\frac{1}{\sqrt{n_{V}}}K_{n,2}}{K_{n,1}}\right)^{2}\\
 & = & \frac{K_{n,1}^{2}z^{2}}{4}+\frac{1}{\sqrt{n_{V}}}K_{n,2}z
\end{eqnarray*}


We check the condition that Then $\psi(z)/z^{r}$ is a decreasing
function in $z$ for $r=3\ge1$. Also $\lim_{z\rightarrow\infty}\psi(z)=\infty$.

Theorem 3.5 states that for all $a>0,q>1$, we have

\[
E\left[\|y-\hat{g}_{MCV}(\cdot|D)\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\le(1+a)\min_{\lambda\in\Lambda}\left(E_{D^{(n_{T})}}\left[\|y-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\right)+\frac{ac\epsilon_{q}(1/q)}{q}
\]


where $\epsilon_{q}(u)=\psi\left(\frac{2q^{r+1}(1+a)u}{a\sqrt{n_{V}}}\right)\vee\frac{1}{n_{V}}\forall u>0$.

We calculate $\epsilon_{q}\left(\frac{1}{q}\right)$: 
\begin{eqnarray*}
\epsilon_{q}\left(\frac{1}{q}\right)=\psi\left(\frac{2q^{4}(1+a)\frac{1}{q}}{a\sqrt{n_{V}}}\right) & = & \frac{K_{n,1}^{2}}{4}\left(\frac{2q^{3}(1+a)}{a\sqrt{n_{V}}}\right)^{2}+\frac{1}{\sqrt{n_{V}}}K_{n,2}\left(\frac{2q^{3}(1+a)}{a\sqrt{n_{V}}}\right)
\end{eqnarray*}


Finally, we get
\begin{eqnarray*}
 &  & E\left[\|y-\hat{g}_{MCV}(\cdot|D)\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\\
 & \le & (1+a)\min_{\lambda\in\Lambda}\left(E_{D^{(n_{T})}}\left[\|y-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\right)+ac\left(\frac{K_{n,1}^{2}}{4}q^{5}\left(\frac{2(1+a)}{a\sqrt{n_{V}}}\right)^{2}+K_{n,2}\frac{1}{\sqrt{n_{V}}}\left(\frac{2q^{2}(1+a)}{a\sqrt{n_{V}}}\right)\right)\\
 & = & (1+a)\min_{\lambda\in\Lambda}\left(E_{D^{(n_{T})}}\left[\|y-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\right)+\frac{c}{n_{V}}\left(K_{n,1}^{2}q^{5}\frac{(1+a)^{2}}{a}+2K_{n,2}q^{2}(1+a)\right)
\end{eqnarray*}


As $q\rightarrow1$, we get 
\[
E\left[\|y-\hat{g}_{MCV}(\cdot|D)\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\le(1+a)\left[\min_{\lambda\in\Lambda}\left[\left(E_{D^{(n_{T})}}\left[\|y-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\right)\right]+\frac{c}{n_{V}}\left(K_{n,1}^{2}\frac{(1+a)}{a}+2K_{n,2}\right)\right]
\]


Let $\mathcal{E}(g)=\left[\left(E_{D^{(n_{T})}}\left[\|y-g\|^{2}\right]-E\left[\|y-g^{*}\|^{2}\right]\right)\right]$

\textbf{4. Massaging to make a pretty theorem}

We can write the above as

\[
\mathcal{E}(\hat{g}_{MCV}(\cdot|D))\le(1+a)\min_{\lambda\in\Lambda}\mathcal{E}\left(\hat{g}_{D^{(n_{T})}}(\cdot|\lambda)\right)+(1+a)\frac{cJ}{n_{V}}\left(\frac{(1+a)}{a}\left(\tilde{C}+\frac{1}{2}\log n_{V}\right)+4G\tilde{C}\left(\log n_{V}\right)\right)
\]


where
\begin{eqnarray*}
\tilde{C} & = & 1+\log\left(288GC(\lambda_{max}-\lambda_{min})\right)\\
 & \le & 5+\log\left(GC\Delta_{\lambda}\right)
\end{eqnarray*}


Moreover, since $(1+a)/a>1$, we can write

\begin{eqnarray*}
\mathcal{E}(\hat{g}_{MCV}(\cdot|D)) & \le & (1+a)\min_{\lambda\in\Lambda}\mathcal{E}\left(\hat{g}_{D^{(n_{T})}}(\cdot|\lambda)\right)+\frac{(1+a)^{2}}{a}\frac{cJ}{n_{V}}\left(\tilde{C}+\frac{1}{2}\log n_{V}+4G\tilde{C}\log n_{V}\right)\\
 & = & (1+a)\min_{\lambda\in\Lambda}\mathcal{E}\left(\hat{g}_{D^{(n_{T})}}(\cdot|\lambda)\right)+\frac{(1+a)^{2}}{a}\frac{cJ}{n_{V}}\left(5+\log\left(GC\Delta_{\lambda}\right)+\frac{1}{2}\log n_{V}+4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}\right)\\
 & \le & (1+a)\min_{\lambda\in\Lambda}\mathcal{E}\left(\hat{g}_{D^{(n_{T})}}(\cdot|\lambda)\right)+\frac{(1+a)^{2}}{a}\frac{cJ}{n_{V}}\left(16G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}\right)
\end{eqnarray*}


as long as $4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\ge\frac{1}{2}$
and $\log\left(GC\Delta_{\lambda}\right)\le4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}$
and $5\le4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}$.

First, note that if $G\ge1/2$, then 
\[
\log\left(GC\Delta_{\lambda}\right)\le2G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\le4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}
\]


and 
\[
5\le2G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\le4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\log n_{V}
\]


Also

\[
\frac{1}{8}\le\frac{1}{2}\left(5+\log\left(GC\Delta_{\lambda}\right)\right)\le4G\left(5+\log\left(GC\Delta_{\lambda}\right)\right)
\]


Also, I want to write 
\[
5\le\log\left(GC\Delta_{\lambda}\right)\implies\exp(5)/G\Delta_{\lambda}\le C
\]


Then
\[
\mathcal{E}(\hat{g}_{MCV}(\cdot|D))\le(1+a)\min_{\lambda\in\Lambda}\mathcal{E}\left(\hat{g}_{D^{(n_{T})}}(\cdot|\lambda)\right)+32\frac{(1+a)^{2}}{a}\frac{cJ}{n_{V}}G\log\left(GC\Delta_{\lambda}\right)\log n_{V}
\]



\section{Lemma 2 for $\lambda$ that changes with $n$}

We will assume that $\sup_{g\in\mathcal{G}}\|g\|_{\infty}\le G$.

Suppose $\Lambda=\left[n_{T}^{-t_{min}},n_{T}^{t_{max}}\right]^{J}.$

Suppose that for all $k=1,...,K$, the following smoothness condition
holds: For some constant $C,\kappa>0$ , for all $\boldsymbol{\lambda}_{1},\boldsymbol{\lambda}_{2}\in\Lambda$,
we have 
\[
\|\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda}_{1})-\hat{g}_{D_{-k}}(\cdot|\boldsymbol{\lambda}_{2})\|_{\infty}\le Cn_{T}^{\kappa}\|\boldsymbol{\lambda}_{1}-\boldsymbol{\lambda}_{2}\|
\]


Then there is an absolute constant $c>0$ and a constant $c_{G}$
 that only depends on $G$ such that for all $a>0$

\[
E\left[\|g^{*}-\hat{g}_{MCV}(\cdot|D)\|^{2}\right]\le(1+a)\min_{\lambda\in\Lambda}E\left[\|g^{*}-\hat{g}_{D^{(n_{T})}}(\cdot|\boldsymbol{\lambda})\|^{2}\right]+\frac{(1+a)^{2}}{a}\frac{c^{2}J}{n_{V}}\left[\left(c_{G}\log n_{V}+1\right)\left(\left(\kappa+t_{max}\right)\log n_{T}+1\right)+c_{G}\right]
\]



\section{Appendix}


\subsubsection{Lemma: Orlicz Norm Properties}

For any function $f$, we have 
\[
\|f\|_{\psi_{1}}\le2\|f\|_{\infty}
\]


Also
\[
\|Kf\|_{\psi}=K\|f\|_{\psi}
\]


Also suppose that $\psi$ is a monotone function. Then
\[
\|gf\|_{\psi}\le\|g\|_{\infty}\|f\|_{\psi}
\]



\subsubsection*{Proof}

To prove the bound:
\[
E\left[\exp\left(\frac{f}{2\|f\|_{\infty}}\right)-1\right]\le\exp\frac{1}{2}-1<1
\]


To prove the norm scaling property: 
\begin{eqnarray*}
\|Kf\|_{\psi} & = & \inf\left\{ C>0:E\psi(|Kf|/C)\le1\right\} \\
 & = & \inf\left\{ C>0:E\psi(|f|/(C/K))\le1\right\} \\
 & = & \inf\left\{ KC>0:E\psi(|f|/C)\le1\right\} \\
 & = & K\inf\left\{ C>0:E\psi(|f|/C)\le1\right\} \\
 & = & K\|f\|_{\psi}
\end{eqnarray*}


To prove the last bound, note that under the assumption that $\psi$
is a monotone function, then
\[
E\psi(|gf|/C)\le E\psi\left(\left|\|g\|_{\infty}f\right|/C\right)
\]


Therefore
\[
\inf_{C}E\psi(|gf|/C)\le\inf_{C}E\psi\left(\left|\|g\|_{\infty}f\right|/C\right)
\]


Therefore

\begin{eqnarray*}
\|gf\|_{\psi} & = & \inf\left\{ C>0:E\psi(|gf|/C)\le1\right\} \\
 & \le & \inf\left\{ C>0:E\psi\left(\left|\|g\|_{\infty}f\right|/C\right)\le1\right\} \\
 & = & \|g\|_{\infty}\inf\left\{ C>0:E\psi\left(\left|f\right|/C\right)\le1\right\} \\
 & = & \|g\|_{\infty}\|f\|_{\psi}
\end{eqnarray*}

\end{document}
