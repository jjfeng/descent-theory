%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[landscape]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\title{Proofs for Smoothness of Parametric Regression Models}

\maketitle

\section*{Intro}

In this document, we consider parametric regression models $g(\cdot|\boldsymbol{\theta})$
where $\boldsymbol{\theta}\in\mathbb{R}^{p}$. Throughout, we will
suppose $\boldsymbol{\theta}^{*}$ is the model such that 
\[
\boldsymbol{\theta}^{*}=\arg\min_{\theta\in\Theta}E_{x,y}\left[\left(y-g(x|\boldsymbol{\theta})\right)^{2}\right]
\]


Technically, all the proofs require is that $\boldsymbol{\theta}^{*}\in\Theta$
is fixed. In the convergence rate proofs, we will need $\boldsymbol{\theta}^{*}$
to satisfy $E[y|x]=g(x|\boldsymbol{\theta}^{*})$.

We are interested in establishing inequalities of the form 
\[
\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\|_{2}\le C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\]


If the functions are $L$-Lipschitz in their parameterization, we
will also be able to bound the distance between the actual functions.
That is, if there is a constant $L>0$ such that for all $\boldsymbol{\theta_{1},\theta_{2}}$
\[
\|g(\cdot|\boldsymbol{\theta}_{1})-g(\cdot|\boldsymbol{\theta}_{2})\|_{\infty}\le L\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|_{2}
\]


Then
\[
\|g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(2)}}})\|_{\infty}\le LC\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\]



\subsubsection*{Document Outline}

First, we consider smooth training criteria and prove smoothness for
two parametric regression examples:
\begin{enumerate}
\item Multiple penalties for a single model
\[
\hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)
\]

\item Additive model (no ridge!)
\[
\hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}_{j})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}_{j})
\]

\end{enumerate}
Then we will extend these results to non-smooth penalty functions.

Finally we will consider examples of parametric penalty functions.
This includes a deep dive into the Sobolev penalty.


\section{Multiple smooth penalties for a single model}

The function class of interest are the minimizers of the penalized
least squares criterion: 
\[
\mathcal{G}(T)=\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right):\boldsymbol{\lambda}\in\Lambda\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$.

Suppose that the penalties and the function $g(x|\boldsymbol{\theta})$
are twice-differentiable and convex wrt $\boldsymbol{\theta}$: 
\begin{itemize}
\item Suppose that $\nabla_{\theta}^{2}P_{j}(\boldsymbol{\theta})$ are
PSD matrices for all $j=1,...,J$. 
\item Suppose that $\nabla_{\theta}^{2}\|y-g(x|\boldsymbol{\theta})\|_{T}^{2}$
is a PSD matrix.
\end{itemize}
Suppose there is some constants $K_{1},K_{0}>0$ such that for all
$j=1,...,J$ and any $\boldsymbol{\theta}'$, we have

\[
\left|\left.\nabla_{\theta}P_{j}\left(\boldsymbol{\theta}\right)\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}'}\right|\le K_{1}\|\boldsymbol{\theta}'\|_{2}+K_{0}
\]


Let 
\[
C_{\theta^{*},\Lambda}=\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta}^{*})\|_{T}^{2}+\lambda_{max}\sum_{j=1}^{J}P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have

\textbf{
\begin{eqnarray*}
\|\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda^{(1)}})-\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda^{(2)}})\| & \le & \frac{1}{\lambda_{min}wJ}\left(\left(K_{1}+w\right)\sqrt{\frac{2}{\lambda_{min}w}C_{\theta^{*},\Lambda}}+K_{0}\right)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert 
\end{eqnarray*}
}

Moreover, if $g(\cdot|\boldsymbol{\theta})$ is $L$-Lipschitz wrt
$\|\cdot\|_{\infty}$, then

\[
\|g(\cdot|\boldsymbol{\theta}_{1})-g(\cdot|\boldsymbol{\theta}_{2})\|_{\infty}\le\frac{L}{\lambda_{min}wJ}\left(\left(K_{1}+w\right)\sqrt{\frac{2}{\lambda_{min}w}C_{\theta^{*},\Lambda}}+K_{0}\right)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert 
\]



\subsubsection*{Proof}

\textbf{1. We calculate $\nabla_{\lambda}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
using the implicit differentiation trick.}

By the KKT conditions, we have 
\[
\left.\nabla_{\boldsymbol{\theta}}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)\right)\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}=0
\]


Now we implicitly differentiate with respect to $\boldsymbol{\lambda}$

\[
\left.\left[\nabla_{\theta}^{2}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)\right)\nabla_{\lambda}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})+\nabla_{\theta}P(\boldsymbol{\theta})+w\boldsymbol{\theta}\vec{\boldsymbol{1}}_{J}^{\top}\right]\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}=0
\]


where 
\[
\nabla_{\theta}P(\boldsymbol{\theta})=\left\{ \begin{array}{ccc}
\nabla_{\theta}P_{1}(\boldsymbol{\theta}) & ... & \nabla_{\theta}P_{J}(\boldsymbol{\theta})\end{array}\right\} 
\]


Rearranging, we have for all $\boldsymbol{\lambda}\in\Lambda$
\[
\nabla_{\lambda}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})=-\left[\nabla_{\theta}^{2}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)\right)_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right]^{-1}\left(\left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}+w\boldsymbol{\theta}\vec{\boldsymbol{1}}_{J}^{\top}\right)
\]


\textbf{2. Bound $\|\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\|$
for $i=1,...,p$}

We know that\textbf{
\begin{eqnarray*}
\left\Vert \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\right\Vert  & = & \left\Vert e_{i}^{\top}\left[\nabla_{\theta}^{2}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)\right)_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right]^{-1}\left(\left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}+w\boldsymbol{\theta}\vec{\boldsymbol{1}}_{J}^{\top}\right)\right\Vert \\
 & = & \left\Vert e_{i}^{\top}\left[\nabla_{\theta}^{2}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right)\right)_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right]^{-1}\left(\left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}+w\boldsymbol{\theta}\vec{\boldsymbol{1}}_{J}^{\top}\right)\right\Vert \\
 & \le & \left\Vert \left[\nabla_{\theta}^{2}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta})\right)_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}+\sum_{j=1}^{J}\lambda_{j}wI\right]^{-1}\right\Vert \left(\left\Vert \left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right\Vert _{F}+w\left\Vert \boldsymbol{\theta}\vec{\boldsymbol{1}}_{J}^{\top}\right\Vert \right)\\
 & \le & \left\Vert \left[\sum_{j=1}^{J}\lambda_{j}wI\right]^{-1}\right\Vert \left(\left\Vert \left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right\Vert _{F}+w\sqrt{J}\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}\right)\\
 & \le & \frac{1}{J\lambda_{min}w}\left(\sqrt{J}\left(K_{1}\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}+K_{0}\right)+w\sqrt{J}\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}\right)\\
 & = & \frac{\left(K_{1}+w\right)\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}+K_{0}}{\lambda_{min}w\sqrt{J}}
\end{eqnarray*}
}

The second inequality follows from the assumption that $\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta})$
is convex in $\boldsymbol{\theta}$. The last inequality follows from
the assumption $\left.\nabla_{\theta}P(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\le K_{1}\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}+K_{0}$.

We can use the definition of $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
to bound $\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}$.
By definition, 
\begin{eqnarray*}
\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}^{2} & \le & \frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta}^{*})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|^{2}\right)\\
 & \le & \frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta}^{*})\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|^{2}\right)\\
 & = & C_{\theta^{*},\Lambda}
\end{eqnarray*}


So 
\[
\|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})\|_{2}\le\sqrt{\frac{2}{J\lambda_{min}w}C_{\theta^{*},\Lambda}}
\]


Hence for all $\boldsymbol{\lambda}\in\Lambda$
\[
\left\Vert \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\right\Vert \le\frac{1}{\lambda_{min}wJ}\left(\left(K_{1}+w\right)\sqrt{\frac{2}{\lambda_{min}w}C_{\theta^{*},\Lambda}}+K_{0}\right)
\]


\textbf{4. Put all the bounds together}

By the mean value theorem, there is a $\alpha\in(0,1)$ such that\textbf{
\begin{eqnarray*}
\|\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda^{(1)}})-\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda^{(2)}})\| & \le & \left\langle \left.\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\right|_{\lambda=\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)}},\boldsymbol{\lambda^{(1)}}-\boldsymbol{\lambda^{(2)}}\right\rangle \\
 & \le & \max_{\lambda\in\Lambda}\left\Vert \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\right\Vert \left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert \\
 & \le & \frac{1}{\lambda_{min}wJ}\left(\left(K_{1}+w\right)\sqrt{\frac{2}{\lambda_{min}w}C_{\theta^{*},\Lambda}}+K_{0}\right)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert 
\end{eqnarray*}
}

Moreover, if $g(\cdot|\boldsymbol{\theta})$ is $L$-Lipschitz, then
\[
\|g(\cdot|\boldsymbol{\theta}_{1})-g(\cdot|\boldsymbol{\theta}_{2})\|_{\infty}\le L\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|_{2}
\]


So
\begin{eqnarray*}
\|g(\cdot|\boldsymbol{\theta}_{1})-g(\cdot|\boldsymbol{\theta}_{2})\|_{\infty} & \le & L\frac{1}{\lambda_{min}wJ}\left(\left(K_{1}+w\right)\sqrt{\frac{2}{\lambda_{min}w}C_{\theta^{*},\Lambda}}+K_{0}\right)\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\end{eqnarray*}



\section{Additive Model}

The function class of interest are the minimizers of the penalized
least squares criterion: 
\[
\mathcal{G}(T)=\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}^{(j)})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)}):\boldsymbol{\lambda}\in\Lambda\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$.

Suppose that the penalties, functions $g_{j}(x|\boldsymbol{\theta}^{(j)})$
are twice-differentiable wrt $\boldsymbol{\theta}$ and for all $j=1,...,J$
\begin{itemize}
\item $\nabla_{\boldsymbol{\theta}^{(j)}}^{2}P_{j}(\boldsymbol{\theta}^{(j)})$
are PSD matrices for all $j=1,...,J$ (so convex penalties)
\item $g_{j}(x|\boldsymbol{\theta}^{(j)})$ is convex in $\boldsymbol{\theta}^{(j)}$
\item $\nabla_{\boldsymbol{\theta}}^{2}\|y-\sum_{j=1}^{J}g_{j}(x|\boldsymbol{\theta}^{(j)})\|_{T}^{2}$
is a PSD matrix
\item There is a $m>0$ such that the training criterion is $m$-strongly
convex at the minimizer 
\[
\left.\nabla_{\boldsymbol{\theta}}^{2}\left(\|y-\sum_{j=1}^{J}g_{j}(x|\boldsymbol{\theta}^{(j)})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right)\right|_{\theta=\hat{\boldsymbol{\theta}}(\lambda)}\succeq mI
\]

\end{itemize}
Suppose there is a constant $L>0$ such that for all $\boldsymbol{\theta,\theta}'$
and all $j=1,...,J$, we have 
\[
\|g_{j}(\cdot|\boldsymbol{\theta})-g_{j}(\cdot|\boldsymbol{\theta}')\|_{\infty}\le L\|\boldsymbol{\theta}-\boldsymbol{\theta}'\|_{2}
\]


Let 
\[
C_{\theta^{*},\Lambda}=\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}^{(j),*})\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}^{(j),*})+\frac{w}{2}\|\boldsymbol{\theta}^{(j),*}\|_{2}^{2}\right)
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$ 

\[
\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(2)})\right\Vert \le\frac{LJ^{3/2}\sqrt{2C_{\theta^{*},\Lambda}}}{w\lambda_{min}^{2}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\]


and
\begin{eqnarray*}
\left\Vert g\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty} & \le & \frac{L^{2}J^{2}\sqrt{2C_{\theta^{*},\Lambda}}}{w\lambda_{min}^{2}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\end{eqnarray*}



\subsubsection*{Proof}

For simplicity, we write 
\[
g(\cdot|\boldsymbol{\theta})=\sum_{i=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}^{(j)})
\]


and

\[
\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})=\left\{ \hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda})\right\} _{j=1}^{J}
\]


\textbf{1. Calculate $\nabla_{\lambda}\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda})$
using the implicit differentiation trick.}

By the KKT conditions, we have for all $j=1:J$ 
\[
\left.\nabla_{\theta^{(j)}}\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}=0
\]


Now we implicitly differentiate with respect to $\lambda$ 

\[
\nabla_{\lambda}\left\{ \left.\nabla_{\theta^{(j)}}\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right\} =0
\]


By the product rule and chain rule, we have 

\begin{eqnarray*}
\left.\left\{ \sum_{k=1}^{J}\left[\nabla_{\boldsymbol{\theta}^{(k)}}\nabla_{\boldsymbol{\theta}^{(j)}}\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}+1[k=j]\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right]\nabla_{\lambda}\hat{\boldsymbol{\theta}}^{(k)}(\boldsymbol{\lambda})\right\} +\left\{ \begin{array}{ccccccc}
\vec{0} & ... & \vec{0} & \nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)}) & \vec{0} & ... & \vec{0}\end{array}\right\} \right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)} & = & 0
\end{eqnarray*}


Define the following matrices 
\begin{eqnarray*}
S:S_{jk} & = & \left.\nabla_{\boldsymbol{\theta}}^{2}\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}
\end{eqnarray*}


\[
D=\left.diag\left(\left\{ \nabla_{\boldsymbol{\theta}^{(j)}}^{2}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right\} _{j=1}^{J}\right)\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}
\]


\[
M=\left.\left\{ \left[\begin{array}{c}
\vec{0}\\
\nabla_{\theta}P_{j}(\boldsymbol{\theta}^{(j)})\\
\vec{0}
\end{array}\right]\right\} _{j=1}^{J}\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\mbox{(stack side by side)}
\]


We can then combine all the equations into the following system of
equations:
\[
\left(\begin{array}{cccc}
\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{1}(\boldsymbol{\lambda}) & \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{2}(\boldsymbol{\lambda}) & ... & \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{p}(\boldsymbol{\lambda})\end{array}\right)=-M^{\top}\left(S+D\right)^{-1}
\]


\textbf{2. We bound every column in $M$}:

Rearranging the KKT conditions, we have 
\begin{eqnarray*}
\left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)} & = & \frac{1}{2\lambda_{j}}\left.\nabla_{\theta^{(j)}}\left\Vert y-g(\cdot|\boldsymbol{\theta})\right\Vert _{T}^{2}\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\\
 & = & \frac{1}{\lambda_{j}}\left.\left\langle \nabla_{\theta^{(j)}}g_{j}(\cdot|\boldsymbol{\theta}^{(j)}),y-g(\cdot|\boldsymbol{\theta})\right\rangle _{T}\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}
\end{eqnarray*}


Hence
\begin{eqnarray*}
\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert  & \le & \left\Vert \frac{1}{\lambda_{j}}\left.\left\langle \nabla_{\theta^{(j)}}g_{j}(\cdot|\boldsymbol{\theta}^{(j)}),y-g(\cdot|\boldsymbol{\theta})\right\rangle \right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert \\
 & \le & \frac{1}{\lambda_{min}n_{T}}\sum_{i=1}^{n_{T}}\left\Vert \nabla_{\theta^{(j)}}g_{j}(x_{i}|\boldsymbol{\theta}^{(j)})\right\Vert _{2}\left|y-g(x_{i}|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right|\\
 & \le & \frac{1}{\lambda_{min}\sqrt{n_{T}}}\left\Vert y-g(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}\sqrt{\sum_{i=1}^{n_{T}}\left\Vert \nabla_{\theta^{(j)}}g_{j}(x_{i}|\boldsymbol{\theta}^{(j)})\right\Vert _{2}^{2}}
\end{eqnarray*}


We bound $\left\Vert y-g(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}$.
By the definition of $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$,
we have
\begin{eqnarray*}
\frac{1}{2}\left\Vert y-g(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}\left(\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda})\right) & \le & \frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta}^{*})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j),*})\\
 & = & \frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\theta}^{*})\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}P_{j}(\boldsymbol{\theta}^{(j),*})\\
 & = & C_{\theta^{*},\Lambda}
\end{eqnarray*}


To bound $\left\Vert \nabla_{\theta^{(j)}}g_{j}(x_{i}|\boldsymbol{\theta}^{(j)})\right\Vert _{2}^{2}$,
note that since $g_{j}(\cdot|\boldsymbol{\theta}^{(j)})$ is $L$-Lipschitz
with respect to $\|\cdot\|_{\infty}$, we have 
\[
\left\Vert \nabla_{\theta^{(j)}}g_{j}(x|\boldsymbol{\theta}^{(j)})\right\Vert _{2}\le L\mbox{ }\forall x
\]


Hence
\[
\left\Vert y-g(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}\le\sqrt{2C_{\theta^{*},\Lambda}}\mbox{ }
\]


Putting all of this together, we get that for all $j=1,...,J$
\begin{eqnarray*}
\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}+w\boldsymbol{\hat{\theta}}^{(j)}(\boldsymbol{\lambda})\right\Vert  & \le & \frac{L}{\lambda_{min}}\sqrt{2C_{\theta^{*},\Lambda}}
\end{eqnarray*}


\textbf{3. We bound the norm of $\nabla_{\lambda_{k}}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
for all $k=1,...,J$.}

For every $i=1,...,p$, we have
\begin{eqnarray*}
\|\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\| & = & \|M^{\top}\left(S+D\right)^{-1}e_{k}\|\\
 & \le & \sum_{j=1}^{J}\|M_{j}\|_{2}\left\Vert \left(S+D\right)^{-1}\right\Vert _{2}\\
 & = & \sum_{j=1}^{J}\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert _{2}\left\Vert \left(S+D\right)^{-1}\right\Vert _{2}\\
 & \le & J\left(\frac{L}{\lambda_{min}}\sqrt{2C_{\theta^{*},\Lambda}}\right)\frac{1}{m}
\end{eqnarray*}


where we used the fact that $\left(S+D\right)^{-1}\preceq m^{-1}I$

Since the derivative of $\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})$
is bounded, then  by Lemma 2 below, $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
must be Lipschitz:
\[
\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}')\right\Vert _{2}\le\frac{LJ^{3/2}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|_{2}
\]


\textbf{4. Put all the bounds together}

Since each $g_{j}(\cdot|\boldsymbol{\theta}^{(j)})$ is Lipschitz
in $\boldsymbol{\theta}^{(j)}$, then 
\begin{eqnarray*}
\left\Vert g\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty} & \le & \sum_{j=1}^{J}\left\Vert g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda^{(1)}})\right)-g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty}\\
 & \le & \sum_{j=1}^{J}L\|\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda^{(1)}})-\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda^{(2)}})\|_{2}\\
 & \le & L\sqrt{J}\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(2)})\right\Vert _{2}\\
 & \le & \frac{LJ^{2}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\end{eqnarray*}



\section{Nonsmooth Penalties}

Suppose we are dealing with parametric regression problems from Section
1 or 2. We keep all the same assumptions, except those that concern
the smoothness of the penalties. 

Recall that $\Lambda\subseteq\mathbb{R}^{J}$. Consider the measure
space over $\Lambda$ with respect to the Lebesgue measure $\mu$.
We suppose that for a given dataset $\left(X,y\right)$, suppose the
following three assumptions hold:

\textbf{Assumption (1):} Let the penalized training criterion be denoted
$L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})$. Denote the differentiable
space of $L_{T}(\cdot,\boldsymbol{\lambda})$ at any point $\boldsymbol{\theta}$
as 
\[
\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\boldsymbol{\theta}\right)=\left\{ \boldsymbol{\eta}|\lim_{\epsilon\rightarrow0}\frac{L_{T}(\boldsymbol{\theta}+\epsilon\boldsymbol{\eta})-L_{T}(\boldsymbol{\theta})}{\epsilon}\mbox{ exists}\right\} 
\]


Suppose there is a set $\Lambda_{smooth}\subseteq\Lambda$ such that 

\textbf{Cond 1:} For every $\boldsymbol{\lambda}\in\Lambda_{smooth}$,
there exists a ball with nonzero radius centered at $\boldsymbol{\lambda}$,
denoted $B(\boldsymbol{\lambda})$, such that
\begin{itemize}
\item For all $\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})$, the training
criterion $L_{T}(\cdot,\cdot)$ is twice differentiable along directions
in $\Omega^{L_{T}(\cdot,\cdot)}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)$.
(So technically the twice-differentiable space is constant)
\item $\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)$
is a local optimality space of $B(\boldsymbol{\lambda})$:
\[
\arg\min_{\theta\in\Theta}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}'\right)=\arg\min_{\theta\in\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}'\right)\mbox{ }\forall\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})
\]

\end{itemize}
\textbf{Cond 2:} For every $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$,
let the line segment between the two points be denoted 
\[
\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})=\left\{ \alpha\boldsymbol{\lambda^{(1)}}+(1-\alpha)\boldsymbol{\lambda^{(2)}}:\alpha\in[0,1]\right\} 
\]


Suppose the intersection $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\cap\Lambda_{smooth}^{C}$
is countable.

\textbf{Assumption} \textbf{modifications:} Previously we bounded
the derivative of $P_{j}$. Now we only need the bound to apply when
the directional derivative exists. The condition on the derivative
of the penalty is now

\[
\left\Vert \nabla_{\boldsymbol{\theta}}P_{j}\left(\boldsymbol{\theta}\right)\right\Vert _{2}\le K_{1}\|\boldsymbol{\theta}\|_{2}+K_{0}\mbox{ if }\frac{\partial}{\partial m}P_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\mbox{exists}
\]


Under these assumptions, the same Lipschitz conditions hold for dataset
$\left(X,y\right)$ and every $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$.


\subsubsection*{Proof}

Consider any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$.
The length of $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
covered by set $A$ can be expressed as 
\[
\mu_{1}\left(A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right)
\]


where $\mu_{1}$ is the Lebesgue measure over the line segment $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$.
(So if $A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
is just a line segment, it is the length $\|A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\|_{2}$)

By the Differentiability Cover Lemma below, there exists a countable
set of points $\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
such that the union of their ``balls of differentiabilities'' entirely
cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$:
\[
\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}\mu_{1}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\cap\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right)=\left\Vert \mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right\Vert _{2}
\]


Let 
\[
\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}=\left\{ \arg\max_{\left\{ \boldsymbol{\ell}^{(i)}\right\} }\mu_{1}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\cap\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right)\right\} \cup\left\{ \boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right\} 
\]


Let $P$ be the intersections of the boundary of $B\left(\boldsymbol{\ell}_{max}^{(i)}\right)$
with the line segment $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$:
\[
P=\cup_{i=1}^{\infty}\mbox{Bd}B\left(\boldsymbol{\ell}_{max}^{(i)}\right)\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})
\]


Every point $p\in P$ can be expressed as $\alpha_{p}\boldsymbol{\lambda^{(1)}}+(1-\alpha_{p})\boldsymbol{\lambda^{(2)}}$
for some $\alpha_{p}\in[0,1]$. This means we can order these points
$\{\boldsymbol{p}^{(i)}\}_{i=1}^{\infty}$ by increasing $\alpha_{p}$.
By our assumptions, the differentiable space of the training criterion
must be constant over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
(so there might be bad behavior at the endpoints). Let the differentiable
space over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
be denoted $\Omega_{i}$. 

By our assumptions, the differentiable space is also a local optimality
space. Let $U^{(i)}$ be an orthonormal basis of $\Omega_{i}$. For
each $i$, we can express $\hat{\boldsymbol{\theta}}_{\lambda}$ for
all $\boldsymbol{\lambda}\in\mbox{Int}\left\{ \mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)\right\} $
as
\[
\hat{\boldsymbol{\theta}}_{\lambda}=U^{(i)}\hat{\boldsymbol{\beta}}_{\lambda}
\]
\[
\hat{\boldsymbol{\beta}}_{\lambda}=\arg\min_{\beta}L_{T}(U^{(i)}\boldsymbol{\beta},\boldsymbol{\lambda})
\]


Now apply the result in Section 1 or 2 over every line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$.
To do this, we must modify the proofs to take directional derivatives
along the columns of $U^{(i)}$. We can establish that there is a
constant $c>0$ independent of $i$ such that for all $i=1,2...$,
we have 
\[
\left\Vert \boldsymbol{\hat{\beta}}_{p^{(i)}}-\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\right\Vert _{2}\le c\|\boldsymbol{p^{(i)}}-\boldsymbol{p^{(i+1)}}\|_{2}
\]


Finally, we can sum these inequalities. By the triangle inequality,
\begin{eqnarray*}
\left\Vert \boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\right\Vert _{2} & \le & \sum_{i=1}^{\infty}\|\boldsymbol{\hat{\theta}}_{p^{(i)}}-\boldsymbol{\hat{\theta}}_{p^{(i+1)}}\|_{2}\\
 & = & \sum_{i=1}^{\infty}\|U^{(i)}\boldsymbol{\hat{\beta}}_{p^{(i)}}-U^{(i)}\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\|_{2}\\
 & = & \sum_{i=1}^{\infty}\|\boldsymbol{\hat{\beta}}_{p^{(i)}}-\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\|_{2}\\
 & \le & \sum_{i=1}^{\infty}c\|\boldsymbol{p^{(i)}}-\boldsymbol{p^{(i+1)}}\|_{2}\\
 & = & c\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}
\end{eqnarray*}



\subsubsection*{Lemma - Differentiability Cover }

For any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$,
there exists a countable set of points $\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
such that the union of their ``balls of differentiabilities'' entirely
cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
\[
\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\right)=\left\Vert \mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right\Vert 
\]



\subsubsection*{Proof}

We prove this by contradiction. Let 
\[
\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}=\arg\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\right)
\]


and for contradiction, suppose that the covered length is less than
the length of the line segment:
\[
d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right)<\left\Vert \mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right\Vert 
\]


By assumption (2), since $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\cap\Lambda_{smooth}^{C}$
is countable, there must exist a point $p\in\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\backslash\left\{ \cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right\} $
such that $p\notin\Lambda_{smooth}^{C}$. However if we consider the
set of points $\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}\cup\{p\}$,
then 
\[
d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right)<d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\cup B(p)\right)
\]


This is a contradiction of the definition of $\{\boldsymbol{\ell}_{max}^{(i)}\}$.
Therefore we should always be able to cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
with ``balls of differentiability.''


\section{Example}


\subsection{Penalties that satisfy the conditions}

We will show penalties that satisfy the condition
\[
\left\Vert \nabla_{\theta}P(\boldsymbol{\theta})\right\Vert \le K_{1}\|\boldsymbol{\theta}\|_{2}+K_{0}
\]


for constants $K_{0},K_{1}>0$.

\textbf{Ridge:}

The perturbation isn't necessary if there is already a ridge penalty
in the original penalized regression problem. Just set the penalties
$P_{j}(\boldsymbol{\theta})\equiv0$ and fix $w=2$.

\textbf{Lasso:}

\begin{eqnarray*}
\left\Vert \nabla_{\boldsymbol{\theta}}\|\boldsymbol{\theta}\|_{1}\right\Vert  & = & \left\Vert sgn\left(\boldsymbol{\theta}\right)\right\Vert \\
 & \le & p
\end{eqnarray*}


\textbf{Generalized Lasso:} let $G$ be the maximum eigenvalue of
$D$.
\begin{eqnarray*}
\left\Vert \nabla_{\boldsymbol{\theta}}\|D\boldsymbol{\theta}\|_{1}\right\Vert  & = & \left\Vert D^{T}sgn(D\boldsymbol{\theta})\right\Vert \\
 & \le & G\left\Vert sgn(D\boldsymbol{\theta})\right\Vert \\
 & \le & pG
\end{eqnarray*}


\textbf{Group Lasso:}

If we have un-pooled penalty parameters as follows 
\[
\sum_{j=1}^{J}\lambda_{j}\|\boldsymbol{\theta}^{(j)}\|_{2}
\]


then we have the bound 

\begin{eqnarray*}
\left\Vert \nabla_{\boldsymbol{\theta}^{(j)}}\|\boldsymbol{\theta}^{(j)}\|_{2}\right\Vert  & = & \frac{\|\boldsymbol{\theta}^{(j)}\|_{2}}{\|\boldsymbol{\theta}^{(j)}\|_{2}}=1
\end{eqnarray*}


If there is a single penalty parameter for the entire group laso penalty
as follows 
\[
\lambda\sum_{j=1}^{J}\|\boldsymbol{\theta}^{(j)}\|_{2}
\]


then we have the bound 
\begin{eqnarray*}
\left\Vert \nabla_{\boldsymbol{\theta}}\sum_{j=1}^{J}\|\boldsymbol{\theta}^{(j)}\|_{2}\right\Vert  & = & \sqrt{\sum_{j=1}^{J}\left\Vert \nabla_{\boldsymbol{\theta}^{(j)}}\|\boldsymbol{\theta}^{(j)}\|_{2}\right\Vert ^{2}}\\
 & = & \sqrt{\sum_{j=1}^{J}\left(\frac{\|\boldsymbol{\theta}^{(j)}\|_{2}}{\|\boldsymbol{\theta}^{(j)}\|_{2}}\right)^{2}}\\
 & = & J
\end{eqnarray*}



\subsection{Sobolev}

Given a function $h$, the Sobolev penalty for $h$ is

\[
P(h)=\int(h^{(r)}(x))^{2}dx
\]


The Sobolev penalty is used in nonparametric regression models, but
such nonparametric regression models can be re-expressed in parametric
form. We will use this to understand the smoothness of models fitted
in this manner.

Consider the class of smoothing splines 
\[
\left\{ \hat{g}(\cdot|\lambda)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(x_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P(g_{j}):\lambda\in\Lambda\right\} 
\]


Each function $\hat{g}_{j}(\cdot|\lambda)$ is a spline that can be
expressed as the weighted sum of $B$ normalized B-splines of degree
$r+1$ for a given set of knots:
\[
\hat{g}_{j}(x|\lambda)=\sum_{i=1}^{B}\theta_{i}N_{j,i}(x)
\]


Note that the normalized B-splines have the property that they sum
up to one at all points within the boundary of the knots. Also recall
that B-splines are non-negative.

Therefore we can re-express the class of smoothing splines as a set
of function parameters
\[
\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}N_{T,j}\boldsymbol{\theta}_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}_{j}):\lambda\in\Lambda\right\} 
\]
where $N_{T,j}$ is a matrix of the evaluations of the normalized
B-spline basis at $x_{j}$. $P_{j}(\boldsymbol{\theta_{j}})$ is the
Sobolev penalty and can be written as $\boldsymbol{\theta}_{j}^{T}V_{j}\boldsymbol{\theta}_{j}$
for an appropriate penalty matrix $V_{j}$. We will not need to express
anything in terms of $V_{j}$ so the penalty will be just written
as $P_{j}(\boldsymbol{\theta}_{j})$.

We will suppose that the training loss is $m-$strongly convex around
its minimizer.

Let 
\[
C_{\theta^{*},\Lambda}=\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}N_{T,j}\boldsymbol{\theta}_{j}^{*}\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}P_{j}(\boldsymbol{\theta}_{j}^{*})
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have 
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty} & \le & \frac{BJ^{3}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\end{eqnarray*}



\subsubsection*{Proof}

To apply the result from Section 2, we just need note that the $\hat{g}_{j}(x|\boldsymbol{\theta})=\sum_{i=1}^{B}\theta_{i}N_{j,i}(x)$
is $\sqrt{B}$-Lipschitz since $N_{T,j}$ is a normalized B-spline
and 
\[
\sup_{x}N_{j,i}(x)=1
\]
Hence for all $j=1,..,J$
\begin{eqnarray*}
\|\hat{g}_{j}(\cdot|\boldsymbol{\theta})-\hat{g}_{j}(\cdot|\boldsymbol{\theta}^{'})\|_{\infty} & = & \sup_{x}\left|\sum_{i=1}^{B}\left(\theta_{i}-\theta_{i}'\right)N_{j,i}(x)\right|\\
 & = & \left|\sum_{i=1}^{B}|\theta_{i}-\theta_{i}'|\right|\\
 & \le & \sqrt{B}\left\Vert \boldsymbol{\theta}-\boldsymbol{\theta}'\right\Vert _{2}
\end{eqnarray*}


Apply the result from Section 2 to get the result for all $j=1,..,J$
that

\begin{eqnarray*}
\left\Vert g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty} & \le & \frac{BJ^{2}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\end{eqnarray*}


The additive model then has the following Lipschitz bound 
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty} & \le & \sum_{j=1}^{J}\left\Vert g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}})\right)-g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}})\right)\right\Vert _{\infty}\\
 & \le & \frac{BJ^{3}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|
\end{eqnarray*}



\section{Appendix}


\subsubsection*{Lemma lipschitz iff bounded gradient}

Suppose $g$ is convex in $\boldsymbol{\theta}$. 
\[
g(x|\boldsymbol{\theta})\mbox{ is }L\mbox{-Lipschitz}\implies\left\Vert \nabla_{\theta}g(x|\boldsymbol{\theta})\right\Vert _{2}\le\sqrt{p}L
\]


(The other direction can also be proved. https://homes.cs.washington.edu/\textasciitilde{}marcotcr/blog/lipschitz/)


\subsubsection*{Proof}

Let $\boldsymbol{\theta}^{'}-\boldsymbol{\theta}=\arg\max_{\boldsymbol{\beta}}\left\langle \nabla_{\theta}g(x|\boldsymbol{\theta})|_{\theta=\theta'},\boldsymbol{\beta}\right\rangle =\left\Vert \nabla_{\theta}g(x|\boldsymbol{\theta})|_{\theta=\theta'}\right\Vert _{2}$.

Since $g$ is convex in $\theta$, then

\begin{eqnarray*}
g(x|\boldsymbol{\theta})-g(x|\boldsymbol{\theta}^{'}) & \ge & \left\langle \nabla_{\theta}g(x|\boldsymbol{\theta})|_{\theta=\theta'},\boldsymbol{\theta}^{'}-\boldsymbol{\theta}\right\rangle \\
 & = & \left\Vert \nabla_{\theta}g(x|\boldsymbol{\theta})|_{\theta=\theta'}\right\Vert _{2}
\end{eqnarray*}


Also, by the Lipschitz assumption,
\[
\left|g(x|\boldsymbol{\theta})-g(x|\boldsymbol{\theta}^{'})\right|\le L\|\boldsymbol{\theta}^{'}-\boldsymbol{\theta}\|
\]



\subsubsection*{Lemma 2: Bounded gradient implies lipschitz}

Suppose $\Lambda$ is a convex set. If $\|\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})|_{\lambda=\lambda'}\|\le B$
at all $\boldsymbol{\lambda}'$ for all $i=1,...,J$ 

Let 
\[
\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})=\left(\begin{array}{ccc}
\hat{\boldsymbol{\theta}}_{1}(\boldsymbol{\lambda}) & ... & \hat{\boldsymbol{\theta}}_{J}(\boldsymbol{\lambda})\end{array}\right)
\]
Then for all $\boldsymbol{\lambda}\in\Lambda$, we have 
\[
\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}')\right\Vert \le\sqrt{J}B\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|
\]



\subsubsection*{Proof}

By the mean value theorem, there is some $\alpha\in(0,1)$ such that
\begin{eqnarray*}
\left|\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})-\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda}')\right| & = & \left|\left\langle \left.\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\right|_{\lambda=\alpha\lambda+(1-\alpha)\lambda'},\boldsymbol{\lambda}-\boldsymbol{\lambda}'\right\rangle \right|\\
 & \le & \max_{\lambda\in\Lambda}\|\nabla_{\boldsymbol{\lambda}}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\|\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|\\
 & \le & B\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|
\end{eqnarray*}


Hence
\[
\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}')\right\Vert \le\sqrt{J}B\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|
\]

\end{document}
