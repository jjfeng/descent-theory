\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }


%opening
\title{Response to Reviewer 2}

\begin{document}
	
	\maketitle
	
	We appreciate the helpful feedback from the reviewer. We have addressed your questions and comments. Below we give a point-by-point response to each of the questions:
	
	\subsubsection*{Suggestions/comments}
	
	\begin{enumerate}
		\point{
			The authors should specify the distribution of $x_i$'s and also explain how random variables $x_i$ and $\epsilon_i$ are related. Is it assumed that observations are i.i.d. across $i$ and that $\epsilon_i$ is independent of $x_i$? I am fine with both but many researchers consider the latter as rather restrictive because it excludes, for example, the case when $y_i$’s are binary.
		}
	
		\reply{
			We apologize for failing to specify the distributions of $x$ and $\epsilon$.
			For Theorem 1, we establish a concentration inequality where the covariates $X$ are given.
			The only requirement is that $\epsilon_i$ satisfy the condition that they are uniformly sub-Gaussian.

			For Theorem 2 where we consider cross-validation, the dataset is composed of iid observations from the some probability distribution over $(X,y)$ where $X$ is independent of $\epsilon$.
			We have clarified this in Theorem 2.
			In addition, Theorem 4 in the Supplementary Materials is a more general result and does not require $X$ and $\epsilon$ to be independent.
			We have updated the beginning of Section S1.2 in the Supplementary Materials to highlight this difference.
		}
		
		\point {
			The previous comment is important because in the case of cross-validation, the author's derive results with the norm $\| \cdot \|$ being the usual L2-norm, e.g. $\|g\|^2 = \int_0^1 g(x)^2 dx $, but I do not see how this is actually possible if the distribution of $x_i$’s is not specified. What if $x_i$’s can only take two values, 0 or 1?
		}
	
		\reply{
			We apologize for poor notation in the paper.
			We have updated Section 3.2 to clarify that $\|g\|^2 = \int_0^1 g(x)^2 d\mu(x) $ where $\mu$ is the probability measure over $X$.
		}
		
	
		\point{
			In the case of the single splitting procedure, the authors derive results with the norm $\|\cdot\|$ being $\|\cdot\|_V$ , which is defined by $\|g \|_V^2 = n_V^{-1} \sum_{i\in V} g^2(x_i)$ where $V$ is the validating subsample and $n_V$ is the number of observations in $V$ . This norm is problematic because it depends on how the observations are splitted into the training and validating subsamples. It would be much cleaner to use the usual L2-norm, like in the case of the cross-validation procedure. (And that again requires specifying the distribution of $x_i$’s.)
		}
	
		\reply{
			We agree with the reviewer that it would be useful to establish results on the L2-norm rather than the observed covariates.
			However, we have decided to leave Theorem 1 untouched since we value its simplicity -- the current result requires minimal assumptions.
			If we were to modify the result to bound the L2-norm, we would need to add new assumptions regarding the tail behavior of the distributions.
			In addition, we believe our oracle inequality would no longer be sharp.

			On a somewhat related note, the reviewer's comments inspired us to establish more general results for Theorem 2 and 4.
			Our original submission assumed the error terms $\epsilon_i$ and the covariates $X_i$ were uniformly bounded.
			In this new revision, we consider a more general situation with unbounded error and covariates.
			This required us to extend the results in Lecue and Mitchell (2012) -- their results assumed that the entropy of the set of models obtained after the training step can be bounded independent of the training data.
			(For instance, consider a penalized estimation method. If the noise of a particular training dataset is large and the set of penalty parameters under consideration includes very small penalty parameter values, the set of penalized estimators can have a large entropy.)
			To remove this unrealistic assumption, we used a simple chaining argument.
		}

		\point{
			Page 2, lines 43-46: “for a given training dataset T and norm $\|\cdot \|$”. Actually, since cross-validation is used, the norm $\|\cdot \|$ can not be arbitrary as the sentence suggests.
		}

		\reply{
			The reviewer is correct in nothing that $\|\cdot\|$ cannot be an arbitrary norm.
			We have removed that sentence entirely and replaced it with the following:
			\begin{quote}
			If $\Lambda$ is a set of possible hyper-parameters, the goal is to find a penalty parameter $\boldsymbol{\lambda} \in \Lambda$ that minimizes the expected generalization error
			$
			E \left [
			\left ( y - \hat{g}(\boldsymbol{\lambda} | T)(\boldsymbol{x}) \right )^2
			\right ].
			$
		\end{quote}
		
		}

		\point{
			Page 4, lines 24-27: “the additional error from adding a hyper-parameter is roughly equivalent to adding a parameter to the model itself.” This seems a bit inaccurate because, for example, for the lasso estimation, the error only scales like $\log p$ instead of $p$.
		}

		\reply {
			The sentence is indeed inaccurate; we did not take into account the lasso.
			We have replaced the sentence with the following: ``For parametric model-estimation procedures, the additional error from tuning hyper-parameters is roughly $O_p(J/n)$, which is similar to the typical parametric model-estimation rate $O_p(p/n)$ where the model parameters are not regularized.''
		}

		\point{
		Page 6, lines 41-45: “We hypothesize that many model-estimation procedure satisfy this Lipschitz assumption.” The word “hypothesize” seems inappropriate here.
		}

		\reply{
			We agree that ``hypothesize'' does not seem appropriate here.
			We have updated this statement in Section 3 to the following: ``We are interested in studying model-estimation procedures that vary smoothly in their hyper-parameters; such procedures tend to be easier to use and therefore tend to be more popular.''
		}
	
		\point{
			Page 7, line 29: $\|h\|_V = \frac{1}{n_V} \sum_{i \in V} h^2(x_i, y_i)$ should be $\|h\|_V^2 = \frac{1}{n_V} \sum_{i \in V} h^2(x_i, y_i).$
		}
	
		\reply{
			Thank you for pointing out this typo. It is now corrected.
		}
	
		\end{enumerate} 
	
\end{document}
