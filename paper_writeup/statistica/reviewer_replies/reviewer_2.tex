\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }


%opening
\title{Response to Reviewer 2}

\begin{document}
	
	\maketitle
	
	We appreciate the helpful feedback from the reviewer. We have addressed your questions and comments. Below we give a point-by-point response to each of the questions:
	
	\subsubsection*{Suggestions/comments}
	
	\begin{enumerate}
		\point{
			The authors should specify the distribution of $x_i$'s and also explain how random variables $x_i$ and $\epsilon_i$ are related. Is it assumed that observations are i.i.d. across $i$ and that $\epsilon_i$ is independent of $x_i$? I am fine with both but many researchers consider the latter as rather restrictive because it excludes, for example, the case when $y_i$’s are binary.
		}
	
		\reply{
			We apologize for failing to specify the distributions of $x$ and $\epsilon$.
			For Theorem 1 where we consider a training/validation split, we suppose $x$ is given, so it is essentially a fixed design.
			For Theorem 2 where we consider cross-validation, the dataset is composed of iid observations from the some probability distribution over $(X,y)$ where $X$ is independent of $\epsilon$.
			We have clarified this in the paper.
			Theorem 4 in the Supplementary Materials is a more general result and does not require $X$ and $\epsilon$ to be independent.
			We have updated the Supplementary Materials to highlight this difference.
		}
		
		\point {
			The previous comment is important because in the case of cross-validation, the author's derive results with the norm $\| \cdot \|$ being the usual L2-norm, e.g. $\|g\|^2 = \int_0^1 g(x)^2 dx $, but I do not see how this is actually possible if the distribution of $x_i$’s is not specified. What if $x_i$’s can only take two values, 0 or 1?
		}
	
		\reply{
			Yes, typo. should be $\|g\|^2 = E_X[g^2]$
		}
		
	
		\point{
			In the case of the single splitting procedure, the authors derive results with the norm $\|\cdot\|$ being $\|\cdot\|_V$ , which is defined by $\|g \|_V^2 = n_V^{-1} \sum_{i\in V} g^2(x_i)$ where $V$ is the validating subsample and $n_V$ is the number of observations in $V$ . This norm is problematic because it depends on how the observations are splitted into the training and validating subsamples. It would be much cleaner to use the usual L2-norm, like in the case of the cross-validation procedure. (And that again requires specifying the distribution of $x_i$’s.)
		}
	
		\reply{
			We agree with the reviewer that it would be ideal to establish results on the L2-norm rather than the observed covariates.
			However, we have decided to leave Theorem 1 about a single training/validation split untouched since we think it is valuable to have an oracle inequality that requires minimal assumptions and is sharp.
			In this latest revision, we make a small extension to Lecue and Mitchell (2012) and establish an oracle inequality regarding the L2-norm under more general conditions.
			Unfortunately we were unable to loosen the regularity condition required in their theorem.
			In order to control the expected risk of the fitted model, our proof techniques and that in Lecue and Mitchell (2012) require that the tail behavior of the estimated functions is totally bounded for all hyper-parameters $\boldsymbol{\lambda}$ in some set $\Lambda$.
			This is unrealistic, say, in penalized regression problems as $\Lambda$ can include very small penalty parameter values.
			On the other hand, this assumption is not necessary in Theorem 1 where we only conclude the model's error over the covariates in the validation set.
		}

		\point{
			Page 2, lines 43-46: “for a given training dataset T and norm $\|\cdot \|$”. Actually, since cross-validation is used, the norm $\|\cdot \|$ can not be arbitrary as the sentence suggests.
		}

		\reply{
			The reviewer is correct in nothing that a norm cannot be used here.
			We have removed that sentence entirely and replaced it with the following:
			\begin{quote}
			If there is a set of possible hyper-parameters $\Lambda$, then the goal is to find a penalty parameter $\boldsymbol{\lambda} \in \Lambda$ that minimizes the expected generalization error
			$
			E \left [
			\left ( y - \hat{g}(\boldsymbol{\lambda} | T)(\boldsymbol{x}) \right )^2
			\right ].
			$
		\end{quote}
		
		}

		\point{
			Page 4, lines 24-27: “the additional error from adding a hyper-parameter is roughly equivalent to adding a parameter to the model itself.” This seems a bit inaccurate because, for example, for the lasso estimation, the error only scales like $\log p$ instead of $p$.
		}

		\reply {
			The sentence is indeed inaccurate; we did not take into account the lasso.
			We have replaced the sentence with the following: ``For parametric model-estimation procedures, the additional error from tuning hyper-parameters is roughly $O_p(J/n)$, which is similar to the typical parametric model-estimation rate $O_p(p/n)$ where the model parameters are not regularized.''
		}

		\point{
		Page 6, lines 41-45: “We hypothesize that many model-estimation procedure satisfy this Lipschitz assumption.” The word “hypothesize” seems inappropriate here.
		}

		\reply{
			We agree that ``hypothesize'' does not seem appropriate here.
			We have updated that sentence in the paper with the following: ``We believe that many model-estimation procedures are Lipschitz – such procedures are well-behaved in their hyper-parameters so they tend to be easier to use and thereby more popular.''
		}
	
		\point{
			Page 7, line 29: $\|h\|_V = \frac{1}{n_V} \sum_{i \in V} h^2(x_i, y_i)$ should be $\|h\|_V^2 = \frac{1}{n_V} \sum_{i \in V} h^2(x_i, y_i).$
		}
	
		\reply{
			We apologize for this typo. It is now corrected.
		}
	
		\end{enumerate} 
	
\end{document}
