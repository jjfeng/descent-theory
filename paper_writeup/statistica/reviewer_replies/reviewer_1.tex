\documentclass[]{article}
\usepackage{amsmath}
\usepackage{color}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\newcommand{\overall}[1]{\textcolor{blue}{#1}}

\newcommand{\point}[1]{\item \textcolor{blue}{#1}}
\newcommand{\reply}{\item[]\ }

%opening
\title{Response to Reviewer 1}

\begin{document}
	
	\maketitle
	
	We appreciate the helpful feedback from the reviewer.
	We have made major modifications to the paper to address your comments and questions.
	Below we give a point-by-point response:
	
	\subsubsection*{Specific Suggestions/comments}
	
	\begin{enumerate}
		\point{
			The key assumption is the C-Lipschitz condition of the penalized criterion function on the tuning parameters.  This is a high level condition and the authors try to illustrate it using some low-level sufficient conditions.
			In the current version, it is not clear whether these low-level conditions can be verified in specific examples, and they are sufficient for the main results of the paper.
			...
			Since the authors consider the smooth and non-smooth penalty functions in the paper, it would be good if the can use two specific examples to illustrate the low-level conditions in Lemma 1, Lemma 2 and Lemma 3. For example, they may consider the penalty function using the Sobolev norm for smooth case, and the Lasso penalty for the non-smooth case.
		}
		\reply{
		We have added four in-depth examples to Section 4 of the paper to illustrate settings where the low-level conditions in Lemmas 1 and 2 are satisfied.
		We check that the conditions in Lemmas 1 and 2 are satisfied and then apply the oracle inequalities to provide an end-to-end analysis of these penalized regression examples.

		For Lemma 1 which applies to smooth penalty functions, we analyze a problem with multiple ridge penalties.
		In addition, we consider a generalized additive model which involves multiple Sobolev penalties.
		We transform the problem to its finite-dimensional equivalent, show that the Lipschitz condition is satisfied, and establish the oracle inequality for the training/validation split setting.

		For Lemma 2 which applies to nonsmooth penalty functions, we analyze the elastic net in both the training/validation split and cross-validation settings.

		Finally, for Lemma 3 which nonparametric additive models, we briefly mention that a problem with ridge penalties on the observed covariates is sufficient to satisfy the low-level conditions.

		}
		
		\point{
			For example, in Lemma 2 of the paper, the authors introduced the subset of the tuning parameters such that the C-Lipschitz condition holds in the case that the penalty function is not smooth.
			Does this mean that one has to replace the original set by this subset in practice since it is not clear whether the C-Lipschitz condition holds outside this subset?
			Is this subset known or it has to be estimated?
		}
		\reply{
		We agree with the reviewer that Lemma 2 in the original submission was not useful in practice since its result was restricted to penalty parameters where the fitted functions were smoothly parameterized.
		In order to make Lemma 2 more applicable, we have modified the assumptions and updated our result in Section 4.1.2.
		In this new revision, Lemma 2 states that for almost every pair of penalty parameters, the Lipschitz condition is satisfied.
		Thus we no longer restrict the penalty parameters to the set where the fitted functions were smooth.
		We believe this new version is applicable to many scenarios.
		For instance, we use Lemma 2 to establish results about the elastic net.
		}
		
		\point{
		The notations of the paper are slightly messy.
		}
	
		\reply{
			We apologize for the messy notation. We have made extensive efforts to clean up the notation throughout the entire paper.
		}
	
		\point{
			In Theorem 2 and Theorem 3, are the expectation operators taken under the empirical measure?
		}
		\reply{
			As there is no expectation operation in Theorem 3, we believe the reviewer means Theorem 2 and 4.
			We have added a note in the paper to clarify that the expectation operators in Theorems 2 and 4 treat datasets $D^{(m)}$ as a random variable.
			Thus it is not an empirical measure but rather the probability distribution of $m$ iid draws from the probability distribution of $(X,Y)$.
		}
		\point{
			In displays (4.26) and (4.32), does “I” stand for the identity matrix? What does the inequality symbol in these two displays mean?
		}
	
		\reply{
			As the reviewer noted, $I$ indeed stands for the identity matrix.
			In our paper, the inequality symbol $\succeq$ states that $A \succeq B$ for two matrices $A$ and $B$, then $A - B$ is a positive semi-definite matrix.
			We have clarified these two points of confusion in the paper.
		}
	\end{enumerate} 
	
\end{document}
