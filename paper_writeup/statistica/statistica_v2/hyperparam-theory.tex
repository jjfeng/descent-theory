\documentclass[12pt]{article} %***
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
%\usepackage[]{hyperref}  %<----modified by Ivan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{sectsty, secdot}
%\sectionfont{\fontsize{12}{15}\selectfont}
\sectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
\renewcommand{\theequation}{\thesection\arabic{equation}}
\subsectionfont{\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=36pc
\textheight=49pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{color}
\usepackage{cite}

\usepackage{xr}
\externaldocument{hyperparam-theory-appendix}


\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assump}{Assumption}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\spann}{span}
\newcommand{\textred}[1]{\textcolor{red}{#1}}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}
%\headrulewidth=0pt  %<-modified by Ivan

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cfoot{\thepage}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{2}

\markright{ \hbox{\footnotesize\rm Statistica Sinica
%{\footnotesize\bf 24} (201?), 000-000
}\hfill\\[-13pt]
\hbox{\footnotesize\rm
%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
}\hfill }

\markboth{\hfill{\footnotesize\rm Jean Feng and Noah Simon} \hfill}
{\hfill {\footnotesize\rm Hyper-parameter selection via split-sample validation} \hfill}

\renewcommand{\thefootnote}{}
$\ $\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont \vspace{0.8pc}
\centerline{\large\bf An analysis of the cost of hyper-parameter selection via split-}
\vspace{2pt} \centerline{\large\bf sample validation, with applications to penalized regression}
\vspace{.4cm} \centerline{Jean Feng, Noah Simon} \vspace{.4cm} \centerline{\it
Department of Biostatistics, University of Washington} \vspace{.55cm} \fontsize{9}{11.5pt plus.8pt minus
.6pt}\selectfont

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{quotation}
\noindent {\it Abstract:}
In the regression setting, given a set of hyper-parameters, a model-estimation procedure constructs a model from training data. The optimal hyper-parameters that minimize generalization error of the model are usually unknown. In practice they are often estimated using split-sample validation. Up to now, there is an open question regarding how the generalization error of the selected model grows with the number of hyper-parameters to be estimated. To answer this question, we establish finite-sample oracle inequalities for selection based on a single training/test split and based on cross-validation. We show that if the model-estimation procedures are smoothly parameterized by the hyper-parameters, the error incurred from tuning hyper-parameters shrinks at nearly a parametric rate. Hence for semi- and non-parametric model-estimation procedures with a fixed number of hyper-parameters, this additional error is negligible. For parametric model-estimation procedures, adding a hyper-parameter is roughly equivalent to adding a parameter to the model itself. In addition, we specialize these ideas for penalized regression problems with multiple penalty parameters. We establish that the fitted models are Lipschitz in the penalty parameters and thus our oracle inequalities apply. This result encourages development of regularization methods with many penalty parameters.
\vspace{9pt}

\noindent {\it Key words and phrases:}
Cross-validation, Regression, Regularization.
\par
\end{quotation}\par

\def\thefigure{\arabic{figure}}
\def\thetable{\arabic{table}}

\renewcommand{\theequation}{\thesection.\arabic{equation}}



\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

\setcounter{section}{1} %***
\setcounter{equation}{0} %-1

\section{Introduction}

%JF wow this paper's english can really use some level-upping. some paragraphs also don't flow.
%JF clean up notation - bold symbols appropriately too!
% sort out T vs D^{(n_T)}

Per the usual regression framework, suppose we observe response $y \in \mathbb{R}$ and predictors $\boldsymbol {x} \in \mathbb{R}^p$. Suppose $y$ is generated by a true model $g^*$ plus random error $\epsilon$ with mean zero, as follows
$y = g^*(\boldsymbol x) + \epsilon$.
Our goal is to estimate $g^*$.
Many model-estimation procedures can be formulated as selecting a model from some function class $\mathcal{G}$ given training data $T$ and $J$-dimensional hyper-parameter vector $\boldsymbol{\lambda}$. For example, in penalized regression problems, the fitted model can be expressed as the minimizer of the penalized training criterion
\begin{equation}
\label{eq:intro_pen_reg}
\hat{g}(\boldsymbol \lambda | T) = \argmin_{g\in \mathcal{G}} \sum_{(\boldsymbol{x}_i, y_i) \in T} \left (y_i -  g(\boldsymbol{x}_i) \right )^2 + \sum_{j=1}^J \lambda_j P_j(g),
\end{equation}
%JF:  I would like to divide by number of obs in T
where $P_j$ are penalty functions and $\lambda_j$ are penalty parameters. As suggested by the notation in \eqref{eq:intro_pen_reg}, the penalty parameters are the hyper-parameters in this model-estimation procedure.

%JF english, man...
Given a set of possible hyper-parameters $\Lambda$, a training dataset $T$, and norm $\|\cdot\|$,
our goal is to find some hyper-parameter $\hat{\boldsymbol{\lambda}}$ that minimizes the expected generalization error.
Typically one uses a sample-splitting procedure where models are trained on a random partition of the observed data and evaluated on the remaining data.
The final hyper-parameters $\hat{\boldsymbol{\lambda}}$ are that which minimizes the error on this validation set.
For a more complete review of cross-validation, refer to \citet{arlot2010survey}.

The performance of split-sample validation procedures is typically characterized by an oracle inequality that bounds the generalization error of the expected model selected from the validation set procedure. For $\Lambda$ that are finite, oracle inequalities have been established for a single training/validation split \citep{gyorfi2006distribution} and a general cross-validation framework \citep{van2003unified, van2004asymptotic}. To handle $\Lambda$ over a continuous range, one can use entropy-based approaches \citep{lecue2012oracle}.
%JF: I would like to add reasons for why we want to think about a continuous range because people don't typically think about this.

The goal of this paper is to characterize the performance of models when the hyper-parameters are tuned by some split-sample validation procedure. We are particularly interested in an open question raised in \citet{bengio2000gradient}: what is the ``amount of overfitting... when too many hyper-parameters are optimized''? In addition, how many hyper-parameters is ``too many''? In this paper we show that actually a large number of hyper-parameters can be tuned without overfitting. In fact, if an oracle estimator converges at rate $R(n)$, then the number of hyper parameters $J$ can grow at roughly a rate of $J = O_p(nR(n))$ up to log terms without affecting the convergence rate. In practice, for penalized regression, this means that one can propose and tune over much more complex models than are currently often used.

To show these results, we prove that finite-sample oracle inequalities of the form
\begin{equation}
\label{thrm:intro_oracle_ineq}
\left \| g^* - \hat{g} (\hat{\boldsymbol{\lambda}} | T ) \right \|^2
\le
(1+a)
\underbrace{\inf_{\lambda \in \Lambda} \left \| g^* - \hat{g}\left (\boldsymbol{\lambda} \middle | T \right ) \right \|^2}_{\text{Oracle risk}}
+ \delta \left(J,n\right)
\end{equation}
%JF: Is review2 referring to this?!
are satisfied with high probability for norm $\| \cdot \|$, constant $a \ge 0$, and remainder $\delta(J,n)$ that depends on the number of tuned hyper-parameters $J$ and the number of samples $n$.
Under the assumption that the model -estimation procedure is Lipschitz in the hyper-parameters, we find that $\delta$ scales linearly in $J$.
% be careful here as some reviewer wants to argue log J also happens when you add parameters to the model itself
For parametric model-estimation procedures, the additional error from adding a hyper-parameter is roughly equivalent to adding a parameter to the model itself.
For semi- and non-parametric model-estimation procedures, this error is generally dominated by the oracle risk so we can actually grow the number of hyper-parameters without affecting the asymptotic convergence rate.

In this paper, we also specialize these results to penalized regression models of the form \eqref{eq:intro_pen_reg}.
We show that the fitted models are indeed Lipschitz in the penalty parameters so our oracle inequalities apply. 
Again, we find that additional penalty parameters only add a near-parametric error term, which has a negligible effect in semi- and non-parametric settings. This result suggests that the recent interest in combining penalty functions (e.g. elastic net and sparse group lasso \citep{zou2003regression, simon2013sparse}) may have artificially restricted themselves to two-way combinations. Adding more penalties may lead to better models.

During our literature search, we found few theoretical results relating the number of hyper-parameters to the generalization error of the selected model. 
Much of the previous work only considered tuning a one-dimensional hyper-parameter over a finite $\Lambda$, proving asymptotic optimality \citep{van2004asymptotic} and finite-sample oracle inequalities \citep{van2003unified, gyorfi2006distribution}. Others have addressed split-sample validation for specific penalized regression problems with a single penalty parameter, such as linear model selection \citep{li1987asymptotic, shao1997asymptotic, golub1979generalized, chetverikov2016cross, chatterjee2015prediction}.
Only the results in \citet{lecue2012oracle} are relevant to answering our question of interest. A potential reason for this dearth of literature is that, historically, tuning multiple hyper-parameters has been computationally difficult. However, there have been many proposals recently for overcoming this computational hurdle \citep{bengio2000gradient, foo2008efficient, snoek2012practical}.

Section \ref{sec:main_results} presents oracle inequalities for sample-splitting procedures to understand how the number of hyper-parameters affects the model error.
Section \ref{sec:examples} applies these results to penalized regression models.
Section \ref{sec:simulations} provides a simulation study to support our theoretical results.
Oracle inequalities for general model-estimation procedures and proofs are given in the Appendix.



\section{Oracle Inequalities} \label{sec:main_results}

In this section, we establish oracle inequalities for models where the hyper-parameters are tuned by a single training/validation split and cross-validation. We first introduce some notation and formalize the model-estimation procedure. 

Let $D^{(n)}$ denote a dataset with $n$ samples. The model-estimation procedure $\hat{g}$ is a set of functions $\{\hat{g}^{(n_T)}\}$ where $\hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)})$ maps hyper-parameter $\boldsymbol{\lambda} \in \Lambda \subseteq \mathbb{R}^J$ and training data $D^{(n_T)}$ to a function in $\mathcal{G}$.
We focus on model-estimation procedures that satisfy the following Lipschitz assumption:
\begin{assump}
	\label{assump:lipschitz}
	There is a function $C_\Lambda$ such that for any $n_T \in \mathbb{N}$, dataset $D^{(n_T)}$, and $\boldsymbol{\lambda}^{(1)}, \boldsymbol{\lambda}^{(2)} \in \Lambda$, we have
	\begin{align}
	\left |
	\hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(\boldsymbol{x}) - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(\boldsymbol{x}) \right |
	\le C_\Lambda(\boldsymbol{x}|D^{(n_T)}) \|\boldsymbol{\lambda}^{(1)} - \boldsymbol{\lambda}^{(2)}\|_2.
	\end{align}
\end{assump}
%JF: reviewer said reword - is this better?
\noindent
We believe that many model-estimation procedures are Lipschitz -- such procedures are well-behaved in their hyper-parameters so they tend to be easier to use and thereby more popular.
For instance, Section \ref{sec:examples} shows that penalized regression models satisfy this assumption.
From this Lipschitz assumption, we find that the error from tuning multiple hyper-parameters shrinks at roughly a parametric rate of $O_p(J/n_V)$.
%JF removing sentences cause it seems repetitive
%Hence for semi- and non-parametric model-estimation procedures, the error from tuning a fixed number of hyper-parameters is negligible.
%Moreover, we can specify an upper bound on the rate at which the number of hyper-parameters can grow without asymptotically increasing the generalization error of the selected model.

\subsection{A Single Training/Validation Split}\label{sec:single}

In the training/validation split procedure, the dataset $D^{(n)}$ is randomly partitioned into a training set $T = (X_T, Y_T)$ and validation set $V = (X_V, Y_V)$ with $n_T$ and $n_V$ observations, respectively.
The selected hyper-parameter $\hat{\boldsymbol{\lambda}}$ is a minimizer of the validation loss
\begin{equation}
\label{eq:train_val_lambda}
\hat{\boldsymbol \lambda} \in \argmin_{\boldsymbol{\lambda} \in\Lambda} \frac{1}{2} \left \| y-\hat{g}^{(n_T)}( \boldsymbol \lambda | T) \right \|_{V}^{2}
\end{equation}
where $\| h \|^2_{V} \coloneqq \frac{1}{n_V}\sum_{(x_i, y_i)\in V} h^2(x_i, y_i)$ for function $h$.

We now present a finite-sample oracle inequality for the single training/validation split assuming Assumption~\ref{assump:lipschitz} holds.
Our oracle inequality is sharp, i.e. $a=0$ in \eqref{thrm:intro_oracle_ineq}, unlike most other work \citep{gyorfi2006distribution, lecue2012oracle, van2003unified}.
Note that the result below is a special case of Theorem \ref{thrm:train_val_complicated} in Appendix \ref{appendix:train_val}, which applies to general model-estimation procedures.
\begin{theorem}
	\label{thrm:train_val}
	Let $\Lambda=[\lambda_{\min},\lambda_{\max}]^{J}$ where $\Delta_{\lambda} = \lambda_{\max} - \lambda_{\min} \ge 0$.
	Suppose independent random variables $\epsilon_i$ associated with the validation data $(x_i, y_i) \in V$ have expectation zero and are uniformly sub-Gaussian with parameters $b$ and $B$:
	$$
	\max_{i: (x_i, y_i) \in V} B^2 \left ( \mathbb{E} e^{|\epsilon_i|^2/B^2} - 1 \right ) \le b^2.
	$$
%	Let training data $T$ and the covariates of the validation set $X_V$ be fixed.
	Let the oracle risk be denoted
	\begin{equation}
	\tilde{R}(X_V|T) = \argmin_{\lambda \in \Lambda} \left \| g^*-\hat{g}^{(n_T)}( \boldsymbol{\lambda} | T) \right \|_{V}^{2}.
	\label{eq:tilde_lambda_def}
	\end{equation}
	Suppose Assumption~\ref{assump:lipschitz} is satisfied.
	Then there is a constant $c>0$ only depending on $b$ and $B$ such that for all $\delta$ satisfying
	\begin{equation}
	\delta^{2}
	\ge
	c \left (
	\frac{J \log(\|C_\Lambda(\cdot |T)\|_V \Delta_{\Lambda} n + 1)}{n_{V}}
	\vee 
	\sqrt{\frac{J \log(\|C_\Lambda(\cdot |T)\|_V \Delta_{\Lambda} n + 1)}{n_{V}}
		\tilde{R}(X_V|T)}
	\right )
	\label{thrm:train_val_delta}
	\end{equation}
	we have
	\begin{align}
	Pr\left(
	\left\Vert g^* - \hat{g}^{(n_T)}( \hat{\boldsymbol{\lambda}} | T) \right\Vert _{V}^2 -
	\tilde{R}(X_V|T)
	\ge\delta^2
	\middle | 
	T, X_V
	\right )
	&\le c\exp\left(-\frac{n_{V}\delta^{4}}{c^{2} \tilde{R}(X_V|T)}\right)
	+ c\exp\left(-\frac{n_{V}\delta^{2}}{c^{2}}\right).
	\end{align}
	
\end{theorem}
\noindent
Theorem \ref{thrm:train_val} states that with high probability, the excess risk, e.g. the error incurred during the hyper-parameter selection process, is no more than $\delta^2$.
As seen in \eqref{thrm:train_val_delta}, $\delta^2$ is the maximum of two terms: a near-parametric term and the geometric mean of the near-parametric term and the oracle risk. To see this more clearly, we express Theorem \ref{thrm:train_val} using asymptotic notation.
\begin{corollary}
	\label{corr:train_val}
	Under the assumptions given in Theorem \ref{thrm:train_val}, we have
	\begin{align}
	\left\Vert g^* - \hat{g}^{(n_T)}( \hat{\boldsymbol{\lambda}} | T) \right\Vert _{V}^2 &
	\le \min_{\lambda \in \Lambda} \left\Vert g^* - \hat{g}^{(n_T)}( {\boldsymbol{\lambda}} | T) \right \Vert^2_{V}
	\label{eq:asym_train_val_oracle_risk}
	\\
	& + O_p \left(\frac{J\log (n \|C_\Lambda\|_V \Delta_{\Lambda} )}{n_{V}} \right) 
	\label{eq:asym_train_val_theorem1} \\
	& + O_p \left(
	\sqrt{
		\frac{J \log (n \|C_\Lambda\|_V \Delta_{\Lambda} )}{n_{V}}
		\min_{\lambda \in \Lambda} \left\Vert g^* - \hat{g}^{(n_T)}( {\boldsymbol{\lambda}} | T) \right \Vert^2_{V}
	}
	\right ).
	\label{eq:asym_train_val_theorem2}
	\end{align}
\end{corollary}
\noindent
Corollary \ref{corr:train_val} show that the risk of the selected model is bounded by the oracle risk, the near-parameteric term \eqref{eq:asym_train_val_theorem1}, and the geometric mean of the two values \eqref{eq:asym_train_val_theorem2}.
We refer to \eqref{eq:asym_train_val_theorem1} as near-parametric because the error term in (un-regularized) parametric regression models is typically $O_p(J/n)$, where $J$ is the parameter dimension and $n$ is the number of training samples. Analogously, \eqref{eq:asym_train_val_theorem1} is $O_p(J/n_V)$ modulo a $\log n$ term in the numerator.
The geometric mean \eqref{eq:asym_train_val_theorem2} can be thought of as a consequence of tuning hyper-parameters over
\begin{equation}
\mathcal{G}(T) = \left \{ \hat{g}^{(n_T)}( {\boldsymbol{\lambda}}| T) : \boldsymbol{\lambda} \in \Lambda \right \}.
\end{equation}
As $\mathcal{G}(T)$ does not (or is very unlikely to) contain the true model $g^*$, tuning the hyper-parameters via training/validation split is tuning over a the misspecified model class.
The geometric mean takes into account this misspecification error.

In the semi- and non-parametric regression settings, the oracle error usually shrinks at a rate of $O_p(n_T^{-\omega})$ where $\omega \in (0, 1)$.
If the number of hyper-parameters is fixed and $n$ is large, the oracle risk will tend to dominate the upper bound.
Hence for such problems, we can actually let the number of hyper-parameters grow -- the asymptotic convergence rate of the upper bound will be unchanged as long as $J$ grows no faster than
$
O_p\left (
\frac{n_{V} n_T^{-\omega}}{\log (n \|C_\Lambda\|_V \Delta_{\Lambda})}
\right ).
$

\subsection{Cross-Validation}\label{sec:cv}

Now we give an oracle inequality for $K$-fold cross-validation.
%JF this needs a rewrite.
Previously, the oracle inequality was with respect to the $L_2$-norm over the validation covariates.
Now we give our result with respect to the functional $L_2$-norm.
Let $\mu$ be the probability distribution of $(X,Y)$.
The functional $L_2$-norm is defined as
$
\left \| h \right \|^2_{L_2} = \int \left |h(x) \right |^2 d\mu(x)
$.

For $K$-fold cross-validation, we randomly partition the dataset $D^{(n)}$ into $K$ sets, which we assume to have equal size for simplicity. Partition $k$ will be denoted $D_k^{(n_V)}$ and its complement will be denoted $D_{-k}^{(n_T)} = D^{(n)} \setminus D_k^{(n_V)}$. We train our model using $D_{-k}^{(n_T)}$ for $k=1,...,K$ and select the hyper-parameter that minimizes the average validation loss
\begin{eqnarray}
\label{kfold_opt}
\hat{\boldsymbol \lambda} &=& \argmin_{\boldsymbol{\lambda} \in\Lambda} \frac{1}{K} \sum_{k=1}^K  \left \| y-\hat{g}^{(n_T)}(\boldsymbol \lambda | D_{-k}^{(n_T)}) \right \|_{D_k^{(n_V)}}^{2}.
\end{eqnarray}

In traditional cross-validation, the final model is retrained on all the data with $\hat{\boldsymbol{\lambda}}$. However bounding the generalization error of the retrained model requires additional regularity assumptions \citep{lecue2012oracle}. We consider the ``averaged version of $K$-fold cross-validation'' instead
\begin{equation}
\label{thrm:avg_cv}
\bar{g}\left ( \hat{\boldsymbol \lambda} \middle | {D^{(n)}} \right ) = 
\frac{1}{K} \sum_{k=1}^K 
\hat{g}^{(n_T)} \left (\hat{\boldsymbol \lambda} \middle | D^{(n_T)}_{-k} \right ).
\end{equation}
To bound the generalization error of \eqref{thrm:avg_cv}, we require an assumption in \citet{lecue2012oracle} that controls the tail behavior of the fitted models.
\begin{assump}
	\label{assump:tail_margin}
	There exist constants $K_0, K_1 \ge 0$ and $\kappa \ge 1$ such that for any $n_T \in \mathbb{N}$, dataset $D^{(n_T)}$, and $\boldsymbol{\lambda} \in \Lambda$, we have
	\begin{align}
	\left \|
	g^* (x) - \hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)})(x)
	\right \|_{L_{\psi_2}} & \le K_0
	\label{eq:cv_assump1}\\
	\left \|
	\left(
	y - \hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)})(x)
	\right)^2
	- \left(
	y - g^*(x)
	\right)^2
	\right \|_{L_2}
	& \le 
	K_1 \left \|
	g^{*}-\hat{g}(\boldsymbol{\lambda}|D^{(n_{T})})
	\right \|_{L_{2}}^{1/\kappa}.
	\label{eq:cv_assump2}
	\end{align}
	where $\|f\|_{L_{\psi_2}}$ is the Orlicz norm
	$\|f\|_{L_{\psi_2}} = \inf \{C > 0: \mathbb{P}\exp(|f|^2/C^2) - 1 \le 1\}$.
\end{assump}

With the above assumption, the following oracle inequality bounds the risk of averaged version of $K$-fold cross-validation.
It is a special case of Theorem~\ref{thrm:jean_cv} in the Appendix, which extends Theorem 3.5 in \citet{lecue2012oracle}.
\begin{theorem}
	\label{thrm:kfold}
	Let $\Lambda=[\lambda_{\min},\lambda_{\max}]^{J}$ where $\Delta_{\Lambda} = (\lambda_{\max} - \lambda_{\min}) \vee 1$.
	%JF do I really need the \vee above?
	Suppose random variables $\epsilon_i$ are independent with expectation zero, satisfy $\|\epsilon\|_{L_{\psi_2}}= b <\infty$, and are independent of $X$.
	Suppose Assumptions~\ref{assump:lipschitz} and \ref{assump:tail_margin} hold.
	Suppose there exists a function $\tilde{h}$ and some $\sigma_0 > 0$ such that
	\begin{align}
	\tilde{h}(n_{T})
	\ge
	1 + \sum_{k=1}^{\infty}
	k\Pr\left(\|C_\Lambda(\cdot |D^{(n_{T})})\|_{L_{\psi_{2}}}\ge2^{k}\sigma_{0}\right).
	\end{align}
	Then there exists an absolute constant $c_{1}>0$ and a constant $c_{K_0, b}>0$ such that for any $a > 0$,
	\begin{align}
	\begin{split}
	\mathbb{P}_{D^{(n)}}\left(
	\|
	\bar{g}(\hat{\boldsymbol{\lambda}}|D^{(n)})(x)
	-g^{*}(x)
	\|_{L_{2}}^{2}\right)
	& \le	(1+a)
	\inf_{\lambda\in\Lambda}
	\left[\mathbb{P}_{D^{(n_{T})}}\left(\|\hat{g}(\boldsymbol{\lambda}|D^{(n_{T})})(x)-g^{*}(x)\|_{L_{2}}^{2}\right)\right] \\
	& +
	c_{1}
	\left (\frac{1+a}{a} \right )^2
	\frac{J\log n_{V}}{n_{V}}
	K_0
	\left[\log\left(\Delta_{\Lambda} c_{K_0, b} n \sigma_0 +1\right)+1\right]
	\tilde{h}(n_{T}).
	\end{split}
	\label{eq:cv_lipschitz_oracle_ineq}
	\end{align}
\end{theorem}

Like in Theorem \ref{thrm:train_val}, Theorem~\ref{thrm:kfold} bounds the generalization error of the fitted model using a near-parametric term $O_p(J/n_V)$.
So as before, adding hyper-parameters to parametric model estimation incurs a similar cost as adding parameters to the parametric model itself and adding hyper-parameters to semi- and non-parametric regression settings is relatively ``cheap" and negligible asymptotically.

%JF english...
There are also some notable differences between Theorems \ref{thrm:train_val} and \ref{thrm:kfold} that highlight the tradeoffs made in the two theorems.
Theorem~\ref{thrm:kfold} tries to make a statement about the functional $L_2$-error instead of the risk over the validation covariates.
We pay the price in two ways.
First, we need to add Assumption~\ref{assump:tail_margin}, which is quite strict since it must hold for all datasets $D$ and hyper-parameters $\Lambda$.
The former requirement can be relaxed so that Assumption~\ref{assump:tail_margin} holds with high probability.
However the latter requirement is problematic -- it is hard to control the tail behavior for all $\Lambda$ unless $\lambda_{\min}$ is shrinking at an appropriate rate.
If $\lambda_{\min}$ shrinks too fast (in the number of samples), $K_0$ may grow with $n$ and the remainder term in \eqref{eq:cv_lipschitz_oracle_ineq} will no longer shrink at a parametric rate.
Unfortunately requiring $\lambda_{\min}$ to shrink at an appropriate rate seems to defeat the purpose of cross-validation.
Thus this assumption is crucial for the proof, but seems insufficient to fully understand cross-validation.
The second price we pay is that we no longer have a sharp oracle inequality since the oracle error is scaled by $1+a$ where $a > 0$.

\section{Penalized regression models}
\label{sec:examples}
Now we apply our results to analyze penalized regression procedures of the form \eqref{eq:intro_pen_reg}.
Penalty functions encourage particular characteristics in the fitted models (e.g. smoothness or sparsity) and combining multiple penalty functions results in models that exhibit a combination of the desired characteristics. 
Naturally, there is a much interest in combining multiple penalty functions, such as the elastic net or sparse group lasso.
However few popular methods incorporate more than two penalties due to (a) the concern that models may overfit the data when selection of many penalty parameters is required; and (b) computational issues in optimizing multiple penalty parameters. In this section, we evaluate the validity of concern (a) using the results of Section~\ref{sec:main_results}. We see that, contrary to popular wisdom, using split-sample validation to select multiple penalty parameters should not result in a drastic increase to the generalization error of the selected model. For computational concerns (b), we refer the reader to recent papers on hyper-parameter estimation \citep{bengio2000gradient, foo2008efficient, snoek2012practical}.

Recall that in our framework, the model-estimation procedure tunes the penalty parameters over some set $\Lambda$. As long as the fitted models are Lipschitz in the penalty parameters over $\Lambda$, we can apply Theorems \ref{thrm:train_val} and \ref{thrm:kfold}.
We will consider the penalty parameter space
\begin{equation}
\label{thrm:lambda_range}
\Lambda = [ n^{-t_{\min}}, n^{t_{\max}}]^J
\end{equation}
for sufficiently large $t_{\min}, t_{\max} \ge 0$.
This regime works well for two reasons: one, our rates depend only quite weakly on $t_{\min}$ and $t_{\max}$; and two, generally oracle $\lambda$-values are $\sim n^{-\alpha}$ for some $\alpha \in (0,1)$ \citep{van2000empirical, van2014additive, buhlmann2011statistics}. So long as $t_{\min} > \alpha$, $\Lambda$ will contain the optimal penalty parameter.
We require $\lambda_{\min}$ to shrink at a polynomial rate in $n$ as the fitted models can be ill-behaved if $\lambda_{\min}$ approaches 0 too quickly.



% JF not sure why this paragraph is here. seems to be repeative
%Therefore we reach the same conclusion for the relationship between penalty parameters and the generalization error of the selected model. In high-dimensional and non-parametric penalized regression problems that satisfy the Lipschitz assumption, the error from tuning penalty parameters is negligible compared to the error from solving the penalized regression problem with the oracle penalty parameters.

In the following sections, we will do an in-depth study of additive models, which have the form
\begin{equation}
g(x_1, ..., x_J)= \sum_{j=1}^J g_j(x_j).
\end{equation}
We first consider parametric additive models (with potentially growing numbers of parameters) fitted with smooth and non-smooth penalties and then nonparametric additive models.
We find that the Lipschitz function $C_\Lambda(x | T)$ scales with $n^{O_p(t_{\min})}$; so applying the above theorems, we find that the remainder term grows only linearly with $t_{\min}$ for both training-validation split and cross-validation.

\subsection{Parametric additive models}
\label{sec:param_add_models}
Parametric additive models have the form
\begin{equation}
g(\boldsymbol{\theta}^{(1)}, ..., \boldsymbol{\theta}^{(J)})(x) = \sum_{j=1}^J g_j(\boldsymbol{\theta}^{(j)})(x)
\end{equation}
where $\boldsymbol{\theta}^{(j)} \in \mathbb{R}^{p_j}$ and $p = \sum_{j=1}^J p_j$. The number of parameters $p$ is allowed to grow with $n$, as commonly done in sieve estimation. For simplicity, let the full parameter vector be denoted $\boldsymbol{\theta} = \left (\boldsymbol{\theta}^{(1)}, ..., \boldsymbol{\theta}^{(J)} \right )^\top$. Then we can write the training criterion for training data $T$ as
\begin{equation}
\label{eq:param_add}
L_T \left (\boldsymbol{\theta}, \boldsymbol{\lambda} \right) 
\coloneqq \frac{1}{2} \left  \| y -  g(\boldsymbol{\theta}) \right \|^2_T 
+ \sum_{j=1}^J \lambda_j P_j(\boldsymbol{\theta}^{(j)}).
\end{equation}
Suppose $\boldsymbol{\theta}^*$ is the unique minimizer of the expected generalization error
\begin{equation}
\boldsymbol{\theta}^*
= \argmin_{\theta \in \mathbb{R}^p} E \left \| y - g(\boldsymbol{\theta}) \right\|^2_{L_2}.
\end{equation}

\subsubsection{Parametric regression with smooth penalties}
\label{sec:param_smooth}
We begin with the simple case where the penalty functions are smooth. The following lemma states that the fitted models are Lipschitz in the penalty parameter vector.
Given matrices $A$ and $B$, $A \succeq B$ means that $A - B$ is a positive semi-definite matrix.
\begin{lemma}
	\label{lemma:param_add}
	Let $\Lambda \coloneqq \left [ \lambda_{\min}, \lambda_{\max} \right ]^J$ where $\lambda_{\max} \ge \lambda_{\min} > 0$.
	For a fixed training dataset $T \equiv D^{(n_T)}$, suppose for all $\boldsymbol{\lambda} \in \Lambda$, $L_T \left (\boldsymbol{\theta}, \boldsymbol{\lambda} \right)$ has a unique minimizer
	\begin{equation}
	\label{eq:param_add_estimator}
	\left\{
	\hat{\boldsymbol{\theta}}^{(j)}\left (\boldsymbol{\lambda} | T \right )
	\right\}_{j=1}^J =
	\argmin_{\boldsymbol{\theta} \in \mathbb{R}^p} L_T \left (\boldsymbol{\theta}, \boldsymbol{\lambda} \right).
	\end{equation}
	Suppose for all $j = 1,...,J$, the parametric class $g_j$ is $\ell_j$-Lipschitz in its parameters
	\begin{align}
	\left|
	g_j (\boldsymbol{\theta}^{(1)} )(\boldsymbol{x})
	-g_j (\boldsymbol{\theta}^{(2)} )(\boldsymbol{x})
	\right|
	\le
	\ell_j (\boldsymbol{x})
	\|\boldsymbol{\theta}^{(1)}-\boldsymbol{\theta}^{(2)}\|_{2}.
	\end{align}
	Further suppose for all $j=1,..,J$, $P_j(\boldsymbol{\theta}^{(j)})$ and $g_j(\boldsymbol{\theta}^{(j)})(\boldsymbol{x})$ are twice-differentiable with respect to $\boldsymbol{\theta}^{(j)}$ for any fixed $\boldsymbol{x}$.
	Suppose there exists an $m(T) > 0$ such that the Hessian of the penalized training criterion at the minimizer satisfies
	\begin{equation}
	\left . \nabla_{\theta}^2 L_T \left (\boldsymbol{\theta}, \boldsymbol{\lambda} \right) \right |_{\theta = \hat{\theta}(\boldsymbol{\lambda} | T )} \succeq m(T) \boldsymbol{I}
	\quad \forall \boldsymbol{\lambda} \in \Lambda,
	\label{eq:smooth_pos_def}
	\end{equation}
	where $I$ is a $p \times p$ identity matrix.
	Then for any $\boldsymbol{\lambda}^{(1)}, \boldsymbol{\lambda}^{(2)} \in \Lambda$,
	Assumption~\ref{assump:lipschitz} is satisfied with
	%JF update this to match appendix!
	\begin{equation}
	\label{eq:param_add_lipschitz}
	C_\Lambda(\boldsymbol{x} | T) =
	\frac{1}{m(T) \lambda_{min}}
	\sqrt{
		\left(
		\left\Vert \epsilon \right \Vert_T^2 + 2 C^*_{\Lambda}
		\right)
		\left(
		\sum_{j=1}^J  \|\ell_j\|_T^2 \ell_j^2(x_j)
		\right)
	}
	\end{equation}
	where $C^*_{\Lambda} = \lambda_{max}\sum_{j=1}^{J} P_{j}(\boldsymbol{\theta}^{(j),*})$.
\end{lemma}

%JF lol this is so out of place
Notice that the result assumes that the training criterion is strongly convex at its minimizer. If this is not true, one can augment the penalty function $P_j(\boldsymbol{\theta}^{(j)})$ with a ridge penalty $\frac{w}{2}\| \boldsymbol{\theta}^{(j)} \|_2^2$ with a fixed $w > 0$ so that the training criterion becomes
\begin{equation}
\label{eq:param_add_models_ridge}
\frac{1}{2} \left  \| y -  g(\boldsymbol{\theta}) \right \|^2_T 
+ \sum_{j=1}^J \lambda_j \left ( P_j(\boldsymbol{\theta}^{(j)}) + \frac{w}{2} \| \boldsymbol{\theta}^{(j)} \|^2_2 \right ).
\end{equation}
We now consider a simple example with multiple ridge penalties.
For all the examples, let the penalty parameter space be $\Lambda = \left [n^{- t_{\min}}, 1 \right ]^J$.
\begin{example}[Multiple ridge penalties]
	Suppose we would like to fit a linear model and want to use ridge regression to stabilize our estimates.
	In addition, suppose we have prior information that groups covariates based on the similarity of their effects on the response, e.g. $\boldsymbol{x} = (\boldsymbol{x}^{(1)}, ... , \boldsymbol{x}^{(J)})$ where $\boldsymbol{x}^{(j)}$ is a vector of length $p_j$.
	For example, each group of covariates could be measurements from a different source or instrument.
	To incorporate this prior knowledge, we may want to penalize each group of covariates differently using the following objective:
	\begin{align}
	L_T \left (\boldsymbol{\theta}, \boldsymbol{\lambda} \right) 
	\coloneqq 
	\frac{1}{2}
	\sum_{i=1}^n
	\left(y_i -  \sum_{j=1}^J \boldsymbol{x}_{i}^{(j)} \boldsymbol{\theta}^{(j)} \right )^2
	+ \sum_{j=1}^J \frac{\lambda_j}{2} \|\boldsymbol{\theta}^{(j)}\|_2^2.
	\end{align}
	Since we do not know what penalty parameter values to use, we then use a split-sample procedure to tune $\boldsymbol{\lambda}$.

	We analyze the performance from tuning the penalty parameters via training-validation split, with training set $T$ and validation set $V$.
	Since we are considering linear functions, the function $g_j(\boldsymbol{\theta}^{(j)})(\boldsymbol{x}^{(j)})$ is $\ell_j$-Lipschitz where $\ell_j(\boldsymbol{x}^{(j)}) = \|\boldsymbol{x}^{(j)}\|_2$.
	Then by Lemma~\ref{lemma:param_add}, the fitted function $g(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))(\boldsymbol{x})$ is $C_\Lambda$-Lipschitz where
	\begin{align}
	C_\Lambda \left ( \boldsymbol{x} | T \right ) =
	n^{2t_{\min}}
	\sqrt{
		C^*_{T}
		\left(
		\sum_{j = 1}^J
		\|\boldsymbol{x}^{(j)}\|_2^2
		\left \| \|\boldsymbol{x}^{(j)}_i \|_2^2 \right \|_T^2
		\right)
	}
	\end{align}
	where $
	C^*_{T} = 
	\|\epsilon\|_{T}^{2}
	+ \sum_{j=1}^J \|\boldsymbol{\theta}^{*,(j)}\|_2^2
	.$
	So by Corollary~\ref{corr:train_val}, the parametric term in the remainder term \eqref{eq:asym_train_val_theorem1} is of the form
	\begin{align}
	\frac{J t_{\min}}{n_{V}}
	\log \left (
	C^*_T n
	\sum_{j = 1}^J \left \|\|\boldsymbol{x}^{(j)} \|_2 \right \|_T^2 \left \|\|\boldsymbol{x}^{(j)}\|_2 \right \|_V^2
	\right ).
	\end{align}
	As mentioned before, we indeed find that the remainder term scales with the logarithm of $\lambda_{\min}$, or in other words, it is proportional to $t_{\min}$.
\end{example}

In the next example, we consider generalized additive models (GAMs) \citep{hastie1990generalized}.
Though GAMs are nonparametric models, it is well-known that the procedure is equivalent to solving a finite-dimensional problem that has a closed-form solution \citep{green1993nonparametric, o1986automatic, buja1989linear}.
In the following example, we start from the closed-form solution to determine the Lipschitz function $C_\Lambda$ and then apply Theorem~\ref{thrm:train_val} to derive an oracle inequality for training-validation split procedures.
\begin{example}[Multiple sobolev penalties]
	To fit a generalized additive model over the domain $[0,1]^J$, a typical setup is to minimize the following objective
	\begin{align}
	\argmin_{\alpha_0 \in \mathbb{R}, g_j}
	\frac{1}{2} \sum_{i\in D^{(n_T)}}
	\left(
	y_i - \alpha_0 - \sum_{j=1}^J g_j(x_{ij})
	\right)^2
	+ \sum_{j=1}^{J} \lambda_j \int_{\mathcal{X}} \left(g_j^{''}(x)\right)^{2} dx
	\label{eq:smoothing_spline}
	\end{align}
	where the penalty function is the Sobolev norm.
	For this example, we will suppose $\mathcal{X} = [0,1]^p$.
	Since the solution to \eqref{eq:smoothing_spline} must be the sum of natural cubic splines \citep{buja1989linear}, we can parameterize the functions $g_j$ by its linear component and an orthogonal component in a Reproducing Kernel Hilbert Space with reproducing kernel $R$:
	\begin{align}
	g_j(x_j) = \alpha_{1j} x_j + \sum_{i=1}^n \theta_{ij} R(x_{ij}, x_j).
	\end{align}
	So we can re-express \eqref{eq:smoothing_spline} as the finite-dimensional problem
	\begin{align}
	\hat{\alpha_0}(\boldsymbol{\lambda}),
	\hat{\boldsymbol{\alpha}_1}(\boldsymbol{\lambda}),
	\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})
	& = \argmin_{\alpha_0, \boldsymbol{\alpha}_1, \boldsymbol{\theta}}
	\frac{1}{2}
	\left \|
	\boldsymbol{y} -
	\alpha_0 \boldsymbol{1}
	- X \boldsymbol{\alpha}_1
	- K \boldsymbol{\theta}
	\right \|^2_T
	+
	\frac{1}{2}
	\boldsymbol{\theta}^\top
	\diag \left (
	\left \{
	\lambda_j K_j
	\right \} \right ) \boldsymbol{\theta}.
	\label{eq:matrix_sobolev}
	\end{align}
	From the KKT conditions, we can derive a closed-form solution to \eqref{eq:matrix_sobolev}, from which we can determine that the fitted functions are $C_\Lambda$-Lipschitz where
	\begin{align}
	C_\Lambda(\boldsymbol{x} | T)
	=
	\left(
	p
	\left \|
	\left(
	X^\top X
	\right)^{-1}
	X^\top
	\right \|_2
	+
	\sum_{j=1}^J \frac{c }{h_j(T)^2}
	\right)
	\sqrt{J}
	n^{2\kappa + 1}
	\|\boldsymbol{y}\|_T
	\label{eq:sobolev_lipschitz}
	\end{align}
	where $h_j(T)$ is the minimum distance between the $j$th covariates in the training data (assuming $X^\top X$ is invertible).
	Then by Corollary~\ref{corr:train_val}, we find that the parametric term in the remainder is
	\begin{align}
	\frac{J t_{\min}}{n_{V}} \log \left (
	n
	J
	\|\boldsymbol{y}_T\|_2
	\left(
	p
	\left \|
	\left(
	X^\top X
	\right)^{-1}
	X^\top
	\right \|_2
	+
	\sum_{j=1}^J h_j^{-2}(T)
	\right)
	\right ).
	\label{eq:sobolev_param}
	\end{align}
	For the full derivation, refer to the Appendix.
\end{example}

\subsubsection{Parametric regression with non-smooth penalties}
\label{sec:param_nonsmooth}

If the penalty functions are non-smooth, similar results do not necessarily hold. Nonetheless we find that for many popular non-smooth penalty functions, such as the lasso \citep{tibshirani1996regression} and group lasso \citep{yuan2006model}, the fitted functions are still smoothly parameterized by $\boldsymbol \lambda$ almost everywhere.
To characterize such problems, we begin with the following definitions from \citet{feng2017gradient}:

\begin{definition}
	The differentiable space of function $f:\mathbb{R}^p \mapsto \mathbb{R}$ at $\boldsymbol{\theta}$ is
	\begin{equation}
	\Omega^{f}(\boldsymbol{\theta}) = \left \{ \boldsymbol{\beta} \middle | \lim_{\epsilon \rightarrow 0} \frac{f(\boldsymbol{\theta} + \epsilon \boldsymbol{\beta}) - f(\boldsymbol{\theta})}{\epsilon} \text{ exists } \right \}.
	\end{equation}
\end{definition}

\begin{definition}
	% JF unique minimizer over what set? R^p?
	Let $f(\cdot, \cdot): \mathbb{R}^p \times \mathbb{R}^J \mapsto \mathbb{R}$ be a function with a unique minimizer.
	$S \subseteq \mathbb{R}^p$ is a local optimality space of $f$ over $W \subseteq \mathbb{R}^J$ if
	\begin{equation}
	\argmin_{\boldsymbol{\theta} \in \mathbb{R}^p} f(\boldsymbol{\theta}, \boldsymbol \lambda) =
	\argmin_{\boldsymbol{\theta} \in S} f(\boldsymbol{\theta}, \boldsymbol \lambda) \quad \forall \boldsymbol \lambda \in W.
	\end{equation}
\end{definition}
%JF clarify this sentence
We also need to characterize the subset of penalty parameters $\Lambda_{smooth} \subseteq \Lambda$ where the fitted functions are well-behaved.
In particular, we need $\Lambda_{smooth}$ to satisfy the following conditions:
\begin{condition}
	\label{condn:nonsmooth1}
	For every $\boldsymbol{\lambda} \in \Lambda_{smooth}$, there exists a ball $B(\boldsymbol{\lambda})$ with nonzero radius centered at $\boldsymbol{\lambda}$ such that
	\begin{itemize}
		\item For all $\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})$, the training criterion $L_{T}(\cdot, \boldsymbol{\lambda}')$ is twice differentiable with respect to $\boldsymbol{\theta}$ at $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'|T)$
		along directions in the product space
		\begin{align}
%		\begin{split}
		\Omega^{L_T(\cdot, \boldsymbol{\lambda})} \left (\hat{\boldsymbol \theta}\left(\boldsymbol{\lambda}|T \right) \right) =
		& \Omega^{P_1(\cdot)}
			\left(\hat{\boldsymbol{\theta}}^{(1)}(\boldsymbol{\lambda} | T)\right)
		\times
%		\Omega^{P_2(\cdot)}
%		\left(\hat{\boldsymbol{\theta}}^{(2)}(\boldsymbol{\lambda} | T)\right)
%		\times
		...
		\times
		\Omega^{P_J(\cdot)}
		\left(\hat{\boldsymbol{\theta}}^{(J)}(\boldsymbol{\lambda} | T)\right)
		.
%		\end{split}
		\end{align}
		\item $\Omega^{L_T(\cdot, \boldsymbol{\lambda})} \left (\hat{\boldsymbol \theta}\left(\boldsymbol{\lambda}|T \right) \right)$ is a local optimality space for $L_T\left(\cdot,\boldsymbol{\lambda}\right)$ over $B(\boldsymbol{\lambda})$.
	\end{itemize}
\end{condition}
\begin{condition}
	\label{condn:nonsmooth2}
	$\Lambda \setminus \Lambda_{smooth}$ has Lebesgue measure zero: $\mu(\Lambda_{smooth}^c) = 0$.
\end{condition}

Equipped with these conditions, we can characterize the smoothness of the fitted functions when the penalties are non-smooth. In fact the Lipschitz constant is exactly the same as that in Lemma \ref{lemma:param_add}.

\begin{lemma}
	\label{lemma:nonsmooth}
	Let $\Lambda \coloneqq \left [ \lambda_{\min}, \lambda_{\max} \right ]^J$ where $\lambda_{\max} \ge \lambda_{\min} > 0$.
	Suppose that $g_j(\boldsymbol{\theta})(x)$ satisfies the Lipschitz condition in Lemma~\ref{lemma:param_add}.
	Suppose for training data $T\equiv D^{(n_{T})}$, the penalized loss function $L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)$
	has a unique minimizer $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$
	for every $\boldsymbol{\lambda}\in\Lambda$.
	Let $\boldsymbol{U}_{\lambda}$
	be an orthonormal matrix with columns forming a basis for the differentiable
	space of $L_{T}(\cdot,\boldsymbol{\lambda})$ at $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$.
	Suppose there exists a constant $m(T)>0$ such that the Hessian of
	the penalized training criterion at the minimizer taken with respect
	to the directions in $\boldsymbol{U}_{\lambda}$ satisfies 
	\begin{equation}
	\left._{U_{\lambda}}\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})\right|_{\theta=\hat{\theta}(\boldsymbol{\lambda})}\succeq m(T)\boldsymbol{I}\quad\forall\boldsymbol{\lambda}\in\Lambda
	\end{equation}
	where \textup{$\boldsymbol{I}$ is the identity matrix.}
	Suppose Conditions~\ref{condn:nonsmooth1} and \ref{condn:nonsmooth2} are satisfied.
	Then any $\boldsymbol{\lambda}^{(1)}, \boldsymbol{\lambda}^{(2)} \in \Lambda$ satisfies Assumption~\ref{assump:lipschitz} with $C_\Lambda$ defined in \eqref{eq:param_add_lipschitz}.
\end{lemma}

We now consider an example with the elastic net \citep{zou2003regression}.

\begin{example}[Multiple elastic nets, training/validation split]
	\label{ex:elastic_net_tv}
	As in the ridge penalty example, suppose we are interested in fitting a linear model and would like to use the elastic net.
	If we have groups of covariates, we can penalize each group differently by employing multiple penalty parameters as follows
	\begin{align}
	\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})
	=\argmin_{\theta^{(j)} \in \mathbb{R}^{p_j}, j = 1,...,J}
	\frac{1}{2} \left \| y - \sum_{j=1}^J \boldsymbol{X}^{(j)} \boldsymbol{\theta}^{(j)} \right \|_T^2
	+ \sum_{j=1}^J \lambda_j \left(
	\| \boldsymbol{\theta}^{(j)}\|_1
	+ \frac{w}{2} \| \boldsymbol{\theta}^{(j)}\|_2^2
	\right)
	\label{eq:elastic_net_ex}
	\end{align}
	where $w$ as a fixed parameter.
	Here we establish an oracle inequality when the penalty parameters are tuned over $\Lambda$ via training/validation split.
	$\Lambda$ is defined the same as that in the ridge example.
	
	For each $j = 1,...,J$, let the nonzero indices of $\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda})$ be denoted $I^{(j)}(\boldsymbol\lambda) = \{i | \hat{\theta}_i(\boldsymbol\lambda) \ne 0 \text{ for } i=1,...,p_j \}$ and let $\boldsymbol I_{I^{(j)}(\boldsymbol \lambda)}$ be a submatrix of the identity matrix with columns $I^{(j)}(\boldsymbol\lambda)$.
	Condition~\ref{condn:nonsmooth1} is satisfied with differentiable space
	\begin{align}
	\Omega^{L_T(\cdot, \lambda)}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda} | T))
	= \spann(I_{I^{(1)}(\boldsymbol \lambda)}) \times ... \times \spann(I_{I^{(J)}(\boldsymbol \lambda)}).
	\label{eq:en_diff_space}
	\end{align}
	Condition~\ref{condn:nonsmooth2} is also satisfied since elastic net solution paths are piecewise linear \citep{zou2003regression}.
	In addition, the Hessian of the penalized training loss has a minimum eigenvalue $m(T) \ge \lambda_{\min}$.

	From Lemma~\ref{lemma:nonsmooth}, we have that the function $g(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda} |T))(\boldsymbol{x})$ is $C_\Lambda$-Lipschitz where
	\begin{align}
	C_\Lambda \left ( \boldsymbol{x} | T \right ) =
	\frac{n^{2t_{\min}}}{w}
	\sqrt{
	C^*_T
	\sum_{j = 1}^J
	\left \|
	\|\boldsymbol{x}^{(j)} \|_2
	\right \|_T^2
	\|\boldsymbol{x}^{(j)}\|_2^2
	}
	\label{eq:elastic_lipschitz}
	\end{align}
	where
	$
	C^*_T =
	\|\epsilon\|_{T}^{2}
	+\sum_{j=1}^J
	2 \|\boldsymbol{\theta}^{*,(j)}\|_1
	+ w\|\boldsymbol{\theta}^{*,(j)}\|_2^2
	$.
	Then by Theorem~\ref{thrm:train_val}, the parametric term in the remainder has the form
	\begin{align}
	\frac{J t_{\min}}{n_{V}}
	\log \left (
	C^*_T
	w^{-1} n
	\sum_{j=1}^J
	\left \| \|\boldsymbol{x}^{(j)}\|_2 \right \|_T^2 \left \| \|\boldsymbol{x}^{(j)}\|_2 \right \|_V^2
	\right ).
	\end{align}
\end{example}

\begin{example}[Multiple elastic nets, cross-validation]
	%JF check the name of the CV method.
	Now we would like to establish an oracle inequality for average modified cross-validation.
	We consider a similar setup as that in \citet{lecue2012oracle}.
	For simplicity, suppose $X$ is drawn uniformly from $[-R, R]^p$ for some $R > 0$.
	Then $X$ is a log-concave vector and
	$
	\sup_{\|a\|_\infty = 1} \left\| X^\top a \right \|_{L_{\psi_2}} < c_R < \infty
	$
	for some constant $c_R > 0$.
	In addition, suppose the noise $\epsilon$ is also sub-gaussian.
	As shown in \citet{lecue2012oracle}, \eqref{eq:cv_assump2} is satisfied.
	To satisfy \eqref{eq:cv_assump1} in Assumption~\ref{assump:tail_margin}, our fitting procedure for $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ entails a thresholding operation reminiscent of that in \citet{lecue2012oracle}.
	In particular, the fitted parameters are
	\begin{align}
	\hat{{\theta}}_{thres, i}(\boldsymbol{\lambda})
	= \hat{{\theta}}_{i}(\boldsymbol{\lambda}) \wedge K_0'
	\quad i = 1,...,p
	\end{align}
	where $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ is the solution to \eqref{eq:elastic_net_ex}.
	Since we are thresholding the fitted parameters, Assumption~\ref{assump:tail_margin} is satisfied:
	\begin{align}
	\left\|
	X^\top (\boldsymbol{\theta} - \boldsymbol{\theta}^*)
	\right \|_{L_{\psi_2}}
	\le
	(\|\boldsymbol{\theta}^*\|_\infty + K_0')
	c_R.
	\end{align}
	The fitted models $\hat{\boldsymbol{\theta}}_{thres}(\boldsymbol{\lambda})$ are $C_\Lambda$-Lipschitz with the constant function
	\begin{align}
	C_\Lambda(\boldsymbol{x} | D^{(n_T)})
	=
	\frac{n^{2t_{\min}}}{w}
	R^2 \sqrt{
		J p
		\left(
		\|\epsilon\|_{T}^{2}
		+\sum_{j=1}^J
		2 \|\boldsymbol{\theta}^{*,(j)}\|_1
		+ w\|\boldsymbol{\theta}^{*,(j)}\|_2^2
		\right)
	}.
	\label{eq:elastic_lipschitz_cv}
	\end{align}
	
	Now we are ready to apply Theorem~\ref{thrm:kfold}.
	Notice that
	$\left \| C_\Lambda(\cdot | D^{(n_T)}) \right \|_{L_{\psi_2}}^2$
	is a sub-exponential random variable.
	Thus we can choose $\sigma_0 = O_p(n^{4t_{\min}}R^4Jp/w^2 )$ so that
	\begin{align}
	1 + \sum_{k = 1}^\infty k \Pr \left (
	\| C_\Lambda(\cdot | D^{(n_T)}) \|_{L_{\psi_2}}
	\ge 2^k \sigma_0
	\right )
	& \le
	1 + \sum_{k = 1}^\infty k \Pr \left (
	\| \epsilon\|_T^2 \ge
	2^{2k}
	\right )\\
%	& \le
%	1 + \sum_{k = 1}^\infty c_0 k
%	\exp \left (
%	- c_0 \frac{2^{2k} n_T}{\|\epsilon\|^2_{L_{\psi_{2}}}}
%	\right ) \\
	& \le
	1 + c_1
	\exp \left (
	- \frac{c_0 n_T}{\|\epsilon\|^2_{L_{\psi_{2}}}}
	\right )
	\end{align}
	for absolute constants $c_0, c_1 > 0$.
	Then for any $a > 0$, we have for a constant $\tilde{c} > 0$
	\begin{align}
	\begin{split}
	\mathbb{P}_{D^{(n)}}
	\left \|
	X \left(
	\bar{\boldsymbol{\theta}}(\hat{\boldsymbol{\lambda}}|D^{(n)})
	- \boldsymbol{\theta}^*
	\right)
	\right \|_{L_{2}}^{2}
	& \le	(1+a)
	\inf_{\lambda\in\Lambda}
	\left[
	\mathbb{P}_{D^{(n_{T})}}
	\left \|
	X \left(
	\bar{\boldsymbol{\theta}}(\hat{\boldsymbol{\lambda}}|D^{(n_T)})
	- \boldsymbol{\theta}^*
	\right)
	\right \|_{L_{2}}^{2}
	\right] \\
	& \quad +
	\tilde{c}
	\left (\frac{1+a}{a} \right )^2
	\frac{J \log n_{V}}{n_{V}}
	R t_{\min}
	\log\left(
	\frac{1+a}{aw} RJpn
	\right).
	\end{split}
	\end{align}
	
	
	In contrast to the lasso example in \citet{lecue2012oracle}, we establish oracle inequalities when the hyper-parameters are tuned over a continuous range.
	This is particularly valuable when $J$ is large and $\boldsymbol{\lambda}$ must be tuned via a continuous optimization procedure like some hill-climbing procedure or Bayesian optimization.
	Unfortunately we were only able to consider thresholded fitted parameters as Assumption~\ref{assump:tail_margin} requires estimators to have a bounded $\psi_2$ norm. We hope to address this in future research.
\end{example}

\subsection{Nonparametric additive models}
\label{sec:nonparam_smooth}

We now generalize the results to nonparametric additive models. We consider estimators of the form
\begin{align}
\label{eq:train_crit_nonparam}
\left\{ \hat{g}_j( \boldsymbol \lambda) \right \}_{j=1}^J &= 
\argmin_{g_j\in \mathcal{G}_j: j=1,...,J}  L_T\left (\left \{ g_j \right \}_{j=1}^J, \boldsymbol{\lambda} \right ) \\
\text{where} \quad L_T \left (\left \{ g_j \right \}_{j=1}^J, \boldsymbol{\lambda} \right ) &=
\frac{1}{2} \left \| \boldsymbol y -  \sum_{j=1}^J g_j(\boldsymbol x_j) \right \|^2_T 
+ \sum_{j=1}^J \lambda_j P_j(g_j)
\end{align}
where $P_j$ are now penalty functionals. For some problems, e.g. smoothing splines, we can show that the solution to~\eqref{eq:train_crit_nonparam} lies in a parametric family (of growing dimension); and use the results of Sections~\ref{sec:param_smooth}, and \ref{sec:param_nonsmooth} to give our Lipschitz bounds. However, this is not always possible. Nevertheless we can still obtain similar results in the non-parametric problem. The following lemma states that the fitted functions are Lipschitz with respect to $\| \cdot \|_{D^{(n)}}$. Let $\left\{ g_j^* \right \}_{j=1}^J$ be the minimizer of the generalization error
\begin{equation}
\left\{ g_j^* \right \}_{j=1}^J = \argmin_{g_j \in \mathcal{G}_j: j=1,...,J} E_T\left[ \left \| y - \sum_{j=1}^J g_j^* \right \|^2 \right].
\end{equation}

\begin{lemma}
	\label{lemma:nonparam_smooth}
	Suppose $\mathcal{G}_1, ..., \mathcal{G}_J$ be linear spaces of univariate functions.
	
	Suppose the penalty functions $P_{j}$ are twice Gateaux differentiable and convex over $\mathcal{G}_j$.
	%JF again, are you sure need be convex?
	Suppose there is a $m > 0$ such that for all $j=1,...,J$, the second Gateaux derivative of the training criterion at $\hat{g}^{(n_T)}_j( \boldsymbol{\lambda} | T)$ satisfies
	\begin{equation}
	\left \langle 
	\left . D^2_{g_j} L_T \left ( \left \{ g_j \right \}_{j=1}^J, \boldsymbol{\lambda} \right ) \right |_{g_j= \hat{g}_j( \boldsymbol{\lambda} | T) }
	\circ h_j, h_j
	\right \rangle 
	\ge m
	\quad \forall h_j \in \mathcal{G}_j,  \|h_j \|_{D^{(n)}} = 1
	\label{eq:gateuax}
	\end{equation}
	%JF for all lambda.
	%JF this implies that the penalty function must contain the values at the validation point!
	where $D^2_{g_j}$ is the second Gateaux derivative where both derivatives are taken in the direction of $g_j$.
	%JF should not be "in the direction". should be "by wrt g_j"
	
	Let $\lambda_{\max} > \lambda_{\min} > 0 $. Let
	\begin{equation}
	C_{\Lambda}^*=
	\frac{1}{2}\left\Vert y- \sum_{j=1}^J g^*_j\right\Vert _{T}^{2}
	+\lambda_{max}\sum_{j=1}^{J} P_{j}(g^*_j).
	\end{equation}
	
	For any $\boldsymbol{\lambda}^{(1)}, \boldsymbol{\lambda}^{(2)} \in \Lambda \coloneqq \left [ \lambda_{\min}, \lambda_{\max} \right ]^J$, we have
	\begin{align}
	\label{eq:nonparam_lipshitz_thrm}
	\left\Vert 
	\sum_{j=1}^J \hat{g}_j\left(\boldsymbol{\lambda}^{(1)} |T \right)-\hat{g}_j\left(\boldsymbol{\lambda}^{(2)} |T \right)\right\Vert _{D^{(n)}} & \le
	\frac{J}{m \lambda_{min}}\sqrt{2 C_{\Lambda}^* \frac{n}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
	\left \|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)} \right \|.
	\end{align}
\end{lemma}

The requirement that \eqref{eq:gateuax} holds implies that the values of the nonparametric model must appear in the penalty.
Note that this is allowed -- we don't actually give the $y$ values for the validation covariates.

Note that \eqref{eq:nonparam_lipshitz_thrm} is over $D^{(n)}$, even though we just need it to be $V$.
To transform, we can easily relate to two quantities.

\section{Simulations}\label{sec:simulations}

We now present a simulation study of the generalized additive model example \eqref{eq:smoothing_spline} to understand how the performance changes as the number of penalty parameters increases.
From Corollary~\ref{corr:train_val}, we know that there are two possibilities.
On one hand, we know that the parametric term in the upper bound is \eqref{eq:sobolev_param}, so it increases linearly with the number of penalty parameters $J$.
On the other hand, we know that the oracle risk \eqref{eq:asym_train_val_oracle_risk} in the upper bound may decrease.
We use two simulations to illustrate that both of these behaviors are possible.

The data is generated as the sum of univariate functions
\begin{align}
Y &= \sum_{j=1}^p g_j^*(X_j) + \sigma \epsilon,
\label{eq:sim_general}
\end{align}
where $\epsilon$ are iid standard Gaussian random variables and $\sigma > 0$ was chosen such that the signal to noise ratio was two. $X$ was drawn from a uniform distribution over $\mathcal{X} = [-2, 2]^p$.
We fit nonparametric additive models by minimizing \eqref{eq:smoothing_spline}.
To vary the number of penalty parameters, we constrain certain $\lambda_j$ to be equal while allowing others to be completely free.
(For instance, if we wanted a single penalty parameter, we would constrain $\lambda_j$ for $j=1,...,p$ to be the same value.) 
The penalty parameters are tuned using a training/validation split.

\textbf{Simulation 1}: The response is the sum of identical univariate sinusoids where
\begin{align}
g_j^*(x_j) &= \sin(x_j) \quad \text{for } j = 1,...,p.
\label{eq:sim1}
\end{align} 
Since the univariate functions are all the same in \eqref{eq:sim1}, the oracle penalty parameters $\tilde \lambda_j$ should be roughly the same for $j = 1,...,p$ and the oracle error should stay relatively constant, even if we increase the number of penalty parameters. So for this first simulation, we should be able to observe that the difference in validation error between the selected model and the oracle model grows linearly in $J$.

\textbf{Simulation 2}: The response is the sum of sinusoid functions with increasing frequency where
\begin{align}
g_j^*(x_j) = \sin(x_j * 1.2^{j - 4}) \quad \text{for } j = 1,...,p.
\label{eq:sim2}
\end{align}
Since the Sobolev norms of $g_j$ increase with $j$, we expect the oracle penalty parameters to be monotonically decreasing, e.g. $\tilde{\lambda}_1 > ... > \tilde{\lambda}_p$. 
As the number of penalty parameters increases, we expect the oracle error term to shrink. If the oracle error term shrinks fast enough, the validation loss of the selected model should decrease.

For both simulations, we used $p = 8$ features and fit the models using 200 training and 200 validation samples. We considered 1, 2, 4, and 8 free penalty parameters, where the penalty parameter structure was nested. That is, when considering $k$ free penalty parameters, we constrained $\{\lambda_{8\ell/k + j} \}_{j = 1,...,8/k}$ to be equal, for $\ell= 0,...,k - 1$. Each simulation setting was replicated forty times.

The penalty parameters were tuned using the nonlinear minimization algorithm \texttt{nlm} in \texttt{R} with initializations at $\{\vec{1}, 0.1 \times \vec{1}, 0.01 \times \vec{1}\}$. 
Multiple initializations were required since minimizing the validation loss with respect to the penalty parameters is a non-convex problem.
We did not use the usual method of grid-search since it is computationally intractable when there are more than three penalty parameters.

Figure \ref{fig:simulations}(a) shows the results from Simulation 1, with the number of penalty parameters on the x-axis and the validation loss difference on the y-axis. The validation loss difference is defined as the difference between the validation loss of the selected model and the oracle model
$$
\left \| \sum_{j=1}^p \hat{g}^{(n_T)}_j(\hat{\boldsymbol{\lambda}}|T) - g^*_j \right \|_V^2 - 
\left \| \sum_{j=1}^p \hat{g}^{(n_T)}_j(\tilde{\boldsymbol{\lambda}} | T) - g^*_j \right \|_V^2.
$$
%JF what is \tilde lambda -- what is it minimized over. how was it found?
As expected, the validation loss difference increases with the number of penalty parameters.
To see if our oracle inequalities match the simulation results, we performed linear regression with the logarithm of the validation loss difference as the response and the logarithm of the number of penalty parameters as the covariate. 
We fit the model over the simulation results with at least two penalty parameters since the data is highly skewed for the case of a single penalty parameter. 
We estimated a slope of 1.00 (standard error 0.15), which suggests that the validation loss difference grows linearly in the number of penalty parameters. 
The results for a single penalty parameter fall below this estimated line --- including the single parameter case gives a slope of 1.45 with standard error 0.14.
This suggests that our oracle inequality might not be tight for a single penalty parameter.

The results from Simulation 2 are shown in Figure \ref{fig:simulations}(b). As the number of penalty parameters increases, the validation loss of the oracle model decreases. In addition, due to the error incurred from tuning multiple hyper-parameters, the gap between the validation loss of the oracle and selected model increases with the number of penalty parameters. Nonetheless, since the validation loss of the oracle model is decreasing at a fast enough rate, the validation loss of the selected model decreases with the number of penalty parameters.

These simulation results suggest that adding more hyper-parameters can improve model-estimation procedures. For nonparametric additive models, having an individual penalty parameter for each additive component allows the model to learn about the differing smoothness of each component. On the other hand, if we know a priori that the additive components have the same smoothness, then it is beneficial to use a single penalty parameter.

\begin{figure}
	\centering
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\textwidth]{../../../R/figures/validation_size_loss_diff_homogeneous.pdf}
		\caption{Simulation 1: sum of identical sinusoids}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\textwidth]{../../../R/figures/validation_size_loss_heterogeneous.pdf}
		\caption{Simulation 2: sum of sinusoids with increasing frequency}
	\end{subfigure}
	\caption{
		Simulation results for nonparametric additive models as the number of penalty parameters grows.
	}
	\label{fig:simulations}
\end{figure}

\section{Discussion}\label{sec:discussion}

The goal of this paper is to characterize the generalization error of model-estimation procedures that tune multiple hyper-parameters via single training/validation split or $K$-fold cross-validation. 
If the estimated models are Lipschitz with respect to the hyper-parameters, the generalization error of the selected model is upper bounded by a combination of the oracle error, a near-parametric term in the number of hyper-parameters, and a geometric mean of the two.
These results show that if additional hyper-parameters shrink the oracle error term by a sufficient amount, they can improve the performance of the selected model.
In the semi- or non-parametric setting, the error incurred from tuning hyper-parameters is dominated by the oracle error asymptotically; so having many hyper-parameters is unlikely to drastically increase the generalization error of the selected model.
In the parametric setting, the error incurred from tuning hyper-parameters is roughly on the same order as the oracle error; so one should be careful about adding more hyper-parameters, though hyper-parameters are not more ``costly'' than model parameters.


We also considered the special case of penalized regression problems with multiple penalty parameters. In our example of additive univariate models, we show that the estimated models are Lipschitz in the penalty parameters, which means our theoretical results apply. Our results suggest using multiple penalties to induce the desired model characteristics and allowing for many penalty parameters, rather than the usual one or two penalty parameters. Moreover, using recently-developed algorithms for tuning multiple hyper-parameters \citep{bengio2000gradient, foo2008efficient, snoek2012practical}, it is now computationally tractable to test out regularization schemes with many penalty parameters.

One drawback of our theoretical results is that we have assumed it is possible to find the global minimizer of the validation loss. Unfortunately this is computationally intractable since the validation loss is not convex with respect to the hyper-parameters. This problem is exacerbated when there are multiple hyper-parameters since it is computationally infeasible to perform an exhaustive grid-search. We hope to address this question in future research.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Supplementary Materials}

Oracle inequalities for general model-estimation procedures and proofs for all the results are given in the Supplementary Materials.
\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 14pt
\noindent {\large\bf Acknowledgements}

Jean Feng was supported by NIH grants DP5OD019820 and T32CA206089. Noah Simon was supported by NIH grant DP5OD019820.
\par

\bibliographystyle{unsrtnat}
\bibliography{hyperparam-theory}

\vskip .65cm
\noindent
Jean Feng, Department of Biostatistics, University of Washington
\vskip 2pt
\noindent
E-mail: jeanfeng@u.washington.edu
\vskip 2pt

\noindent
Noah Simon, Department of Biostatistics, University of Washington
\vskip 2pt
\noindent
E-mail: nrsimon@u.washington.edu


\end{document}
