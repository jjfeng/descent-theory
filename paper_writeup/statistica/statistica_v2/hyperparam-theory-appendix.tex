\documentclass[10pt]{book}
\usepackage[sectionbib]{natbib}
\usepackage{array,epsfig,fancyheadings,rotating}
%\usepackage[dvipdfm]{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textwidth=36pc
\textheight=49pc
\oddsidemargin=1pc
\evensidemargin=1pc
\headsep=15pt
\topmargin=.6cm
\parindent=1.7pc
\parskip=0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{xr}
\externaldocument{hyperparam-theory}

\setcounter{page}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}


\setcounter{theorem}{2}
\setcounter{lemma}{3}
\setcounter{definition}{3}
\DeclareMathOperator*{\argmin}{arg\,min}

\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\def\n{\noindent}
\lhead[\fancyplain{} \leftmark]{}
\chead[]{}
\rhead[]{\fancyplain{}\rightmark}
\cfoot{}
%\headrulewidth=0pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{2}

\markright{ \hbox{\footnotesize\rm Statistica Sinica: Supplement
%{\footnotesize\bf 24} (201?), 000-000
}\hfill\\[-13pt]
\hbox{\footnotesize\rm
%\href{http://dx.doi.org/10.5705/ss.20??.???}{doi:http://dx.doi.org/10.5705/ss.20??.???}
}\hfill }

\markboth{\hfill{\footnotesize\rm Jean Feng and Noah Simon} \hfill}
{\hfill {\footnotesize\rm Hyper-parameter selection via split-sample validation} \hfill}

\renewcommand{\thefootnote}{}
$\ $\par \fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\centerline{\large\bf An analysis of the cost of hyper-parameter selection via split-sample}
\vspace{2pt}
\centerline{\large\bf validation, with applications to penalized regression}
\vspace{.25cm}
\centerline{Jean Feng and Noah Simon}
\vspace{.4cm}
\centerline{\it Department of Biostatistics, University of Washington}
\vspace{.55cm}
\centerline{\bf Supplementary Material}
\fontsize{9}{11.5pt plus.8pt minus .6pt}\selectfont
\noindent
\par

\setcounter{section}{0}
\setcounter{equation}{0}
\def\theequation{S\arabic{section}.\arabic{equation}}
\def\thesection{S\arabic{section}}

\fontsize{12}{14pt plus.8pt minus .6pt}\selectfont

\section{Appendix}\label{sec:proofs}

We will use the following notation: for functions $f$ and $g$ and a dataset $D$ with $m$ samples, we denote the inner product of $f$ and $g$ at covariates $D$ as $\langle f,g \rangle_{D} = \frac{1}{m} \sum_{x_i \in D} f(x_i) g(x_i) $.

\subsection{A single training/validation split}
\label{appendix:train_val}

Theorem \ref{thrm:train_val} is a special case of Theorem \ref{thrm:train_val_complicated}, which applies to general model-estimation procedures. The proof is based on the inequality below. Inequalities of this form are often called ``basic inequalities'', since they are derived directly from the definition. In this ``basic inequality'' we see that the quantity of interest, the difference in the error of the selected model and the oracle model, is bounded by an empirical process term.

\begin{lemma}{Basic inequality}
	\begin{equation}
	\label{thrm:basic_ineq}
	\left \| g^* - \hat{g}^{(n_T)}(\hat{\boldsymbol{\lambda}}|T) \right \|^2_V 
	- \left \| g^* - \hat{g}^{(n_T)}(\tilde{\boldsymbol{\lambda}}|T) \right \|^2_V
	\le 
	\left \langle \epsilon, \hat{g}^{(n_T)}(\tilde{\boldsymbol{\lambda}}|T) - \hat{g}^{(n_T)}(\hat{\boldsymbol{\lambda}}|T) \right \rangle_V
	\end{equation}
\end{lemma}

\begin{proof}
	By definition,
	\begin{equation}
	\left \| y - \hat{g}^{(n_T)}(\hat{\boldsymbol{\lambda}}|T) \right \|^2_V \le 
	\left \| y - \hat{g}^{(n_T)}(\tilde{\boldsymbol{\lambda}}|T) \right \|^2_V.
	\end{equation}
\end{proof}

We are therefore interested in bounding the empirical process term in \eqref{thrm:basic_ineq}. A common approach is to use a measure of complexity of the function class. For a single training/validation split, where we treat the training set as fixed, we only need to consider the complexity of the fitted models from the model-selection procedure
\begin{equation}
\mathcal{G}(T)=\left\{ \hat{g}^{(n_T)}(\boldsymbol{\lambda}|T) : \boldsymbol{\lambda} \in \Lambda \right\}.
\end{equation}
This model class can be considerably less complex compared to the original function class $\mathcal{G}$, such as the special case in Theorem \ref{thrm:train_val} where we suppose $\mathcal{G}(T)$ is Lipschitz. For this proof, we will use metric entropy as a measure of model class complexity. We recall its definition below.
\begin{definition}
	Let $\mathcal{F}$ be a function class. Let the covering number $N(u, \mathcal{F}, \| \cdot \|)$ be the smallest set of $u$-covers of $\mathcal{F}$ with respect to the norm $\| \cdot \|$. The metric entropy of $\mathcal{F}$ is defined as the log of the covering number:
	\begin{equation}
	H (u, \mathcal{F}, \| \cdot \| ) = \log N(u, \mathcal{F}, \| \cdot \|).
	\end{equation}
\end{definition}

We will bound the empirical process term using the following Lemma, which is a simplification of Corollary 8.3 in \citet{van2000empirical}.

\begin{lemma}
	\label{lemma:cor83}
	Suppose $D^{(m)} = \{x_1,...,x_m\}$ are fixed and $\epsilon_1,...,\epsilon_m$ are independent random variables with mean zero and uniformly sub-gaussian with parameters $b$ and $B$. Suppose
	the model class $\mathcal{F}$ has elements $\sup_{f\in\mathcal{F}}\|f\|_{D^{(m)}}\le R$
	and satisfies
	\[
	\psi(R)\ge\int_{0}^{R}H^{1/2}(u,\mathcal{F},\|\cdot\|_{D^{(m)}})du.
	\]
	
	
	There is a constant $a > 0$ dependent only on $b$ and $B$ such that
	for all $\delta>0$ satisfying
	\[
	\sqrt{m}\delta\ge a(\psi(R)\vee R),
	\]
	we have 
	\[
	Pr\left(\sup_{f\in\mathcal{F}}\left|\frac{1}{m}\sum_{i=1}^{m}\epsilon_{i}f(x_{i})\right|\ge\delta\right)
	\le 
	a\exp\left(-\frac{m\delta^{2}}{4a^{2}R^{2}}\right).
	\]
	
\end{lemma}

We are now ready to prove the oracle inequality. It uses a standard peeling argument.

\begin{theorem}
	\label{thrm:train_val_complicated}
	Consider a set of hyper-parameters $\Lambda$.
	% ONLY necessary for the validation set
	Suppose independent random variables $\epsilon_1, ... \epsilon_n$ have expectation zero and are uniformly sub-Gaussian with parameter $b$ and $B$.
	Let training data $T$ be fixed, as well as the covariates of the validation set $X_V$.
	
	Suppose there is a function $\psi(\cdot | T):\mathbb{R}\mapsto\mathbb{R}$ and constant $r > 0$ such that
	\begin{equation}
	\label{eq:dudley_bound}
	\int_{0}^{R}H^{1/2}(u,\mathcal{G}(T),\|\cdot\|_{V})du\le\psi(R| T) \quad \forall R>r
	\end{equation}
	
	Also, suppose $\psi\left(u | T \right)/u^{2}$ is non-increasing in $u$ for all $u > r$.
	Let $\tilde{\boldsymbol \lambda}$ be defined as in \eqref{eq:tilde_lambda_def}.
	
	Then there is a constant $c>0$ only depending on $b$ and $B$ such that for all $\delta$ satisfying
	\begin{equation}
	\label{eq:train_val_delta_condn}
	\sqrt{n_V}\delta^{2}
	\ge
	c \left ( 
	\psi(\delta| T)
	\vee 
	\delta
	\vee
	\psi \left (
	4\left\Vert g^* - \hat{g}^{(n_T)}( \tilde{\boldsymbol{\lambda}} | T)\right\Vert_{V}
	\middle | T
	\right ) 
	\vee
	4 \left\Vert g^* - \hat{g}^{(n_T)}( \tilde{\boldsymbol{\lambda}} | T)\right\Vert_{V}
	\right ),
	\end{equation}
	we have
	\begin{align*}
		& Pr\left(
		\left\Vert g^* - \hat{g}^{(n_T)}( \hat{\boldsymbol{\lambda}} | T) \right\Vert _{V}^2 -
		\left\Vert g^* - \hat{g}^{(n_T)}( \tilde{\boldsymbol{\lambda}} | T) \right\Vert _{V}^2
		\ge\delta^2
		\middle | 
		T, X_V
		\right )\\
		&\le c\exp\left(-\frac{n_{V}\delta^{4}}{
			c^{2}
			\left\Vert g^* - \hat{g}^{(n_T)}( \tilde{\boldsymbol{\lambda}} | T) \right\Vert _{V}^2
		}\right) 
		+c\exp\left(-\frac{n_{V}\delta^{2}}{c^{2}}\right).
	\end{align*}
\end{theorem}

\begin{proof}
	We will use the simplified notation $\hat{g}(\hat{\boldsymbol{\lambda}}) \coloneqq \hat{g}^{(n_T)}(\hat{\boldsymbol{\lambda}} | T)$ and $\hat{g}(\tilde{\boldsymbol{\lambda}}) \coloneqq \hat{g}^{(n_T)}(\tilde{\boldsymbol{\lambda}} | T)$. In addition, the following probabilities are all conditional on $X_V$ and $T$ but we leave them out for readability.
	\begin{align}
	&  Pr\left(\left\Vert \hat{g}(\hat{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}-\left\Vert \hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}\ge\delta^{2}\right) \label{eq:train_val_prob}\\
	& = \sum_{s=0}^{\infty}Pr\left(2^{2s}\delta^{2}\le\left\Vert \hat{g}(\hat{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}-\left\Vert \hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}\le2^{2s+2}\delta^{2}\right) 
	\label{eq:peeled} \\
	&\le \sum_{s=0}^{\infty} Pr\left(2^{2s}\delta^{2}\le2\left\langle \epsilon,\hat{g}(\hat{\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\rangle _{V}\right. \label{eq:peel_ineq}\\
	& \qquad  \left.\wedge \left\Vert \hat{g}(\hat{\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\Vert_{V}^{2}\le2^{2s+2}\delta^{2}+ 2\left|\left\langle \hat{g}(\tilde{\boldsymbol{\lambda}})-\hat{g}(\hat{\boldsymbol{\lambda}}),\hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\rangle _{V}\right| \right ),
	\end{align}
	where we applied the basic inequality in the last inequality.
	Each summand in \eqref{eq:peel_ineq} can be bounded by splitting the event into the cases where either $2^{2s+2} \delta^2$ or $2\left|\left\langle \hat{g}(\tilde{\boldsymbol{\lambda}})-\hat{g}(\hat{\boldsymbol{\lambda}}),\hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\rangle _{V}\right|$ is larger. Splitting up the probability and applying Cauchy Schwarz gives us the following bound for \eqref{eq:train_val_prob}
	\begin{align}
	& Pr\left(
	\sup_{\boldsymbol{\lambda} \in \Lambda: \left\Vert \hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\Vert _{V}
		\le
		4\left\Vert \hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}}
	2\left\langle \epsilon,\hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\rangle _{V}
	\ge 
	\delta^{2}
	\right)
	\label{eq:train_val_1}
	\\
	& + \sum_{s=0}^{\infty} Pr\left(
	\sup_{\boldsymbol{\lambda} \in \Lambda: \left\Vert \hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\Vert _{V}
		\le
		2^{s+3/2}\delta}
	2\left\langle \epsilon,\hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\rangle _{V}
	\ge
	2^{2s} \delta^{2}
	\right)
	\label{eq:train_val_2}.
	\end{align}
	
	We can bound both \eqref{eq:train_val_1} and \eqref{eq:train_val_2} using Lemma \ref{lemma:cor83}. For our choice of $\delta$ in \eqref{eq:train_val_delta_condn},
	there is some constant $a>0$ dependent only on $b$ such that \eqref{eq:train_val_1} is bounded above by
	\[ 
	a\exp\left(-\frac{n_{V}\delta^{4}}{4a^{2}\left(16\left\Vert \hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}\right)}\right).
	\]
	In addition, our choice of $\delta$ from \eqref{eq:train_val_delta_condn} and our assumption that $\psi(u)/u^2$ is non-increasing implies that the condition in Lemma \ref{lemma:cor83} is satisfied for all $s=0,1,...,\infty$ simultaneously. Hence for all $s=0,1,...,\infty$, we have
	\begin{align}
	Pr\left(
	\sup_{\boldsymbol{\lambda} \in \Lambda: \left\Vert \hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\Vert _{V}
		\le
		2^{s+3/2}\delta}
	2\left\langle \epsilon,\hat{g}({\boldsymbol{\lambda}})-\hat{g}(\tilde{\boldsymbol{\lambda}})\right\rangle _{V}
	\ge
	2^{2s} \delta^{2}
	\right)
	& \le 
	a\exp\left(-n_{V}\frac{2^{4s-2}\delta^{4}}{4a^{2}2^{2s+3}\delta^{2}}\right).
	\end{align}
	
	Putting this all together, there is a constant $c$ such that \eqref{eq:train_val_prob} is bounded above by
	\begin{equation}
	c\exp\left(-\frac{n_{V}\delta^{4}}{4 c^{2}\left(16\left\Vert \hat{g}(\tilde{\boldsymbol{\lambda}})-g^{*}\right\Vert _{V}^{2}\right)}\right)
	+
	c\exp\left(-\frac{n_{V} \delta^2}{c^{2}}\right).
	\end{equation}
	
\end{proof}

We can apply Theorem \ref{thrm:train_val_complicated} to get Theorem \ref{thrm:train_val}. Before proceeding, we determine the entropy of $\mathcal{G}(T)$ when the functions are Lipschitz in the hyper-parameters.

\begin{lemma}
	\label{lemma:covering_cube}
	Let $\Lambda = [\lambda_{\min}, \lambda_{\max}]^J$ where $\lambda_{\min} \le \lambda_{\max}$. Suppose $\mathcal{G}(T)$ is Lipschitz with function $C(\cdot | T)$ over $\boldsymbol{\lambda}$.
	Then the entropy of $\mathcal{G}(T)$ with respect to $\| \cdot \|$ is
	\begin{equation}
	H\left(u, \mathcal{G}(T),\|\cdot\|\right) \le
	J \log \left(\frac{4 \|C(\cdot | T)\| \left(\lambda_{max}-\lambda_{min}\right)+2u}{u}\right).
	\end{equation}
\end{lemma}
\begin{proof}
	Using a slight variation of the proof for Lemma 2.5 in \citet{van2000empirical}, we can show
	\begin{align}
	N\left(u,\Lambda,\|\cdot\|_{2}\right) \le \left(\frac{4\left(\lambda_{max}-\lambda_{min}\right)+2u}{u}\right)^{J}.
	\end{align}
	Under the Lipschitz assumption, a $\delta$-cover for $\Lambda$
	is a $\|C(\cdot | T)\|\delta$-cover for $\mathcal{G}(T)$. The covering number for $\mathcal{G}(T)$ wrt $\|\cdot\|$ is bounded by the covering number for $\Lambda$ as follows
	\begin{eqnarray}
	N\left(u,\mathcal{G}(T),\|\cdot\|\right)
	&\le& N\left(\frac{u}{\|C(\cdot | T)\|},\Lambda,\|\cdot\|_{2}\right)\\
	&\le& \left(\frac{4\left(\lambda_{max}-\lambda_{min}\right)+2u/\|C(\cdot | T)\|}{u/\|C(\cdot | T)\|}\right)^{J}.
	\end{eqnarray}
\end{proof}

\subsubsection{Proof for Theorem \ref{thrm:train_val}}
\begin{proof}
	By Lemma \ref{lemma:covering_cube}, we have
	\begin{align}
	\int_{0}^{R}H^{1/2}(u,\mathcal{G}(T),\|\cdot\|_{V})du 
	&= \int_{0}^{R} \left ( 
	J \log \left(\frac{4 \|C_\Lambda\|_V \Delta_{\Lambda}+2u}{u}\right)
	\right )^{1/2}
	du\\
	& \le J^{1/2}\int_{0}^{R}\left[
	\log\left(
	\frac{4 \|C_\Lambda(\cdot | T)\|_V \Delta_{\Lambda} + 2R }
	{u}
	\right)
	\right]^{1/2}du\\
	& = J^{1/2}R \int_{0}^{1}\left[
	\log\left(
	\frac{4 \|C_\Lambda(\cdot | T)\|_V \Delta_{\Lambda} + 2R }
	{vR}
	\right)
	\right]^{1/2}dv\\
	& \le J^{1/2}R \int_{0}^{1}
	\log^{1/2}\left(
	\frac{4 \|C_\Lambda(\cdot | T)\|_V \Delta_{\Lambda} + 2R}
	{R}
	\right)
	+
	\log^{1/2}(1/v)
	dv\\
	& < J^{1/2}R \left (
	\log^{1/2}\left(
	\frac{4 \|C_\Lambda(\cdot | T)\|_V \Delta_{\Lambda} + 2R}
	{R}
	\right)
	+
	1
	\right ).
	% Show fewer steps maybe
	\end{align}
	If we restrict $R > n^{-1}$, then for an absolute constant $c$, we have
	\begin{equation}
	\label{eq:train_val_entropy}
	\int_{0}^{R}H^{1/2}(u,\mathcal{G}(T),\|\cdot\|_{V})du
	\le
	\psi (R) 
	\coloneqq c R\left ( J \log(\|C_\Lambda(\cdot |T)\|_V \Delta_{\Lambda} n + 1) \right )^{1/2}.
	\end{equation}
	Applying Theorem \ref{thrm:train_val_complicated}, we get our desired result.
\end{proof}

\subsection{Cross-validation}
\label{app:cv}
As mentioned before, Theorem \ref{thrm:kfold} is an application of Theorem \ref{thrm:jean_cv}, which extends Theorem 3.5 in \citet{lecue2012oracle}.
Theorem 3.5 in \citet{lecue2012oracle} assumes that there is a single function $J$ that bounds the complexity of the post-training model class $\mathcal{G}(T)$. (need more explanation)
However the complexity of the post-training model class depends on training data -- for instance, if the noise in the training data were particularly large, the complexity of the post-training model class can be very high.
In our extension, we rely on the fact that the noise in the dataset are sub-gaussian, so that the complexity of the post-training model class can be controlled with high probability.

For this section, suppose we have a measurable space $(\mathcal{Z}, \mathcal{T})$ and $\mathcal{F}$ is a class of measurable functions from $\mathcal{Z} \mapsto \mathbb{R}$.
We consider a general, convex loss function $Q: \mathcal{Z} \times \mathcal{F} \mapsto \mathbb{R}$ (rather than the least squares loss considered in the previous section).
The model-estimation procedure selects functions from the class $\mathcal{F}$.
The risk function $R$ is defined as...

Our theorem again begins with a result reminiscent of the basic inequality.
The following lemma is Lemma 3.1 from \citet{lecue2012oracle}.
\begin{lemma}
	lemma 3.1
\end{lemma}

We need to bound the supremum of the shifted empirical process term.
To do this, we again use techniques from empirical process theory.
Instead of using metric entropy from the previous section, we use Talagrand's $\gamma$ function, which bounds supremum of stochastic processes using a generic chaining mechanism.
The $\gamma$ function is defined as follows
\begin{definition}
	admissible sequences...
	talagrand...
\end{definition}

To prove our result, we first extend Lemma 3.4 in \citet{lecue2012oracle} as follows.
The proof used in \citet{lecue2012oracle} is actually sufficient, with minor modifications.
The main point of the following lemma is to restate Lemma 3.4 using notation that clarifies the conditional dependencies and to introduce a new function $h(D^{(n_T)})$ that is a function of the training data and a new bounding function $J_\delta$.
\begin{lemma}
Let $\mathcal{Q}(D^{(m)})\equiv\left\{ Q(\lambda|D^{(m)}):\lambda\in\Lambda\right\} $
and $\mathcal{Q}\equiv\cup_{m\in\mathbb{N}}\cup_{D^{(m)}}\mathcal{Q}(D^{(m)})$.

Suppose there exists $C_{1}>0$ and increasing function $G(\cdot)$
such that $\forall Q\in\mathcal{Q}$, 
\[
\|Q(Z)\|_{L_{2}}\le G\left(\mathbb{P}Q(Z)\right).
\]


Let $n_{T},n_{V}\in\mathbb{N}$ and fix a dataset $D^{(n_{T})}$.
Suppose there exist functions $D^{(n_{T})}\mapsto h(D^{(n_{T})})\in\mathbb{R}^{+}$
and $\left\{ w \mapsto J_{\delta}(w) :\delta>0\right\} $
and a constant $w_{\min}>0$ such that for any dataset $D^{(n_{T})}$ and any $w \ge w_{\min}$,
\[
h(D^{(n_{T})})\le\delta\implies\frac{\log n_{V}}{\sqrt{n_{V}}}\gamma_{1}\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),\|\cdot\|_{L_{\psi_{1}}}\right)+\gamma_{2}\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),\|\cdot\|_{L_{2}}\right)\le J_{\delta}(w)
\]
where $\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})})\equiv\left\{ Q\in\mathcal{Q}(D^{(n_{T})}):\|Q(Z)\|_{L_{2}}\le G(w)\right\}$.

Then there exists some absolute constant $L,c>0$ such that for all
$w\ge w_{\min}$ and all $u\ge1$,
\[
\Pr\left(
\sup_{Q\in\mathcal{Q}(D^{(n_{T})}): PQ \le w}
\left(\left(\mathbb{P}-P_{n_{V}}\right)Q\right)_{+}\le uL\frac{J_{\delta}(w)}{\sqrt{n_{V}}}\mid h\left(D^{(n_{T})}\right)\le\delta
\right)
\ge1-L\exp(-cu).
\]
\end{lemma}

Next we restate Lemma 3.2 in \citet{lecue2012oracle} by clarifying the conditional dependencies and using our new functions $h$ and $J_\delta$.
\begin{lemma}
	Let $a>0$. Let $\mathcal{Q}(D^{(m)})\equiv\left\{ Q(\lambda|D^{(m)}):\lambda\in\Lambda\right\} $
	be a set of measurable functions.
	
	Let random variable $Z$ satisfy for all $m\in\mathbb{N}$, any dataset
	$D^{(m)}$, for all $Q\in\mathcal{Q}\left(D^{(m)}\right)$, $\mathbb{P}Q(Z)\ge 0 $.
	
	Suppose for any $n_{T},n_{V}\in\mathbb{N}$ and dataset $D^{(n_{T})}$
	there exists some absolute constant $L,c>0$ such that for all $w\ge w_{\min}$
	and for all $u\ge1$,
	\[
	\Pr\left(
	\sup_{Q\in\mathcal{Q}(D^{(n_{T})}): PQ \le w}
	\left(\left(\mathbb{P}-P_{n_{V}}\right)Q\right)_{+}\le uL\frac{J_{\delta}(w)}{\sqrt{n_{V}}}\mid h\left(D^{(n_{T})}\right)\le\delta\right)
	\ge 1-L\exp(-cu).
	\]
	
	
	Suppose every function in $\left\{ J_{\delta}:\delta>0\right\} $
	is strictly increasing and its inverse is strictly convex.
	Let $\psi_{\delta}$ be the convex conjugate of $J_{\delta}^{-1}$,
	e.g. $\psi_{\delta}(u)=\sup_{v>0}uv-J_{\delta}^{-1}(v)$ for all $u>0$.
	Assume there is a $r\ge1$ such that $x>0\mapsto\psi(x)/x^{r}$ decreases
	and define for $q>1$ and $u\ge1$,
	\[
	\tilde \psi_{q,\delta}(u)=\psi_{\delta}\left(\frac{2q^{r+1}(1+a)u}{a\sqrt{n_{V}}}\right)\vee w_{\min}.
	\]
	
	
	Then there exists a constant $L_{1}$ that only depends on $L$ such
	that for every $u\ge1$,
	\[
	\Pr\left(
	\sup_{Q\in\mathcal{Q}(D^{(n_{T})})}
	\left(\left(\mathbb{P}-(1+a)P_{n_{V}}\right)Q\right)_{+}
	\le \frac{a \tilde{\psi}_{q,\delta}(u/q)}{q}\mid h\left(D^{(n_{T})}\right)\le\delta
	\right)
	\ge 1-L_{1}\exp(-cu).
	\]
	
	
	Moreover, assume that $\psi_{\delta}$ increases such that $\psi_{\delta}(\infty)=\infty$.
	Then there exists a constant $c_{1}$ that depends only on $L$ and $c$ such that
	\[
	\mathbb{P}\left[\sup_{Q\in\mathcal{Q}(D^{(n_{T})})}
	\left(\left(\mathbb{P}-(1+a)P_{n_{V}}\right)Q\right)_{+}
	\middle |
	h\left(D^{(n_{T})}\right)\le\delta
	\right]
	\le\frac{ac_{1} \tilde{\psi}_{q,\delta}(1/q)}{q}.
	\]
\end{lemma}

Now we state the following lemma, that will allow us to extend of Theorem 3.5 in \citet{lecue2012oracle}.
Using a simple chaining argument, we arrive at the following lemma.
\begin{lemma}
	\label{lemma:chain}
Consider any $a>0$.
Suppose there exists a constant $c_{1}$ such that for any $n_{T},n_{V} \in \mathbb{N}$, $\delta>0$,
and $q>1$, we have
\[
\mathbb{P}\left[
\sup_{Q\in\mathcal{Q}(D^{(n_{T})})}\left(\left(\mathbb{P}-(1+a)P_{n_{V}}\right)Q\right)_{+}
\middle | 
h\left(D^{(n_{T})}\right)\le\delta\right]
\le\frac{ac_{1}\epsilon_{q,\delta}(1/q)}{q}.
\]
Then for any $\sigma > 0$, we have
\[
\mathbb{P}\left[\sup_{Q\in\mathcal{Q}(D^{(n_{T})})}\left(\left(\mathbb{P}-(1+a)P_{n_{V}}\right)Q\right)_{+}\right]
\le
\frac{ac}{q}
\left(
\tilde{\psi}_{q,2\sigma}(1/q)
+\sum_{k=1}^{\infty}\Pr\left(h\left(D^{(n_{T})}\right)\ge2^{k}\sigma\right)
\tilde{\psi}_{q,2^{k}\sigma}(1/q)
\right)
.
\]
\end{lemma}
Obviously the above lemma is only useful if we can show that $h$ is bounded with high probability.
For instance, in our examples later, $h$ has exponential tails so the upper bound in Lemma~\ref{lemma:chain} is well-controlled.

Putting the three lemmas above together, we have the following result.

\begin{theorem}
	\label{thrm:jean_cv}
	Consider a set of hyper-parameters $\Lambda$. Consider a loss function $Q:(\mathcal{Z}, \mathcal{G}) \mapsto \mathbb{R}$ with convex risk function $R: \mathcal{G} \mapsto \mathbb{R}$. Let
	$$
	\mathcal{Q} = \{ 
	Q(\cdot, \hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)}) - Q(\cdot, g^*) : \boldsymbol{\lambda} \in \Lambda \}.
	$$
	Suppose there exist constants $K_0, K_1 \ge 0$ and $\kappa \ge 1$ such that for any $m \in \mathbb{N}$ and any dataset $D^{(m)}$, the following two inequalities hold
	\begin{align}
	\left \| Q(\cdot, \hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)}) - Q(\cdot, g^*) \right \|_{L_{\psi_1}} & \le K_0
	\label{eq:cv_assump1}\\
	\left \| Q(\cdot, \hat{g}^{(n_T)}(\boldsymbol{\lambda} | D^{(n_T)}) - Q(\cdot, g^*)  \right \|_{L_2} & \le 
	K_1 \left ( R(\hat{g}^{(m)}(\boldsymbol{\lambda}|D^{(m)})) - R(g^*) \right )^{1/2\kappa}.
	\label{eq:cv_assump2}
	\end{align}
	Suppose there is an $w_{\min} > 0$ and
	% JF: is this the right notation for a training set?
	functions $h: {\mathcal{Z}}^{(n_T)} \mapsto \mathbb{R}$
	and $J_\delta: \mathbb{R}\mapsto \mathbb{R}$ such that
	for all $w \ge w_{\min}$,
	\[
	h(D^{(n_{T})})\le\delta\implies
	\frac{\log n_{V}}{\sqrt{n_{V}}}
	\gamma_{1}\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),\|\cdot\|_{L_{\psi_{1}}}\right)
	+\gamma_{2}\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),\|\cdot\|_{L_{2}}\right)\le J_{\delta}(w)
	\]
	where $\mathcal{Q}_w = \{Q \in \mathcal{Q}: \| Q \|_{L_2} \le w^{1/2\kappa} \}$.
	Moreover, suppose that for all $\delta > 0$, $J_\delta$ is a strictly increasing function, $J_\delta^{-1}(\epsilon)$ is strictly convex,
	the convex conjugate $\psi_\delta$ of $J^{-1}_\delta$ increases, $\psi_\delta(\infty ) = \infty$ and there exists $r \ge 1$ such that $\psi_\delta(x)/x^r$ decreases.
	
	Consider any $\sigma > 0$. Then there is an absolute constant $c > 0$ such that for every $a > 0$ and $q > 1$, the following inequality holds
	\begin{align}
	\begin{split}
	E_{D^{(n)}} 
	\left(
	R\left(\bar{g} ( \hat{\boldsymbol \lambda} | {D^{(n)}} ) \right )
	- R (g^*)
	\right)
	&\le
	(1+a) \inf_{\boldsymbol{\lambda} \in \Lambda} 
	E_{D^{(n_T)}}
	\left(
	R\left(\bar{g} ( \hat{\boldsymbol \lambda} | {D^{(n)}} ) \right )
	- R (g^*)
	\right)
	\\
	& +
	\frac{ac}{q}
	\left(
	\tilde{\psi}_{q,2\sigma}(1/q)
	+\sum_{k=1}^{\infty}
	\Pr\left(h\left(D^{(n_{T})}\right)\ge2^{k}\sigma\right)
	\tilde{\psi}_{q,2^{k}\sigma}(1/q)
	\right).
	\end{split}
	\end{align}
	where $\tilde{\psi}_{q, \delta}(u) = \psi_\delta\left(\frac{2q^{r+1}(1 + a)u}{a\sqrt{n_V}}\right) \vee w_{\min}$ for all $u > 0$.
\end{theorem}

To see why this extension of Theorem 3.5 in \citet{lecue2012oracle} was useful, we now apply it to prove Theorem~\ref{thrm:kfold} where we consider the squared error loss $Q((x,y), g) = (y - g(x))^2$ and model-estimation methods where the estimated functions are Lipschitz in the hyper-parameters.
First we need the following lemma that describes the relationship between Lipschitz functions
\begin{lemma}
	Suppose the same conditions as Theorem~\ref{thrm:jean_cv}.
	Suppose for any $n_T \in \mathbb{N}$ and any dataset $D^{(n_T)}$, we have
	\begin{align}
	\left |\hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x) - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x) \right | \le C_\Lambda(x|D^{(n_T)}) \|\boldsymbol{\lambda}^{(1)} - \boldsymbol{\lambda}^{(2)}\|_2.
	\end{align}
	Let $w > 0$ and define 
	$\mathcal{Q}_{w}^{L_{2}} = \{g^* - \hat{g}(\boldsymbol{\lambda}|D^{(n_{T})}) : P (g^* - \hat{g}(\boldsymbol{\lambda}|D^{(n_{T})}))^2 < w\}$.
	Then
	\begin{align}
	N\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),u,\|\cdot\|_{L_{2}}\right)\le N\left(\Lambda,\frac{u}{2\left(\|\epsilon\|_{L_{2}}+\sqrt{w}\right)
		\|C_\Lambda(x|D^{(n_{T})})\|_{L_{2}}},\|\cdot\|_{2}\right).
	\end{align}
	Moreover, if there is some $r > 0$ such that
	\begin{align}
	\|g^{*}-\hat{g}(\boldsymbol{\lambda}|D^{(n_{T})})\|_{L_{\psi_{2}}}\le c\left(\|g^{*}-\hat{g}(\boldsymbol{\lambda}|D^{(n_{T})})\|_{L_{2}}\right)^{r},
	\end{align}
	then we also have
	\begin{align}
	N\left(\mathcal{Q}_{w}^{L_{2}}(D^{(n_{T})}),u,\|\cdot\|_{L_{\psi_{1}}}\right)\le N\left(\Lambda,\frac{u}{2\left(\|\epsilon\|_{L_{\psi_{2}}}+cw^{r/2}\right)
		\|C_\Lambda(x|D^{(n_{T})})\|_{L_{\psi_{2}}}},\|\cdot\|_{2}\right).
	\end{align}
	\label{lemma:covering_lipschitz}
\end{lemma}
\begin{proof}
Let us first consider a general norm $\|\cdot \|$ such that for any random variables $X, Y$, we have $\|XY\| \le \|X\|_* \|Y\|_*$.
Then for all $\boldsymbol{\lambda} \in \Lambda$ such that
$P (g^* - \hat{g}(\boldsymbol{\lambda} | D^{n_T}))^2 \le w$, we have
\begin{align}
& \left \|
Q(\cdot , \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x))
- Q(\cdot , \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x))
\right \|\\
& = \left \|
\left (y - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x) \right) ^2 - \left (y - g^*(x) \right) ^2
- \left (y - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x) \right) ^2 - \left (y - g^*(x) \right) ^2
\right \|
\label{eq:loss_diff}
\\
%& = \|
%\left (y - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x) \right)
%\left(\hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x) - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x)\right ) \\ & - \left(\hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x) - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x)\right )^2 \|\\
%%JF falling off page
& \le
\left \|2\epsilon + g^*(x) - \hat{g}(\boldsymbol{\lambda}^{(1)} | D^{(n_T)})(x)
+ g^*(x) - \hat{g}(\boldsymbol{\lambda}^{(2)} | D^{(n_T)})(x) \right \|_* \\
& \quad \left \| \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(2)}|D^{(n_T)})(x) - \hat{g}^{(n_T)}(\boldsymbol{\lambda}^{(1)}|D^{(n_T)})(x) \right \|_* \\
& \le  \left (2 \|\epsilon\|_* +
2 \sup_{\lambda \in \Lambda: P(g^* - \hat{g}(\boldsymbol{\lambda} | D^{n_T}))^2 \le w} 
\left \| g^*(x) - \hat{g}(\boldsymbol{\lambda}^{(1)} | D^{(n_T)})(x) \right \|_* 
\right)
\left \|C_\Lambda (x | D^{(n_T)}) \right \|_*
\|\boldsymbol{\lambda}^{(2)} - \boldsymbol{\lambda}^{(1)} \|_2
\end{align}

For $\|\cdot \| = \|\cdot\|_{L_{2}}$, the upper bound reduces to
$$
2\left(\|\epsilon\|_{L_{2}}+\sqrt{w}\right)
\|C_\Lambda (x | D^{(n_T)})\|_{L_{2}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}$$
as the $L_2$ norm is its own dual norm.

For $\|\cdot \| = \|\cdot\|_{L_{\psi_{1}}}$, the upper bound reduces to 
$$
2\left(\|\epsilon\|_{L_{\psi_{2}}}+cw^{r/2}\right)
\|C_\Lambda (x | D^{(n_T)})\|_{L_{\psi_{2}}}\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}
$$
as the dual of the $L_{\psi_1}$ norm is $L_{\psi_2}$.
\end{proof}

\subsubsection{Proof for Theorem \ref{thrm:kfold}}
\begin{proof}
	Next we determine $J_\delta$.
	By Lemma \ref{lemma:covering_lipschitz}, we know
	\begin{align}
	N(\mathcal{Q}_\epsilon^{L_2}, u, \|\cdot \|_{L_2})
	& \le N \left (
	\Lambda, 
	\frac{u}{
	\left (2 \|\epsilon\|_{L_2} + 6 \sup_{g \in \mathcal G} \left \|g(x) \right \|_{L_2} \right) \|C_\Lambda(x|D^{(n_T)})\|_2},
	\|\cdot \|_{2} \right)\\
	& \le
	\left (
	\frac{
		c_0 \left (\|\epsilon\|_{L_2} + \sup_{g \in \mathcal G} \left \|g(x) \right \|_{L_2} \right) \|C_\Lambda(x|D^{(n_T)})\|_{L_2} \Delta_{\Lambda} + u
	}{u}
	\right )^J
	\end{align}
	for an absolute constant $c_0 > 0$.
	Similarly,
	\begin{align}
	N(\mathcal{Q}_\epsilon^{L_2}, u, \|\cdot \|_{L_{\psi_1}})
	& \le
	\left (
	\frac{
		c_0 \left (\|\epsilon\|_{L_2} + \sup_{g \in \mathcal G} \left \|g(x) \right \|_{L_{\psi_2}} \right) \|C_\Lambda(x|D^{(n_T)})\|_{L_{\psi_2}} \Delta_{\Lambda} + u
	}{u}
	\right )^J
	\end{align}
	Hence we have
	\begin{equation}
	\psi(d) \coloneqq
	\sqrt{d}K_{n,1}
	+
	\frac{K_{n,2}}{\sqrt{n_V}} 
	\end{equation}
	for $$K_{n,1} = \left[J\left(1+\log\left(c_1\sqrt{n_V} GC \Delta_{\Lambda} \right)\right)\right]^{1/2}$$ 
	and 
	$$K_{n,2} = \log n_V 2GJ\left(1+\log\left(c_1 GC \Delta_{\Lambda} \right)\right)$$
	where $c_1 > 0$ is an absolute constant.
	
	$\psi(d)$ is a valid upper bound in \eqref{eq:mitchell_dudley_bound}  for all $d > n_V^{-1}$.
	
	We can show that the convex conjugate of $\psi^{-1}$ is
	\begin{equation}
	\label{eq:convex_conjugate}
	\psi^*(z) = \frac{z^2 K_{n,1}^2}{4} + \frac{z K_{n,2}}{\sqrt{n_V}}.
	\end{equation}	
	
	Plugging in \eqref{eq:convex_conjugate} into Theorem \ref{thrm:mitchell} and taking $q \rightarrow 1$ gives us the result in Theorem \ref{thrm:kfold}.
\end{proof}

\subsection{Penalized regression for additive models}

We now show that penalized regression problems for additive models satisfies the Lipschitz condition.
\subsubsection{Proof for Lemma \ref{lemma:param_add}}
\begin{proof}
	We will use the notation $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}) \coloneqq \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda} | T)$. By the gradient optimality conditions, we have for all $j=1:J$ 
	\begin{equation}
	\label{eq:grad_opt}
	\left.\nabla_{\theta^{(j)}} \left [
	\frac{1}{2}\left\Vert y-g(\boldsymbol{\theta})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})
	\right ]
	\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}=0.
	\end{equation}
	
	Now we implicitly differentiate with respect to $\boldsymbol{\lambda}$ 
	\begin{equation}
	\label{eq:implicit_diff}
	\nabla_{\lambda}\left\{ \left.\nabla_{\theta^{(j)}}
	\left [
	\frac{1}{2}\left\Vert y-g(\boldsymbol{\theta})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})
	\right ]
	\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})}\right\} =0.
	\end{equation}
	
	Define the following matrices:
	\begin{align*}
		S = \left.\nabla_{\theta}\left [
		\frac{1}{2}\left\Vert y-g(\boldsymbol{\theta})\right\Vert _{T}^{2}
		\right ]
		\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda | T)}
	\end{align*}
	\todo{S is the second deriv!}
	\[
	D=\left.diag\left(\left\{ \nabla_{\boldsymbol{\theta}^{(j)}}^{2}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j)})\right\} _{j=1}^{J}\right)\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda|T)}
	\]
	
	
	\[
	M: \text{column } M_j = \left. \nabla_{\theta}P_{j}(\boldsymbol{\theta}^{(j)})
	\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda|T)}
	\]
	From the product rule and chain rule, we can then write the system of equations in \eqref{eq:implicit_diff} as
	\begin{equation}
	\label{eq:param_matrix}
	\left(\begin{array}{cccc}
	\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{1}(\boldsymbol{\lambda}) & \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{2}(\boldsymbol{\lambda}) & ... & \nabla_{\lambda}\hat{\boldsymbol{\theta}}_{p}(\boldsymbol{\lambda})\end{array}\right)=-M^{\top}\left(S+D\right)^{-1}.
	\end{equation}
	We now bound each column in $M$. From the \eqref{eq:grad_opt} and Cauchy Schwarz, we have 
	\begin{align*}
		\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert  & \le  \frac{1}{\lambda_{min}\sqrt{n_{T}}}\left\Vert y-g(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}\sqrt{\sum_{i=1}^{n_{T}}\left\Vert \nabla_{\theta^{(j)}}g_{j}(x_{i}|\boldsymbol{\theta}^{(j)})\right\Vert _{2}^{2}}.
	\end{align*}
	\todo{the sqrt nT got lost later on!}
	
	The norm of the gradients of $g_j$ can be bounded since $g_j$ is Lipschitz. Also, by definition of $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$, we have
	\begin{align*}
	\frac{1}{2}\left\Vert y-g(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}))\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}\left(\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda})\right) & \le \frac{1}{2}\left\Vert y-g(\boldsymbol{\theta}^{*})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}^{(j),*})\\
	& \le C_{\theta^{*},\Lambda}.
	\end{align*}
	
	Hence for all $j=1,...,J$
	\begin{align}
	\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert  & \le \frac{L}{\lambda_{min}}\sqrt{2C_{\theta^{*},\Lambda}}.
	\end{align}
	
	Now we bound the norm of $\nabla_\lambda \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$. From \eqref{eq:param_matrix}, we have for all $j=1,...,J$
	\begin{align}
	\|\nabla_{\lambda}\hat{\boldsymbol{\theta}}_{i}(\boldsymbol{\lambda})\| & =  \|M^{\top}\left(S+D\right)^{-1}e_{k}\|\\
	& \le \sum_{j=1}^{J}\left\Vert \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}(\lambda)}\right\Vert _{2}\left\Vert \left(S+D\right)^{-1}\right\Vert _{2}\\
	& \le  J\left(\frac{L}{\lambda_{min}}\sqrt{2C_{\theta^{*},\Lambda}}\right)\frac{1}{m}
	\end{align}
	\todo{make this calculate differently}
	where the last line follows from the fact that $(S + D)^{-1} \succeq m^{-1}I$.
	\todo{typo in the matrix ineq}
	Since the norm of the gradient is bounded, $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$ must be Lipschitz:
	\begin{equation}
	\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}')\right\Vert _{2}\le\frac{LJ^{3/2}\sqrt{2C_{\theta^{*},\Lambda}}}{m\lambda_{min}}\|\boldsymbol{\lambda}-\boldsymbol{\lambda}'\|_{2}.
	\end{equation}
	Finally the result \eqref{eq:param_add_lipschitz} clearly follows since $g_j(\boldsymbol{\theta})$ are Lipschitz in $\boldsymbol{\theta}$ with respect to $\| \cdot \|_\infty$.
	\todo{is it wrt infty norm? do we want to assume this?}
\end{proof}

\subsubsection{Proof for Lemma \ref{lemma:nonsmooth}}

\begin{proof}
	Consider any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$. 
	From Condition \ref{condn:nonsmooth1}, every point $\boldsymbol{\lambda} \in \Lambda_{smooth}$ is the center of a ball $B(\boldsymbol{\lambda})$ with nonzero radius where the differentiable space within $B(\boldsymbol{\lambda})$ is constant.
	Hence by Condition \ref{condn:nonsmooth2}, it can be shown that there must exist a countable
	set of points $\bar{\boldsymbol{\ell}} 
	%=\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}
	\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$ where $\boldsymbol{\lambda}^{(1)}, \boldsymbol{\lambda}^{(2)} \in \bar{\boldsymbol{\ell}}$
	such that the union of their differentiable neighborhoods cover  $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$ entirely:
	\[
	\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right) \subseteq \cup_{\ell \in \bar{\boldsymbol{\ell}}}B(\boldsymbol{\ell}).
	%\mu_{1}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})
	%\mu_{1}\left(\cup_{\ell \in \bar{\boldsymbol{\ell}}}B(\boldsymbol{\ell})
	%\cap\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right)=\left\Vert \mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right\Vert _{2}
	\]
	%where $\mu_{1}$ is the Lebesgue measure over  $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$.
	
	Consider the intersections of boundaries of the differentiable neighborhoods with the line segment:
	\[
	P=
	\left ( 
	\cup_{\ell \in \bar{\boldsymbol{\ell}}}
	\mbox{Bd}B\left(\boldsymbol{\ell}\right)
	\right )
	\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}).
	\]
	Every point $p\in P$ can be expressed as $\alpha_{p}\boldsymbol{\lambda^{(1)}}+(1-\alpha_{p})\boldsymbol{\lambda^{(2)}}$
	for some $\alpha_{p}\in[0,1]$. So points in $P$ can ordered by increasing $\alpha_{p}$ to get the sequence $\boldsymbol{p}^{(1)}, \boldsymbol{p}^{(2)}, ... $.
	By Condition \ref{condn:nonsmooth1}, the differentiable space of the training criterion
	is also constant over $\mathcal{L}\left(\boldsymbol{p}^{(i)},\boldsymbol{p}^{(i+1)}\right)$ since each of these sub-segments are a subset of $B(\boldsymbol{\ell})$ for some $\boldsymbol{\ell} \in \bar{\boldsymbol{\ell}}$. 
	
	Let the differentiable
	space over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
	be denoted $\Omega_{i}$. 
	By Condition \ref{condn:nonsmooth1}, the differentiable space is also a local optimality
	space. Let $U^{(i)}$ be an orthonormal basis of $\Omega_{i}$. For
	each $i$, we can express $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda} | T)$ for
	all $\boldsymbol{\lambda}\in\mbox{Int}\left\{ \mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)\right\} $
	as
	\[
	\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda} | T)=U^{(i)}\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda} | T)
	\]
	\[
	\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda} | T)=\arg\min_{\beta}L_{T}(U^{(i)}\boldsymbol{\beta},\boldsymbol{\lambda}).
	\]
	Apply Lemma \ref{lemma:param_add} with $\Lambda= \mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$, and modify the proofs to take directional derivatives
	along the columns of $U^{(i)}$ instead. Then there is a constant
	constant $c>0$ independent of $i$ such that for all $i=1,2...$,
	such that
	\[
	\left\Vert 
	\boldsymbol{\hat{\beta}}(\boldsymbol{p}^{(i)} | T)
	-\boldsymbol{\hat{\beta}}(\boldsymbol{p}^{(i)} | T)
	\right\Vert _{2}
	\le 
	c\|\boldsymbol{p}^{(i)}-\boldsymbol{p}^{(i+1)}\|_{2}.
	\]
	
	
	Finally, we can sum these inequalities. By the triangle inequality,
	\begin{align*}
	\left\Vert 
	\hat{\theta}(\boldsymbol{\lambda}^{(1)} |T)
	- \hat{\theta}(\boldsymbol{\lambda}^{(2)} |T)
	\right\Vert _{2} 
	& \le \sum_{i=1}^{\infty}\|\boldsymbol{\hat{\theta}}_{p^{(i)}}-\boldsymbol{\hat{\theta}}_{p^{(i+1)}}\|_{2}\\
	& \le \sum_{i=1}^{\infty}
	c\|\boldsymbol{p}^{(i)}-\boldsymbol{p}^{(i+1)}\|_{2}\\
	& = c\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}.
	\end{align*}
\end{proof}

\subsubsection{Proof for Lemma \ref{lemma:nonparam_smooth}}

\begin{proof}
	Let $H_{0} = \left \{
	j:\left\Vert \hat{g}_{j}(\boldsymbol{\lambda}^{(2)}|T)-\hat{g}_{j}(\boldsymbol{\lambda}^{(1)}|T)\right\Vert _{D^{(n)}} \ne 0\,\, \forall j = 1,...,J
	\right \}$.
	\todo{notation is confusing. not for all.}
	For all $j \in H_0$, let 
	\[
	h_{j}=
	\frac{\hat{g}_{j}(\boldsymbol{\lambda}^{(2)}|T)-\hat{g}_{j}(\boldsymbol{\lambda}^{(1)}|T)}{\left\Vert \hat{g}_{j}(\boldsymbol{\lambda}^{(2)}|T)-\hat{g}_{j}(\boldsymbol{\lambda}^{(1)}|T)\right\Vert _{D^{(n)}}}.
	\]
	For notational convenience, let $\hat{g}_{1,j} = \hat{g}_{j}(\boldsymbol{\lambda}^{(1)}|T)$. Consider the optimization problem
	\begin{equation}
	\hat{\boldsymbol{m}}(\boldsymbol{\lambda})=\left\{ \hat{m}_{j}(\boldsymbol{\lambda})\right\} _{j\in H_0}
	=\argmin_{m_{j} \in \mathbb{R}: j\in H_0}
	\frac{1}{2}
	\left \|y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right) \right \|_{T}^{2}
	+\sum_{j=1}^{J}\lambda_{j}
	P_{j} \left (\hat{g}_{1,j}+m_{j}h_{j} \right ).
	\end{equation}
	\todo{notation is confusing cause m is already defined}
	By the gradient optimality conditions, we have
	\begin{equation}
	\nabla_{m} \left .
	\left[\frac{1}{2}\|y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})\right] \right |_{m=\hat{m}(\lambda)}
	= 0.
	\label{eq:nonparam_grad_opt}
	\end{equation}
	Implicit differentiation with respect to $\boldsymbol{\lambda}$ gives us
	\begin{equation}
	\nabla_\lambda 
	\nabla_m
	\left . \left[
	\frac{1}{2}\|y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})\right] \right |_{m=\hat{m}(\lambda)}
	= 0.
	\label{eq:nonparam_imp_diff}
	\end{equation}
	Define the following matrices
	\[
	S:S_{ij}=\langle h_{j},h_{j}\rangle_{T}
	\]
	\todo{this doesnt look right. where is i?}
	
	\[
	D_{1}=diag\left(
	\left \{
	\left.\lambda_{j}\frac{\partial^{2}}{\partial m_{j}^{2}}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})\right|_{m=\hat{m}(\lambda)}
	\right \}_{j=1}^J
	\right)
	\]
	
	
	\[
	D_{2}=diag\left(\left \{ \left.
	\frac{\partial}{\partial m_{j}}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})\right|_{m=\hat{m}(\lambda)}
	\right \}_{j=1}^J
	\right)
	\]
	
	
	\[
	M:\mbox{ column }M_{j}=\nabla_{\lambda}\hat{m}_{j}(\lambda) \quad \forall j=1,...,J.
	\]
	
	
	The system of equations from \eqref{eq:nonparam_imp_diff} can be written as  $M=D_{2}\left(S+D_{1}\right)^{-1}$.
	
	Now we bound each element in $D_2$. From \eqref{eq:nonparam_grad_opt} and Cauchy Schwarz, we have for all $k=1,...,J$
	\begin{equation}
	\left|\frac{\partial}{\partial m_{k}}P_{k}(\hat{g}_{1,k}+m_{k}h_{k})\right|_{m=\hat{m}(\lambda)}
	\le 
	\frac{1}{\lambda_{min}}
	\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)
	\right\Vert _{T}\|h_{k}\|_{T}.
	\end{equation}
	We note that $\|h_{k}\|_{T} \le \sqrt{\frac{n_{D}}{n_{T}}}$. Also, by definition of $\hat{m}(\boldsymbol{\lambda})$,
	\begin{align*}
	\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j})
	& \le \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\hat{g}_{1,j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j})\\
	& \le C_{\Lambda}^* + J\lambda_{max}\max_{j=1:J}P_{j}(\hat{g}_{1,j}).
	\end{align*}
	\todo{this step might be hard to follow. add more explanation}
	Hence every diagonal element in $D_2$ is bounded above by
	\begin{equation}
	\frac{1}{\lambda_{min}}
	\sqrt{2C_{\Lambda}^* \frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}.
	\end{equation}
	
	
	Now we can bound the norm of the gradient $\boldsymbol{\hat{m}}_k(\boldsymbol{\lambda})$ for $k \in H_0$
	\begin{eqnarray}
	\|\nabla_{\lambda}\hat{m}_{k}(\boldsymbol{\lambda})\|
	& = & \left\Vert D_{2}\left(S+D_{1}\right)^{-1}e_{k}\right\Vert \\
	& \le & \frac{1}{\lambda_{min}}
	\sqrt{2C_{\Lambda}^*
		\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\left\Vert \left(S+D_{1}\right)^{-1}e_{k}\right\Vert.
	\end{eqnarray}
	By the assumption in \eqref{eq:gateuax}, $e_{k}^\top \left(S+D_{1}\right) e_{k} \ge m$.
	
	By the mean value theorem and Cauchy Schwarz, we have
	\todo{MVT or mean value ineq?}
	\begin{equation}
	\left|\hat{m}_{j}(\boldsymbol{\lambda}^{(2)})-\hat{m}_{j}(\boldsymbol{\lambda}^{(1)})\right| 
	\le \left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \frac{1}{d\lambda_{min}}\sqrt{2C^*_\Lambda \frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}.
	\end{equation}
	\todo{what is d?}
	Since $
	\left|
	\hat{m}_{j}(\boldsymbol{\lambda}^{(2)})-\hat{m}_{j}(\boldsymbol{\lambda}^{(1)})
	\right|  =
	\left \| 
	\hat{g}_{j}(\boldsymbol{\lambda}^{(2)}|T)-\hat{g}_{j}(\boldsymbol{\lambda}^{(1)}|T)
	\right  \|_{D^{(n)}}
	$, the result in \eqref{eq:nonparam_lipshitz_thrm} follows.
\end{proof}


\bibliographystyle{unsrtnat}
\bibliography{hyperparam-theory-appendix}

\end{document}
