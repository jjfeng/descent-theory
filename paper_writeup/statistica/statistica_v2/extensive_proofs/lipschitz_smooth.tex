%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}

\makeatother

\usepackage{babel}
  \providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}

\begin{document}
We show that smooth penalties result in estimated model parameters
that are Lipschitz.
\begin{lem}
Suppose that $g(x|\boldsymbol{\theta})$ satisfies the following Lipschitz
condition:
\[
\left|g\left(x|\boldsymbol{\theta}^{(1)}\right)-g\left(x|\boldsymbol{\theta}^{(2)}\right)\right|\le\ell(x)\|\boldsymbol{\theta}^{(1)}-\boldsymbol{\theta}^{(2)}\|_{2}.
\]


Let $\Lambda=\left[\lambda_{\min},\lambda_{\max}\right]^{J}$ where
$\lambda_{\max}>\lambda_{\min}>0$. Let $T\equiv D^{(n_{T})}$ be
a fixed set of training data. Suppose the penalized loss function
$L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)$ has
a unique minimizer $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$
for every $\boldsymbol{\lambda}\in\Lambda$, e.g. 

\begin{equation}
\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}|T\right)=\arg\min_{\boldsymbol{\theta}\in\mathbb{R}^{p}}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\label{eq:param_add_estimator-1}
\end{equation}
Suppose $L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)$
is twice-differentiable with respect to $\boldsymbol{\theta}$. Suppose
there exists a constant $m(T)>0$ such that the Hessian of the penalized
training criterion satisfies 
\begin{equation}
\left.\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})\right|_{\theta=\hat{\theta}(\boldsymbol{\lambda}|T)}\succeq m(T)\boldsymbol{I}\quad\forall\boldsymbol{\lambda}\in\Lambda
\end{equation}


\noindent Let $C_{\Lambda}^{*}=\lambda_{max}\sum_{j=1}^{J}P_{j}(\boldsymbol{\theta}^{(j),*})$\textup{.}
Then, for any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda$,
we have 
\begin{equation}
\left|g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(1)}}|T)\right)-g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda^{(2)}}|T)\right)\right|\le J\frac{m(T)}{\lambda_{\min}}\sqrt{\frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}}\ell^{2}(x)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert _{2}.\label{eq:param_add_lipschitz}
\end{equation}
\end{lem}
\begin{proof}
As $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$ is the minimizer
over $\mathbb{R}^{p}$ and $L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})$
is differentiable wrt $\boldsymbol{\theta}$, then 
\[
\left.\nabla_{\theta}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}=0.
\]


Implicit differentiation of this equation with respect to $\boldsymbol{\lambda}$
gives us
\begin{eqnarray*}
 &  & \nabla_{\lambda}\left[\left.\nabla_{\theta}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]\\
 & = & \left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)+\left\{ \left.\nabla_{\theta}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\theta=\hat{\theta}(\lambda|T)}\right\} _{j=1}^{J}\\
 & = & 0.
\end{eqnarray*}
where the second term in the second line denotes each column of the
matrix. Rearranging, we have
\begin{equation}
\frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)=-\left[\left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]^{-1}\left\{ \left.\nabla_{\theta}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\theta=\hat{\theta}(\lambda|T)}\right\} _{j=1}^{J}.\label{eq:grad_theta}
\end{equation}
Note that 
\[
\nabla_{\theta}P_{j}(\boldsymbol{\theta}^{(j)})=\left[\begin{array}{c}
\boldsymbol{0}\\
\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\\
\boldsymbol{0}
\end{array}\right]
\]
for appropriately sized zero vectors. From the gradient conditions,
\begin{eqnarray}
 &  & \left.\nabla_{\theta^{(j)}}P_{j}(\boldsymbol{\theta}^{(j)})\right|_{\theta=\hat{\theta}(\lambda|T)}\nonumber \\
 & = & -\frac{1}{\lambda_{j}}\left.\nabla_{\theta^{(j)}}\frac{1}{2}\|y-g(x|\theta)\|_{T}^{2}\right|_{\theta=\hat{\theta}(\lambda|T)}\\
 & = & -\frac{1}{\lambda_{j}}\sum_{i=1}^{n_{T}}\left(y_{i}-g\left(x_{i}|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right)\left.\nabla_{\theta^{(j)}}g_{j}\left(x_{i}\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}.\label{eq:gradient_pen}
\end{eqnarray}


Plugging \ref{eq:gradient_pen} into \ref{eq:grad_theta} gives us
\[
\frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda}|T)=\frac{1}{\lambda_{j}}\sum_{i=1}^{n_{T}}\left(y_{i}-g\left(x_{i}|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right)\left[\left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]^{-1}\left.\nabla_{\theta^{(j)}}g_{j}\left(x_{i}\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}.
\]
Taking the norm, we have
\begin{eqnarray*}
\left\Vert \frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda}|T)\right\Vert _{2} & = & \frac{1}{\lambda_{j}}\left\Vert \sum_{i=1}^{n_{T}}\left(y_{i}-g\left(x_{i}|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right)\left[\left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]^{-1}\left.\nabla_{\theta^{(j)}}g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right\Vert _{2}\\
 & \le & \frac{1}{\lambda_{j}}\sum_{i=1}^{n_{T}}\left|y_{i}-g\left(x_{i}|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right|\left\Vert \left[\left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]^{-1}\left.\nabla_{\theta^{(j)}}g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right\Vert _{2}\\
 & \le & \frac{1}{\lambda_{\min}}\left\Vert y-g\left(x|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right\Vert _{T}\left\Vert \left\Vert \left[\left.\nabla_{\theta}^{2}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right]^{-1}\left.\nabla_{\theta^{(j)}}g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right\Vert _{2}\right\Vert _{T}\\
 & \le & \frac{m(T)}{\lambda_{\min}}\left\Vert y-g\left(x|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right\Vert _{T}\left\Vert \left\Vert \left.\nabla_{\theta^{(j)}}g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)\right|_{\theta=\hat{\theta}(\lambda|T)}\right\Vert _{2}\right\Vert _{T}
\end{eqnarray*}


where we used triangle inequality, then Cauchy-Schwarz, and then the
assumption that the minimum eigenvalue is bounded from below. From
our Lipschitz assumption, we have
\[
\left\Vert \nabla_{\theta_{k}^{(j)}}g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)\right\Vert _{2}=\left\Vert \lim_{\delta\rightarrow0}\frac{g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}+\delta\boldsymbol{e}_{k}\right)-g_{j}\left(x\mid\boldsymbol{\theta}^{(j)}\right)}{\delta}\right\Vert _{2}\le\ell(x).
\]
Thus
\[
\left\Vert \frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda}|T)\right\Vert _{2}\le\frac{m(T)}{\lambda_{\min}}\left\Vert y-g\left(x|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right\Vert _{T}\ell(x).
\]
Moreover, by the definition of $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$,
we have 
\begin{eqnarray*}
\frac{1}{2}\left\Vert y-g\left(x|\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)\right)\right\Vert _{T}^{2} & \le & \frac{1}{2}\left\Vert y-g(x\mid\theta^{*})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}^{*(j)})\\
 & = & \frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}.
\end{eqnarray*}
Therefore
\[
\left\Vert \frac{\partial}{\partial\lambda}\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda}|T)\right\Vert _{2}\le\frac{m(T)}{\lambda_{\min}}\sqrt{\frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}}\ell(x)
\]
and by the Lipschitz assumption again,
\[
\left\Vert \frac{\partial}{\partial\lambda}g\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}(\boldsymbol{\lambda}|T)\right)\right\Vert _{2}\le\frac{m(T)}{\lambda_{\min}}\sqrt{\frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}}\ell^{2}(x).
\]
Finally, by the mean value theorem/inequality, we have 
\[
\left|g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(1)}|T\right)\right)-g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(2)}|T\right)\right)\right|\le\frac{m(T)}{\lambda_{\min}}\sqrt{\frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}}\ell^{2}(x)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert _{2}.
\]
By triangle inequality
\begin{eqnarray*}
\left|g\left(x\mid\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(1)}|T\right)\right)-g\left(x\mid\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(2)}|T\right)\right)\right| & = & \left|\sum_{j=1}^{J}g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(1)}|T\right)\right)-g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(2)}|T\right)\right)\right|\\
 & \le & \sum_{j=1}^{J}\left|g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(1)}|T\right)\right)-g_{j}\left(x\mid\hat{\boldsymbol{\theta}}^{(j)}\left(\boldsymbol{\lambda}^{(2)}|T\right)\right)\right|\\
 & \le & J\frac{m(T)}{\lambda_{\min}}\sqrt{\frac{1}{2}\|\epsilon\|_{T}^{2}+C_{\Lambda}^{*}}\ell^{2}(x)\left\Vert \boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\right\Vert _{2}.
\end{eqnarray*}
\end{proof}

\end{document}
