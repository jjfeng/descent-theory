%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}

\makeatother

\usepackage{babel}
  \providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}

\begin{document}
We show that non-smooth penalties can result in estimated model paramaters
that are Lipschitz. Before we move on, we prove that the estimated
model parameters are continuous in $\boldsymbol{\lambda}$.
\begin{lem}
Let
\[
\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})=\arg\min_{\theta\in\mathbb{R}^{p}}L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda}).
\]
If the penalized training loss $L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})$
is continuous in $\boldsymbol{\theta},\boldsymbol{\lambda}$ and there
exists a unique minimizer $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
for each $\boldsymbol{\lambda}\in\Lambda$, then the fitted model
parameters are continuous with respect to $\boldsymbol{\lambda}$
over all $\Lambda$.\end{lem}
\begin{proof}
Suppose $\hat{\theta}(\boldsymbol{\lambda})$ is not continuous in
$\boldsymbol{\lambda}$ for all $\boldsymbol{\lambda}\in\Lambda$.
Then there exists a $\boldsymbol{\lambda}\in\Lambda$ and $\delta>0$
such that 
\[
\lim_{\lambda'\rightarrow\lambda}\|\hat{\theta}(\boldsymbol{\lambda}')-\hat{\theta}(\boldsymbol{\lambda})\|_{2}>\delta.
\]


Notice that 
\begin{eqnarray*}
L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda}')-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda}') & = & L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda}')-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda})\\
 &  & +L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda})-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda})\\
 &  & +L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda})-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda}').
\end{eqnarray*}


By the assumption that there is a unique minimizer, the LHS is negative
for all $\boldsymbol{\lambda}'\ne\boldsymbol{\lambda}$.

Taking the limit as $\boldsymbol{\lambda}'\rightarrow\boldsymbol{\lambda}$,
the first and third lines in the RHS approach zero as $L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})$
is continuous with respect to $\boldsymbol{\lambda}$. Therefore
\[
\lim_{\lambda'\rightarrow\lambda}L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda}')-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda})=L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'),\boldsymbol{\lambda})-L_{T}(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}),\boldsymbol{\lambda}).
\]


By our assumption that there is a unique minimizer, the RHS is positive.

This is a contradiction, as the LHS is negative for all $\boldsymbol{\lambda}'\ne\boldsymbol{\lambda}$
whereas the RHS is positive. Thus $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
must be continuous.

(Do we need this proof? Is it satisfied if we just do an implicit
function theorem or inverse function theorem? Those theorems require
differentiability though...)

Before showing that the estimated model parameters are Lipschitz,
recall our definitions.
\end{proof}
\textbf{Definition 1:} The differentiable space of function $f:\mathbb{R}^{p}\mapsto\mathbb{R}$
at $\boldsymbol{\theta}$ is 
\begin{equation}
\Omega^{f}(\boldsymbol{\theta})=\left\{ \boldsymbol{\beta}\mid\lim_{\epsilon\rightarrow0}\frac{f(\boldsymbol{\theta}+\epsilon\boldsymbol{\beta})-f(\boldsymbol{\theta})}{\epsilon}\text{ exists }\right\} .
\end{equation}


\textbf{Definition 2:} Let $f(\cdot,\cdot):\mathbb{R}^{p}\times\mathbb{R}^{J}\mapsto\mathbb{R}$
be a function with a unique minimizer. $S\subseteq\mathbb{R}^{p}$
is a local optimality space of $f$ over $W\subseteq\mathbb{R}^{J}$
if 
\begin{equation}
\arg\min_{\boldsymbol{\theta}\in\mathbb{R}^{p}}f(\boldsymbol{\theta},\boldsymbol{\lambda})=\arg\min_{\boldsymbol{\theta}\in S}f(\boldsymbol{\theta},\boldsymbol{\lambda})\quad\forall\boldsymbol{\lambda}\in W.
\end{equation}


We also need the following conditions:

\textbf{Condition} \textbf{1}: There is a subset $\Lambda_{smooth}\subseteq\Lambda$
such that for every $\boldsymbol{\lambda}\in\Lambda_{smooth}$, there
exists a ball $B(\boldsymbol{\lambda})$ with nonzero radius centered
at $\boldsymbol{\lambda}$ such that 
\begin{itemize}
\item For all $\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})$, the training
criterion $L_{T}$ is twice differentiable at $(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}'|T),\boldsymbol{\lambda}')$
along directions in $\Omega^{L_{T}(\cdot,\cdot)}\left(\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T),\boldsymbol{\lambda}\right)$. 
\item $\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}|T\right)\right)$
is a local optimality space for $L_{T}\left(\cdot,\boldsymbol{\lambda}\right)$
over $B(\boldsymbol{\lambda})$. 
\end{itemize}
Now we show that the estimated model parameters are Lipschitz in $\boldsymbol{\lambda}$.

\textbf{Condition 2: }$\mu\left(\Lambda_{smooth}\right)=0$ where
$\mu$ is the Lebesgue measure in $\mathbb{R}^{J}$.

Now we define some notation. Let $\mathcal{L}(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)})$
be the line segment connecting $\boldsymbol{\lambda}^{(1)}$ and $\boldsymbol{\lambda}^{(2)}$.
Let $\mu_{1}(z)$ be the 1-dimensional Lebesgue measure in the direction
of $z$ (so if $z$ is a continuous line segment, $\mu_{1}(z)=\|z\|_{2}$;
if $z$ is composed of multiple line segments $z_{i}$, then $\mu(z)=\sum\mu(z_{i})$).

We first show consider the set of $\boldsymbol{\lambda}$ where $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda})$
is well-behaved.
\begin{lem}
Suppose that $g(x|\boldsymbol{\theta})$ satisfies the following Lipschitz
condition:
\[
\left|g\left(x|\boldsymbol{\theta}^{(1)}\right)-g\left(x|\boldsymbol{\theta}^{(2)}\right)\right|\le\ell(x)\|\boldsymbol{\theta}^{(1)}-\boldsymbol{\theta}^{(2)}\|_{2}.
\]


Let $T\equiv D^{(n_{T})}$ be a fixed set of training data. Suppose
the penalized loss function $L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}\right)$
has a unique minimizer $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$
for every $\boldsymbol{\lambda}\in\Lambda$. Let $\boldsymbol{U}_{\lambda}$
be an orthonormal matrix with columns forming a basis for the differentiable
space of $L_{T}(\cdot,\boldsymbol{\lambda})$ at $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$.
Suppose there exists a constant $m(T)>0$ such that the Hessian of
the penalized training criterion at the minimizer taken with respect
to the directions in $\boldsymbol{U}_{\lambda}$ satisfies 
\begin{equation}
\left._{U_{\lambda}}\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})\right|_{\theta=\hat{\theta}(\boldsymbol{\lambda})}\succeq m(T)\boldsymbol{I}\quad\forall\boldsymbol{\lambda}\in\Lambda
\end{equation}
where \textup{$\boldsymbol{I}$ is the identity matrix.} Suppose $\Lambda_{smooth}\subseteq\Lambda$
satisfies Condition 1. Define 
\[
\Lambda_{ext}=\left\{ (\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}):\mu_{1}\left(\mathcal{L}(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)})\cap\Lambda_{smooth}^{c}\right)>0\right\} .
\]
Then any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda_{ext}^{c}$
satisfies the same Lipschitz condition as that in the Lemma for smooth
penalty functions (see other document).\end{lem}
\begin{proof}
Consider any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$.
From Condition 1, every point $\boldsymbol{\lambda}\in\Lambda_{smooth}$
is the center of a ball $B(\boldsymbol{\lambda})$ with nonzero radius
where the differentiable space within $B(\boldsymbol{\lambda})$ is
constant. Hence by Condition 2, it can be shown that there must exist
a countable set of points $%\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}
\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$ where $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in%\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}
$ such that the union of their differentiable neighborhoods cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
entirely: 
\[
\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\subseteq\cup_{i=1}^{\infty}B\left(\boldsymbol{\ell}^{(i)}\right)
\]
Consider the intersections of boundaries of the differentiable neighborhoods
with the line segment: 
\[
P=\left(\cup_{i=1}^{\infty}B\left(\boldsymbol{\ell}^{(i)}\right)\right)\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}).
\]
Every point $p\in P$ can be expressed as $\alpha_{p}\boldsymbol{\lambda^{(1)}}+(1-\alpha_{p})\boldsymbol{\lambda^{(2)}}$
for some $\alpha_{p}\in[0,1]$. So points in $P$ can ordered by increasing
$\alpha_{p}$ to get the sequence $\boldsymbol{p}^{(1)},\boldsymbol{p}^{(2)},...$.
By Condition 1, the differentiable space of the training criterion
is also constant over $\mathcal{L}\left(\boldsymbol{p}^{(i)},\boldsymbol{p}^{(i+1)}\right)$
since each of these sub-segments are contained in some $B(\boldsymbol{\ell}^{(i)})$
for $i\in\mathbb{N}$.

Let the differentiable space over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
be denoted $\Omega_{i}$. By Condition 1, the differentiable space
is also a local optimality space. Let $U^{(i)}$ be an orthonormal
basis of $\Omega_{i}$. For each $i$, we can express $\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)$
for all $\boldsymbol{\lambda}\in\mbox{Int}\left\{ \mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)\right\} $
as 
\[
\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}|T)=U^{(i)}\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda}|T)
\]
\[
\hat{\boldsymbol{\beta}}(\boldsymbol{\lambda}|T)=\arg\min_{\beta}L_{T}(U^{(i)}\boldsymbol{\beta},\boldsymbol{\lambda}).
\]
Apply the Lemma (Lipschitz for smooth penalty functions) with $\Lambda=\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$,
and modify the proofs to take directional derivatives along the columns
of $U^{(i)}$ instead. Then there is a Lipschitz constant $c>0$ independent
of $i$ such that for all $i=1,2...$, such that 
\[
\left\Vert \boldsymbol{\hat{\beta}}(\boldsymbol{p}^{(i)}|T)-\boldsymbol{\hat{\beta}}(\boldsymbol{p}^{(i)}|T)\right\Vert _{2}\le c\|\boldsymbol{p}^{(i)}-\boldsymbol{p}^{(i+1)}\|_{2}.
\]


We can sum these inequalities by the triangle inequality as follows:
\begin{align*}
\left\Vert \hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(1)}|T)-\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(2)}|T)\right\Vert _{2} & \le\sum_{i=1}^{\infty}\|\boldsymbol{\hat{\theta}}_{p^{(i)}}-\boldsymbol{\hat{\theta}}_{p^{(i+1)}}\|_{2}\\
 & \le\sum_{i=1}^{\infty}c\|\boldsymbol{p}^{(i)}-\boldsymbol{p}^{(i+1)}\|_{2}\\
 & =c\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}.
\end{align*}


Finally, by the Lipschitz assumption, we have that 
\[
\left|g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(1)}|T)\right)-g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(2)}|T)\right)\right|\le c\ell(x)\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}.
\]

\end{proof}
In order to extend this Lipschitz result to all of $\Lambda$, we
show that $\Lambda_{ext}$ is a set with measure zero.
\begin{lem}
Suppose Condition 2. Then $\mu_{2J}(\Lambda_{ext})=0$ where $\mu_{J}$
is the Lebesgue measure in $\mathbb{R}^{2J}$ and $\Lambda_{ext}$
was defined in Lemma 2.\end{lem}
\begin{proof}
Suppose that $\mu_{2J}(\Lambda_{ext})>0$. If this is the case, we
claim there exists an $\epsilon>0$ and some non-empty box contained
in $\Lambda_{ext}$ with one corner as $\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(3)}\right)$
and the other corner as $\left(\boldsymbol{\lambda}^{(2)},\boldsymbol{\lambda}^{(4)}\right)$,
denoted $\text{Box}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)},\boldsymbol{\lambda}^{(3)},\boldsymbol{\lambda}^{(4)}\right)\subseteq\Lambda_{ext}$,
such that 
\begin{equation}
\inf_{(\lambda',\lambda'')\in\text{Box}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)},\boldsymbol{\lambda}^{(3)},\boldsymbol{\lambda}^{(4)}\right),\lambda'\ne\lambda''}\frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\right)}>\epsilon>0.\label{eq:inf_measure}
\end{equation}


To see why such an $\epsilon$ and box must exist, consider a $\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)\in\Lambda_{ext}$
where 
\[
\frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)\right)}>\delta>0.
\]
Let $\left\{ B_{i}\right\} $ be a sequence of boxes shrinking monotonically
(as in $B_{i+1}\subset B_{i}$) towards $\text{Box}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)},\boldsymbol{\lambda}^{(2)}\right)$.
By the monotone convergence theorem, 
\begin{eqnarray*}
 &  & \lim_{i\rightarrow\infty}\inf_{(\lambda',\lambda'')\in B_{i},\lambda'\ne\lambda''}\frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\right)}\\
 & = & \inf_{(\lambda',\lambda'')\in\text{Box}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)},\boldsymbol{\lambda}^{(2)}\right),\lambda'\ne\lambda''}\frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\right)}\\
 & = & \frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)\right)}>\delta>0
\end{eqnarray*}
since $\inf$ of a function is measurable, the quotient of measurable
functions is measurable if the denominator is never zero.

By the definition of limits, there must exist a non-empty box $B_{i}$
in the sequence of monotonically shrinking boxes such that 
\[
\inf_{(\lambda',\lambda'')\in B_{i},\lambda'\ne\lambda''}\frac{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\cap\Lambda_{smooth}^{c}\right)}{\mu_{1}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\right)}>\delta/2>0.
\]


Given that there exists a box $B$ with edge lengths $b_{1}$ and
$b_{2}$ satisfying \ref{eq:inf_measure}, then 
\[
\mu\left(\Lambda_{smooth}^{c}\right)\ge\mu\left(\cup_{(\lambda',\lambda'')\in B}\left(\mathcal{L}\left(\boldsymbol{\lambda}',\boldsymbol{\lambda}''\right)\cap\Lambda_{smooth}^{c}\right)\right)>\epsilon\left(b_{1}\vee b_{2}\right)>0.
\]


This contradicts our assumption that $\mu\left(\Lambda_{smooth}^{c}\right)$
has measure zero.
\end{proof}
Finally, combining Lemma 1, 2, and 3, we can show that the Lipschitz
condition is satisfied over all of $\Lambda$.
\begin{lem}
Assume the same conditions as Lemma 2. In addition, assume Condition
2 holds. Then any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda$
satisfies the same Lipschitz condition as Lemma 2.\end{lem}
\begin{proof}
By Lemma 2, any $\left(\boldsymbol{\lambda}^{'},\boldsymbol{\lambda}^{''}\right)\in\Lambda_{ext}^{c}$
satisfies the Lipschitz condition. By Lemma 3, $\mu_{2J}\left(\Lambda_{ext}\right)=0$.
So for any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda$,
there is a sequence $\left(\boldsymbol{\lambda}^{(1,i)},\boldsymbol{\lambda}^{(2,i)}\right)\in\Lambda_{ext}^{c}$
such that $\lim_{i\rightarrow\infty}\left(\boldsymbol{\lambda}^{(1,i)},\boldsymbol{\lambda}^{(2,i)}\right)=\left(\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right)$.
Then for any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda$,
we have 
\begin{eqnarray*}
\|\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(1)}\right)-\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(2)}\right)\|_{2} & = & \lim_{i\rightarrow\infty}\|\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(1,i)}\right)-\hat{\boldsymbol{\theta}}\left(\boldsymbol{\lambda}^{(2,i)}\right)\|_{2}\\
 & \le & \lim_{i\rightarrow\infty}c\|\boldsymbol{\lambda}^{(1,i)}-\boldsymbol{\lambda}^{(2,i)}\|_{2}\\
 & = & c\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}
\end{eqnarray*}
where $c$ is the Lipschitz constant in Lemma 2. Finally, by the Lipschitz
assumption, we have that 
\[
\left|g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(1)}|T)\right)-g\left(x\mid\hat{\boldsymbol{\theta}}(\boldsymbol{\lambda}^{(2)}|T)\right)\right|\le c\ell(x)\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}.
\]
\end{proof}

\end{document}
