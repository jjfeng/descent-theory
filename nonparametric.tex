%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[landscape]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\title{Proofs for Smoothness of Non-Parametric Regression Models}

\maketitle

\section*{Intro}

In this document, we consider nonparametric regression models $g$
from function class $\mathcal{G}$. Throughout, we will suppose that
the projection of the true model into the model space $\mathcal{G}$
is $g^{*}$.

We are interested in establishing inequalities of the form 
\[
\|\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(2)}\right)-\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(1)}\right)\|_{D}\le C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\]



\subsubsection*{Document Outline}

Let $D$ be some set of observed covariates (it could be the training
and validation sets combined or just the validation set).

We prove smoothness for two nonparametric regression examples:
\begin{enumerate}
\item Additive model (no ridge!)
\[
\hat{g}\left(\cdot|\boldsymbol{\lambda}\right)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(g_{j})\right)
\]

\item Multiple penalties for a single model
\[
\hat{g}\left(\cdot|\boldsymbol{\lambda}\right)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(g)+\frac{w}{2}\|g\|_{D}^{2}\right)
\]


\begin{enumerate}
\item This regression problem is complicated and we may want to just leave
it out of the real paper. This regression model will give two possible
smoothness conditions

\begin{enumerate}
\item $\|\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(2)}\right)-\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(1)}\right)\|_{D}^{2}\le C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}$
\item $\|\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(2)}\right)-\hat{g}\left(\cdot|\boldsymbol{\lambda}^{(1)}\right)\|_{D}\le C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}$
\end{enumerate}
\end{enumerate}
\end{enumerate}
We also note that Sobolev is an example of a non-parametric additive
regression model that satisfies the theorem conditions.

We note that there is a functional definition for convexity using
Gateaux derivatives. Refer to Convex Functional Analysis, Authors:
Andrew J. Kurdila, Michael Zabarankin


\section{Additive Model}

Consider the problem 

\[
\mathcal{G}(T)=\left\{ \hat{g}\left(\cdot|\boldsymbol{\lambda}\right)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(g_{j})\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$.

For all $j=1,...,J$, suppose the penalty functions $P_{j}$ are convex
and twice-differentiable. Suppose there is a $d>0$ such that the
second Gateaux derivative of the training criterion at the minimizer
is strongly convex. That is, it satisfies 
\[
\left\langle \left.D_{g}^{2}\left(\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(g_{j})\right)\right|_{g=\hat{g}(\lambda)}\circ h,h\right\rangle \ge d\mbox{ }\forall h\in\mathcal{G}\mbox{ and }\|h\|_{D}=1
\]


Let
\[
C_{g^{*},\Lambda}=\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}^{*}\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}P_{j}(g_{j}^{*})
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have for all $j=1,...,J$

\[
\left\Vert \hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{D}\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \frac{1}{d\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]



\subsubsection*{Proof}

Let the set of additive components with nonzero differences be denoted
\[
H_{nonzero}=\left\{ j:\|\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})\|_{D}>0\right\} 
\]


For every $j\in H_{nonzero}$, let 
\[
h_{j}=\frac{\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})}{\left\Vert \hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{D}}
\]
For notational convenient, let $\hat{g}_{1,j}(\cdot)=\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})$.

We consider the optimization problem restricted to the set of non-zero
differences

\[
\hat{\boldsymbol{m}}(\boldsymbol{\lambda})=\left\{ \hat{m}_{j}(\boldsymbol{\lambda})\right\} _{j\in H_{nonzero}}=\arg\min_{m_{j}:j\in H_{nonzero}}\frac{1}{2}\|y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})
\]


\textbf{1. Calculate $\nabla_{\lambda}\hat{m}_{j}(\lambda)$}

By the gradient optimality conditions, the gradient of the objective
with respect to $m_{\ell}$ for all $\ell\in H_{nonzero}$

\begin{eqnarray*}
 &  & \frac{\partial}{\partial m_{\ell}}\left[\frac{1}{2}\|y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j}+m_{j}h_{j})\right]_{m=\hat{m}(\lambda)}\\
 & = & \left.\left\langle y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right),h_{\ell}\right\rangle _{T}+\lambda_{\ell}\frac{\partial}{\partial m_{\ell}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right|_{m=\hat{m}(\lambda)}\\
 & = & 0
\end{eqnarray*}


Now we implicitly differentiate with respect to $\lambda_{k}$ for
all $k\in H_{nonzero}$ to get 
\begin{eqnarray*}
 &  & \frac{\partial}{\partial\lambda_{k}}\left[\left\langle y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+m_{j}h_{j}\right),h_{\ell}\right\rangle _{T}+\lambda_{\ell}\frac{\partial}{\partial m_{\ell}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right]_{m=\hat{m}(\lambda)}\\
 & = & \left.\sum_{j=1}^{J}\left[\left\langle h_{j},h_{\ell}\right\rangle _{T}+1[\ell=j]\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right]\frac{\partial\hat{m}_{j}(\lambda)}{\partial\lambda_{k}}+1\left[\ell=k\right]\frac{\partial}{\partial m_{\ell}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right|_{m=\hat{m}(\lambda)}\\
 & = & 0
\end{eqnarray*}


Define the following square matrices 
\[
S:S_{ij}=\langle h_{j},h_{\ell}\rangle_{T}\forall\ell,j\in H_{nonzero}
\]
\[
D_{1}=diag\left(\left.\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right|_{m=\hat{m}(\lambda)}\forall\ell\in H_{nonzero}\right)
\]


\[
D_{2}=diag\left(\left.\frac{\partial}{\partial m_{\ell}}P_{\ell}(\hat{g}_{1,\ell}+m_{\ell}h_{\ell})\right|_{m=\hat{m}(\lambda)}\forall\ell\in H_{nonzero}\right)
\]


\[
M:\mbox{ column }M_{j}=\nabla_{\lambda}\hat{m}_{j}(\lambda)\forall j\in H_{nonzero}
\]


From the implicit differentiation equations, we have the following
system of equations: 
\[
M=D_{2}\left(S+D_{1}\right)^{-1}
\]


\textbf{2. We bound every diagonal element in $D_{2}$}:

We first bound $\left|\frac{\partial}{\partial m_{k}}P_{k}(\hat{g}_{1,k}+m_{k}h_{k})\right|$
for all $k\in H_{nonzero}$.

Note that from the gradient optimality conditions, we have that for
all $k=1,...,J$

\begin{eqnarray*}
\left|\frac{\partial}{\partial m_{k}}P_{k}(\hat{g}_{1,k}+m_{k}h_{k})\right|_{m=\hat{m}(\lambda)} & = & \left|\frac{1}{\lambda_{k}}\left\langle y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right),h_{k}\right\rangle _{T}\right|\\
 & \le & \frac{1}{\lambda_{min}}\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}\|h_{k}\|_{T}\\
 & \le & \frac{1}{\lambda_{min}}\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}\sqrt{\frac{n_{D}}{n_{T}}}
\end{eqnarray*}


where the last line uses the fact that
\[
\|h_{k}\|_{T}=\frac{\left\Vert \hat{g}_{k}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{k}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{T}}{\left\Vert \hat{g}_{k}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{k}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{D}}\le\sqrt{\frac{n_{D}}{n_{T}}}
\]
 

We can bound $\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,k}+\hat{m}_{k}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}$
using the basic inequality
\begin{eqnarray*}
 &  & \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j})\\
 & \le & \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\hat{g}_{1,j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\hat{g}_{1,j})\\
 & = & \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\hat{g}_{1,j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}P_{j}(\hat{g}_{1,j})+\sum_{j=1}^{J}\left(\lambda_{j}-\lambda_{j}^{(1)}\right)P_{j}(\hat{g}_{1,j})\\
 & \le & \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}^{*}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}P_{j}(g_{j}^{*})+J\lambda_{max}\max_{j=1:J}P_{j}(\hat{g}_{1,j})\\
 & = & C_{g^{*},\Lambda}+J\lambda_{max}\max_{j=1:J}P_{j}(\hat{g}_{1,j})
\end{eqnarray*}


To bound $\max_{j=1:J}P_{j}(\hat{g}_{1,j})$, we also use the basic
inequality
\begin{eqnarray*}
\lambda_{min}\max_{j=1:J}P_{j}(\hat{g}_{1,j}) & \le & \frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\hat{g}_{1,j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}P_{j}(\hat{g}_{1,j})\\
 & \le & C_{g^{*},\Lambda}
\end{eqnarray*}


Putting the two above inequalities together, we get
\[
\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}^{2}\le C_{g^{*},\Lambda}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\implies\left\Vert y-\sum_{j=1}^{J}\left(\hat{g}_{1,j}+\hat{m}_{j}(\boldsymbol{\lambda})h_{j}\right)\right\Vert _{T}\le\sqrt{2C_{g^{*},\Lambda}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]


So 
\begin{eqnarray*}
\left|\frac{\partial}{\partial m_{k}}P_{k}(\hat{g}_{1,k}+m_{k}h_{k})\right|_{m=\hat{m}(\lambda)} & \le & \frac{1}{\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\end{eqnarray*}


Define the matrix $D_{upper}$ which bounds the diagonal elements
of $D_{2}$
\[
D_{upper}=\frac{1}{\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]


So $D_{upper}\succeq D_{3}$.

\textbf{3. We bound the norm of $\nabla_{\lambda}\hat{m}_{k}(\lambda)$
for all $k=1,...,J$.}

Hence
\begin{eqnarray*}
\|\nabla_{\lambda}\hat{m}_{k}(\lambda)\| & = & \|Me_{k}\|\\
 & = & \left\Vert D_{2}\left(S+D_{1}\right)^{-1}e_{k}\right\Vert \\
 & \le & \left\Vert D_{upper}\left(S+D_{1}\right)^{-1}e_{k}\right\Vert \\
 & \le & \frac{1}{\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\left\Vert \left(S+D_{1}\right)^{-1}e_{k}\right\Vert \\
 & \le & \frac{1}{\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}d^{-1}
\end{eqnarray*}


where we used the fact that the second Gateuax derivative 
\[
e_{k}^{\top}\left(S+D_{1}\right)e_{k}\ge\left\langle D_{g}^{2}\left(\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(g_{j})\right)\circ h,h\right\rangle \ge d
\]


\textbf{4. Apply the Mean Value Theorem}

By the MVT, for all $j=1,..,J$, we have that there exists an $\alpha\in(0,1)$
such that 
\begin{eqnarray*}
\left|\hat{m}_{j}(\boldsymbol{\lambda}^{(2)})-\hat{m}_{j}(\boldsymbol{\lambda}^{(1)})\right| & = & \left|\left\langle \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)},\nabla_{\lambda}\hat{m}_{j}(\boldsymbol{\lambda})\right\rangle _{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda}^{(1)}+(1-\alpha)\boldsymbol{\lambda}^{(2)}}\right|\\
 & \le & \left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \frac{1}{d\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\end{eqnarray*}


Since 
\[
\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})=\hat{m}_{j}(\boldsymbol{\lambda}^{(2)})\frac{\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})}{\left\Vert \hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{D}}
\]


Then 
\[
\left\Vert \hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}_{j}(\cdot|\boldsymbol{\lambda}^{(1)})\right\Vert _{D}\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \frac{1}{d\lambda_{min}}\sqrt{2C_{g^{*},\Lambda}\frac{n_{D}}{n_{T}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]



\section{Multiple smooth penalties for a single model}

Consider the problem 

\[
\mathcal{G}(T)=\left\{ \hat{g}\left(\cdot|\boldsymbol{\lambda}\right)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-g\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}^{v_{j}}(g)+\frac{w}{2}\|g\|_{D}^{2}\right)\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$ and $v_{j}>1$
for all $j-1,...,J$.

For all $j=1,...,J$, suppose the penalty functions $P_{j}$ are convex
and twice-differentiable: For any functions $g,h$, the following
second-derivative exists and the inequality holds: 
\[
\frac{\partial^{2}}{\partial m^{2}}P_{j}(g+mh)\ge0\forall j=1,..,J
\]


Also, suppose that the penalty functions $P_{j}$ are semi-norms:
for all functions $a,b$, the triangle inequality is satisfied
\[
P_{j}(a)+P_{j}(b)\ge P_{j}(a+b)
\]


For $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$ where $\|\boldsymbol{\lambda^{(1)}-\lambda^{(2)}}\|$
is sufficiently small, we have

\[
\|\hat{g}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}(\cdot|\boldsymbol{\lambda}^{(1)})\|_{D}^{2}\le\|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left(w\sqrt{J}\lambda_{min}\right)^{-1}C_{0}
\]


where $C_{0}$ is a constant.


\subsubsection*{Proof}

Let $h=\hat{g}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}(\cdot|\boldsymbol{\lambda}^{(1)})$.
For notational convenient, let $\hat{g}_{1}(\cdot)=\hat{g}(\cdot|\boldsymbol{\lambda}^{(1)})$.
Suppose $\|h\|_{D}>0$.

We consider the optimization problem restricted to the set of non-zero
differences

\[
\hat{m}(\boldsymbol{\lambda})=\arg\min_{m}\frac{1}{2}\left\Vert y-\left(\hat{g}_{1}+mh\right)\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}^{v_{j}}(\hat{g}_{1}+mh)+\frac{w}{2}\|\hat{g}_{1}+mh\|_{D}^{2}\right)
\]


\textbf{1. Calculate $\frac{\partial}{\partial\lambda}\hat{m}(\lambda)$}

By the gradient optimality conditions, we have that

\begin{eqnarray*}
\left.\left\langle y-\left(\hat{g}_{1}+mh\right),h\right\rangle _{T}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial}{\partial m}P_{j}^{v_{j}}(\hat{g}_{1}+mh)+w\langle h,\hat{g}_{1}+mh\rangle_{D}\right)\right|_{m=\hat{m}(\lambda)} & = & 0
\end{eqnarray*}


Now we implicitly differentiate with respect to $\lambda_{k}$ to
get 
\begin{eqnarray*}
 &  & \frac{\partial}{\partial\lambda_{k}}\left[\left.\left\langle y-\left(\hat{g}_{1}+mh\right),h\right\rangle _{T}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial}{\partial m}P_{j}^{v_{j}}(\hat{g}_{1}+mh)+w\langle h,\hat{g}_{1}+mh\rangle_{D}\right)\right|_{m=\hat{m}(\lambda)}\right]\\
 & = & \left.\left[\|h\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial^{2}}{\partial m^{2}}P_{j}^{v_{j}}(\hat{g}_{1}+mh)+w\|h\|_{D}^{2}\right)\right]_{m=\hat{m}(\lambda)}\frac{\partial\hat{m}(\lambda)}{\partial\lambda_{k}}+\left(\frac{\partial}{\partial m}P_{k}^{v_{k}}(\hat{g}_{1}+mh)+w\langle h,\hat{g}+mh\rangle_{D}\right)\right|_{m=\hat{m}(\lambda)}\\
 & = & 0
\end{eqnarray*}


So 
\[
\frac{\partial\hat{m}(\lambda)}{\partial\lambda_{k}}=\left.-\left[\|h\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial^{2}}{\partial m^{2}}P_{j}^{v_{j}}(\hat{g}_{1}+mh)+w\|h\|_{D}^{2}\right)\right]^{-1}\left(\frac{\partial}{\partial m}P_{k}^{v_{k}}(\hat{g}_{1}+mh)+w\langle h,\hat{g}+mh\rangle_{D}\right)\right|_{m=\hat{m}(\lambda)}
\]


\textbf{2. Bound $\frac{\partial\hat{m}(\lambda)}{\partial\lambda_{k}}$}

The first multiplicand is bounded by 
\[
\left|\|h\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial^{2}}{\partial m^{2}}P_{j}^{v_{j}}(\hat{g}_{1}+mh)+w\|h\|_{D}^{2}\right)\right|^{-1}\le\left(wJ\lambda_{min}\|h\|_{D}^{2}\right)^{-1}
\]


since the penalty functions are convex.

By Lemma Semi-norm derivatives (Appendix), we have that since $P_{k}$
is a semi-norm, then 
\begin{eqnarray*}
\left|\frac{\partial}{\partial m}P_{j}(\hat{g}_{1}+mh)\right| & \le & P_{j}(h)\\
 & = & P_{j}\left(\hat{g}(\cdot|\boldsymbol{\lambda}^{(2)})-\hat{g}(\cdot|\boldsymbol{\lambda}^{(1)})\right)\\
 & \le & P_{j}\left(\hat{g}(\cdot|\boldsymbol{\lambda}^{(2)})\right)+P_{j}\left(\hat{g}(\cdot|\boldsymbol{\lambda}^{(1)})\right)
\end{eqnarray*}


By the basic inequality, we know that
\begin{eqnarray*}
\lambda_{min}P_{j}\left(\hat{g}(\cdot|\boldsymbol{\lambda})\right) & \le & \frac{1}{2}\left\Vert y-g^{*}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}^{v_{j}}(g^{*})+\frac{w}{2}\|g^{*}\|_{D}^{2}\right)\\
 & \le & C
\end{eqnarray*}


where
\[
C=\frac{1}{2}\left\Vert y-g^{*}\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}^{v_{j}}(g^{*})+\frac{w}{2}\|g^{*}\|_{D}^{2}\right)
\]


Therefore
\begin{eqnarray*}
\left|\frac{\partial}{\partial m}P_{j}(\hat{g}_{1}+mh)\right| & \le & 2C/\lambda_{min}
\end{eqnarray*}


Also by the definition of $\hat{m}(\boldsymbol{\lambda})$,
\begin{eqnarray*}
\lambda_{min}\left(P_{k}^{v_{k}}(\hat{g}_{1}+\hat{m}(\boldsymbol{\lambda})h)+\frac{w}{2}\|\hat{g}+\hat{m}(\boldsymbol{\lambda})h\|_{D}^{2}\right) & \le & \frac{1}{2}\left\Vert y-\hat{g}_{1}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)\\
 & = & \frac{1}{2}\left\Vert y-\hat{g}_{1}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)+\sum_{j=1}^{J}\left(\lambda_{j}-\lambda_{j}^{(1)}\right)\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)\\
 & \le & \frac{1}{2}\left\Vert y-g^{*}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}^{v_{j}}(g^{*})+\frac{w}{2}\|g^{*}\|_{D}^{2}\right)+J\lambda_{max}\max_{j}\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)\\
 & \le & C+J\lambda_{max}\max_{j}\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)
\end{eqnarray*}


And by the definition of $\hat{g}_{1}$, 
\[
\lambda_{min}\max_{j}\left(P_{j}^{v_{j}}(\hat{g}_{1})+\frac{w}{2}\|\hat{g}_{1}\|_{D}^{2}\right)\le\frac{1}{2}\left\Vert y-g^{*}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}^{v_{j}}(g^{*})+\frac{w}{2}\|g^{*}\|_{D}^{2}\right)\le C
\]


Therefore
\[
\lambda_{min}P_{k}^{v_{k}}(\hat{g}_{1}+\hat{m}(\boldsymbol{\lambda})h)\le C\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\implies P_{k}^{v_{k}-1}(\hat{g}_{1}+\hat{m}(\boldsymbol{\lambda})h)\le\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}
\]


and
\[
\lambda_{min}\frac{w}{2}\|\hat{g}+\hat{m}(\boldsymbol{\lambda})h\|_{D}^{2}\le C\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\implies\|\hat{g}+\hat{m}(\boldsymbol{\lambda})h\|_{D}\le\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]


Hence
\begin{eqnarray*}
\left|\frac{\partial}{\partial m}P_{k}^{v_{k}}(\hat{g}_{1}+mh)+w\langle h,\hat{g}+mh\rangle_{D}\right| & \le & \left|\frac{\partial}{\partial m}P_{k}^{v_{k}}(\hat{g}_{1}+mh)\right|+w\|h\|_{D}\|\hat{g}+mh\|_{D}\\
 & \le & \frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\end{eqnarray*}


Therefore 
\[
\left|\frac{\partial\hat{m}(\boldsymbol{\lambda})}{\partial\lambda_{k}}\right|\le\left(wJ\lambda_{min}\|h\|_{D}^{2}\right)^{-1}\left[\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\right]
\]


Therefore 
\begin{eqnarray*}
\left\Vert \nabla_{\lambda}\hat{m}(\boldsymbol{\lambda})\right\Vert  & \le & \sqrt{J}\left[\left(wJ\lambda_{min}\|h\|_{D}^{2}\right)^{-1}\left[\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\right]\right]\\
 & = & \left(w\sqrt{J}\lambda_{min}\|h\|_{D}^{2}\right)^{-1}\left[\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\right]
\end{eqnarray*}


\textbf{3. Apply the Mean Value Theorem}

Assuming that the penalty functions are smooth, then $\hat{m}(\boldsymbol{\lambda})$
is continuous and differentiable. Then by the MVT, there is an $\alpha\in(0,1)$
such that

\begin{eqnarray*}
\left|\hat{m}(\boldsymbol{\lambda}^{(2)})-\hat{m}(\boldsymbol{\lambda}^{(1)})\right| & = & \left\langle \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)},\left.\nabla_{\lambda}\hat{m}(\boldsymbol{\lambda})\right|_{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda}^{(1)}+(1-\alpha)\boldsymbol{\lambda}^{(2)}}\right\rangle \\
 & \le & \|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left\Vert \left.\nabla_{\lambda}\hat{m}(\boldsymbol{\lambda})\right|_{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda}^{(1)}+(1-\alpha)\boldsymbol{\lambda}^{(2)}}\right\Vert \\
 & \le & \|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left(w\sqrt{J}\lambda_{min}\|h\|_{D}^{2}\right)^{-1}\left[\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\right]
\end{eqnarray*}


Since $\hat{m}(\boldsymbol{\lambda}^{(2)})-\hat{m}(\boldsymbol{\lambda}^{(1)})=1$,
then we have
\[
\|h\|_{D}^{2}\le\|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left(w\sqrt{J}\lambda_{min}\right)^{-1}\left[\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}+w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}\right]
\]


\textbf{Case 1:}

Suppose $\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}\ge w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}$. 

Then 
\[
\|h\|_{D}^{2}\le\|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left(w\sqrt{J}\lambda_{min}\right)^{-1}\frac{4Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}
\]


\textbf{Case 2: }

Suppose $\frac{2Cv_{k}}{\lambda_{min}}\left[\frac{C}{\lambda_{min}}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\right]^{(v_{k}-1)/v_{k}}\le w\|h\|_{D}\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}$. 

\[
\|h\|_{D}\le\|\boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\|\left(\sqrt{J}\lambda_{min}\right)^{-1}2\sqrt{\frac{2C}{\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)}
\]


Unfortunately, for $\|\lambda^{(2)}-\lambda^{(1)}\|$ sufficiently
small, then $\|h\|_{D}$ will be sufficiently small such that we will
always be in Case 1.


\section{Nonsmooth penalties}

It is possible for penalties to be non-smooth, like the total variation
penalty

\[
P_{j}(g_{j})=\int\left|g_{j}^{(r_{j})}(x)\right|dx
\]


We might be able to suppose the conditions as before, but are they
even reasonable? That is, will we be able to say that the differentiable
space of the training criterion at $\hat{g}(\cdot|\boldsymbol{\lambda})$
is a local optimality space. A differentiable space is now thought
to be $\left\{ h:\frac{d}{dt}P\left(\hat{g}(\cdot|\boldsymbol{\lambda})+th\right)\mbox{ exists}\right\} $.
In the case of TV, the penalty is not differentiable if $h$ has any
non-zero-measure segment of zero values (doesn't matter what the point
we are evaluating the penalty's derivative). This is very confusing...
Is a differentiable space also a local optimality space for any non-smooth
penalties? This is already a conjecture for the lasso, and I have
no idea what the behavior is for non-parametric settings. Let us not
think about it.


\section{Examples}

Sobolev satisfies the conditions in Section 2
\[
\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(g_{j})+\frac{w}{2}\|g_{j}\|_{D}^{2}\right)
\]


where 
\[
P_{j}(g_{j})=\int\left(g_{j}^{(r_{j})}(x)\right)^{2}dx
\]


Note that the Sobolev penalty is convex since 
\[
\frac{\partial^{2}}{\partial m^{2}}P_{j}(g+mh)=P_{j}(h)\ge0
\]



\section{Appendix}


\subsubsection*{Lemma: Bounding the derivative of a semi-norm}

Let $P$ be a semi-norm. Then

\[
\left|\frac{\partial}{\partial m}P(a+mb)\right|\le P(b)
\]



\subsubsection*{Proof}

By triangle inequality, we know


\subsubsection*{
\[
\left|P(a+mb)-P(a)\right|\le|m|P(b)
\]
}

Therefore as we take $m\rightarrow0$, we have 
\[
\left|\frac{\partial}{\partial m}P(a+mb)\right|\le P(b)
\]



\subsubsection*{Lemma: second gateaux differential}

Suppose 
\[
D_{v,w}^{2}F(g)=\lim_{\epsilon\rightarrow0}\frac{D_{v}F(g+\epsilon w)-D_{v}F(g)}{\epsilon}\ge0
\]


where 
\[
D_{v}F(g)=\lim_{\epsilon\rightarrow0}\frac{F(g+\epsilon v)-F(g)}{\epsilon}
\]
Then 
\[
\frac{\partial^{2}}{\partial m^{2}}F\left(g+mh\right)\ge0
\]



\subsubsection*{Proof}

\begin{eqnarray*}
\frac{\partial}{\partial m}F\left(g+mv\right)|_{m=m} & = & \lim_{\epsilon\rightarrow0}\frac{F(g+\left(m+\epsilon\right)v)-F(g+mv)}{\epsilon}\\
 & = & \lim_{\epsilon\rightarrow0}\frac{F(g+mv+\epsilon v)-F(g+mv)}{\epsilon}\\
 & = & D_{v}F(g+mv)
\end{eqnarray*}



\subsubsection*{
\begin{eqnarray*}
\frac{\partial^{2}}{\partial m^{2}}F\left(g+mv\right) & = & \lim_{\epsilon\rightarrow0}\frac{\frac{\partial}{\partial m}F\left(g+mv\right)|_{m=m+\epsilon}-\frac{\partial}{\partial m}F\left(g+mv\right)|_{m=m}}{\epsilon}\protect\\
 & = & \lim_{e\rightarrow0}\frac{D_{v}F(g+\left(m+\epsilon\right)v)-D_{v}F(g+mv)}{\epsilon}\protect\\
 & = & \lim_{e\rightarrow0}\frac{D_{v}F(g+mv+\epsilon v)-D_{v}F(g+mv)}{\epsilon}\protect\\
 & = & D_{v,v}^{2}F(g+mv)\protect\\
 & \ge & 0
\end{eqnarray*}
}
\end{document}
